{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dennis-224/Data_cleaning/blob/main/ComparingFS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UA5V1eTYRAXf"
      },
      "source": [
        "#**COMPARING FS TECHNIQUES**\n",
        "#**SCIENTIST: DENNIS OKEREKE**\n",
        "\n",
        "**FIRST** we need a covid-19 dataset from Kaggle\n",
        "\n",
        "Note that Covid machine models are usually classification problems where we use algorithms like *Logistic Regression, SVM, Decision trees, k-nearestneighbors,  naive bayes.*\n",
        "\n",
        "Here, however, we have a regression problem on our hands, based on the Covid dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58s_dnvmXO-A"
      },
      "source": [
        "#Dataset\n",
        "\n",
        "I got a dataset from kaggle titled '*COVID19 Worldwide Testing Data*' , a 12-column file containing daily & cumulative number of tests conducted, number of positive, hospitalized, recovered & death cases reported by country\n",
        "\n",
        "**Content**\n",
        "\n",
        "Data of daily & cumulative number of tests conducted, number of positive, hospitalized, recovered & death cases reported from each country per day. There are over 100 countries in the dataset. This dataset is updated daily, last updated on Nov 05, 2020.\n",
        "\n",
        "**Reference**: SLOTH (Owner)\n",
        "\n",
        "**Tagged**: Health, Standardized testing, CoronaVirus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpWYr8k4s1Ye"
      },
      "source": [
        "## Abstract\n",
        "\n",
        "This study addresses the critical challenge of forecasting daily positive COVID-19 cases using a global dataset of **27,641 entries**. *147 countries* were included in the study. Initial data preprocessing involved handling significant missing values, with up to **69.58%** observed in the 'hospitalized' column, addressed via RobustScaler for outlier treatment, followed by country-grouped forward-fill and median imputation. Exploratory Data Analysis revealed a highly positively skewed distribution for 'daily_positive' cases (skewness: **11.72**) and strong correlations, notably between 'daily_tested' and 'daily_positive' (Pearson r: **0.93**), and 'positive' cases and 'death' (Pearson r: **0.96**). Diverse feature selection techniques were employed: VIF, CMIM, MDG (Mean Decrease Gini), ANOVA, SBE (Sequential Backward Elimination), Lasso regularization, and Boruta-VI. A robust consensus emerged among most methods, with MDG, ANOVA, SBE, and Boruta-VI demonstrating **100%** Jaccard similarity and selecting 8 features. VIF and CMIM showed **100%** similarity, identifying 7 features, and a high **88%** similarity with the aforementioned group. Conversely, Lasso regularization (alpha=0.01) exhibited extreme parsimony, selecting only 'death' with low **12-14%** similarity to other methods. Nine regression models were evaluated using 3-times repeated 3-fold cross-validation. The **Random Forest Regressor** consistently outperformed all other models, achieving a superior mean R-squared of **0.9738** (MAE: **0.0011**, MSE: **0.000026**) with high stability (R-squared standard deviation: **0.0058**). In stark contrast, the Support Vector Regressor (SVR) demonstrated exceptionally poor performance, yielding a negative mean R-squared of **-1.6368**. These findings underscore the effectiveness of ensemble tree-based models and the identified robust feature set in accurately predicting COVID-19 case trends, highlighting the importance of comprehensive feature analysis and model validation in public health forecasting."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading...\n",
        "\n",
        "We load our dataset directly from kaggle using API. Then we perform Exploratory Data Analysis."
      ],
      "metadata": {
        "id": "lsBf73cKnfGK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kagglehub\n",
        "import kagglehub"
      ],
      "metadata": {
        "id": "f13K5w2pYsha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zM2Wx_rhAsU"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"lin0li/covid19testing\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/MyDrive/HiddenAPI'"
      ],
      "metadata": {
        "id": "uMx60Vd-dYUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BMyyDsqh8r-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYG5qXmoiF9u"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "csv_files = [f for f in os.listdir(path) if f.endswith('.csv')]\n",
        "if csv_files:\n",
        "    df = pd.read_csv(os.path.join(path, csv_files[0]))\n",
        "    print(df.describe(include= 'all'))\n",
        "    df.info()\n",
        "    df.head()\n",
        "else:\n",
        "    print(\"No CSV files found in the dataset directory.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36b425d4"
      },
      "outputs": [],
      "source": [
        "df.to_csv('covid19_testing_data.csv', index=False)\n",
        "print(\"DataFrame successfully saved to 'covid19_testing_data.csv'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "844e7cd3"
      },
      "outputs": [],
      "source": [
        "print(df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29dd9705"
      },
      "outputs": [],
      "source": [
        "missing_values = df.isnull().sum()\n",
        "missing_values = missing_values[missing_values > 0]\n",
        "if not missing_values.empty:\n",
        "    print(\"Columns with missing values and their counts:\")\n",
        "    print(missing_values)\n",
        "else:\n",
        "    print(\"No missing values found in any column.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZeD0ij18_f4d"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ggFlJKSuVsI"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpOxusZxm8kb"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "# Calculate the number of data entries (rows) for each country\n",
        "country_data_counts = df['Country_Region'].value_counts().reset_index()\n",
        "country_data_counts.columns = ['Country_Region', 'Data_Entries_Count']\n",
        "\n",
        "# Create a Plotly table\n",
        "fig = go.Figure(data=[go.Table(\n",
        "    header=dict(values=list(country_data_counts.columns), fill_color='paleturquoise', align='left'),\n",
        "    cells=dict(values=[country_data_counts.Country_Region, country_data_counts.Data_Entries_Count], fill_color='lavender', align='left'))\n",
        "])\n",
        "\n",
        "fig.update_layout(title_text='Number of Data Entries per Country/Region')\n",
        "fig.show()\n",
        "\n",
        "print(\"Plotly table showing number of data entries per country/region generated.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "db177e78"
      },
      "outputs": [],
      "source": [
        "# Count the number of unique countries\n",
        "num_countries = df['Country_Region'].nunique()\n",
        "print(f\"Number of unique countries in the dataset: {num_countries}\")\n",
        "\n",
        "# Count the number of features (columns) in the dataset\n",
        "num_features = df.shape[1]\n",
        "print(f\"Number of features (columns) in the dataset: {num_features}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1ad8e15"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "import pandas as pd\n",
        "\n",
        "# Ensure country_data_counts is available and has the 'Percentage' column.\n",
        "# Re-calculate if not in memory or if the 'Percentage' column is missing.\n",
        "if 'country_data_counts' not in locals() or 'Percentage' not in country_data_counts.columns:\n",
        "    country_data_counts = df['Country_Region'].value_counts().reset_index()\n",
        "    country_data_counts.columns = ['Country_Region', 'Data_Entries_Count']\n",
        "    total_entries = len(df)\n",
        "    country_data_counts['Percentage'] = (country_data_counts['Data_Entries_Count'] / total_entries) * 100\n",
        "    country_data_counts = country_data_counts.sort_values(by='Percentage', ascending=False)\n",
        "\n",
        "# Define a threshold for 'scanty' entries (e.g., less than 10 entries)\n",
        "scanty_threshold = 10\n",
        "scanty_countries_df = country_data_counts[country_data_counts['Data_Entries_Count'] < scanty_threshold]\n",
        "\n",
        "# If there are countries with exactly 0 entries (though value_counts typically won't include them\n",
        "# unless explicitly included by a full list of countries), they would be handled here.\n",
        "# For this dataset, countries with 0 entries are unlikely to be in country_data_counts itself.\n",
        "\n",
        "if not scanty_countries_df.empty:\n",
        "    # Create a Plotly table for scanty entries\n",
        "    fig = go.Figure(data=[go.Table(\n",
        "        header=dict(values=list(scanty_countries_df.columns), fill_color='paleturquoise', align='left'),\n",
        "        cells=dict(values=[scanty_countries_df.Country_Region, scanty_countries_df.Data_Entries_Count, scanty_countries_df.Percentage],\n",
        "                   fill_color='lavender', align='left'))\n",
        "    ])\n",
        "\n",
        "    fig.update_layout(title_text=f'Countries with less than {scanty_threshold} Data Entries')\n",
        "    fig.show()\n",
        "    print(f\"Interactive table showing countries with less than {scanty_threshold} data entries generated.\")\n",
        "else:\n",
        "    print(f\"No countries found with less than {scanty_threshold} data entries.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10b155d3"
      },
      "outputs": [],
      "source": [
        "# Identify numerical columns that are suitable for predictions (excluding the target variable)\n",
        "# Assuming 'daily_positive' is the target variable\n",
        "\n",
        "# Get all numerical columns from the DataFrame\n",
        "numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
        "\n",
        "# Exclude the target variable 'daily_positive' from the list of features\n",
        "columns_for_prediction = [col for col in numerical_cols if col != 'daily_positive']\n",
        "\n",
        "# Count the number of features for prediction\n",
        "num_prediction_features = len(columns_for_prediction)\n",
        "\n",
        "print(f\"Columns identified for prediction: {columns_for_prediction}\")\n",
        "print(f\"Number of columns identified for prediction: {num_prediction_features}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ec7aaa02"
      },
      "outputs": [],
      "source": [
        "# Count the number of unique countries\n",
        "num_countries = df['Country_Region'].nunique()\n",
        "print(f\"Number of unique countries in the dataset: {num_countries}\")\n",
        "\n",
        "# Count the number of features (columns) in the dataset\n",
        "num_features = df.shape[1]\n",
        "print(f\"Number of features (columns) in the dataset: {num_features}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3b35c8e2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# --- Calculate Daily Positive Rate (N and Percentage) ---\n",
        "\n",
        "# Ensure 'Date' column is in datetime format and set as index for time-series operations if needed,\n",
        "# but for overall rate, simple sum/mean across non-nulls is sufficient.\n",
        "# Using the original 'df' for this calculation before any scaling for raw numbers.\n",
        "\n",
        "# Drop rows where both daily_positive and daily_tested are NaN to avoid issues in calculation\n",
        "# And ensure we only consider days where testing and positive cases are reported.\n",
        "df_daily_rates = df[['daily_positive', 'daily_tested']].dropna()\n",
        "\n",
        "total_daily_positive_N = df_daily_rates['daily_positive'].sum()\n",
        "total_daily_tested_N = df_daily_rates['daily_tested'].sum()\n",
        "\n",
        "if total_daily_tested_N > 0:\n",
        "    daily_positive_percentage = (total_daily_positive_N / total_daily_tested_N) * 100\n",
        "else:\n",
        "    daily_positive_percentage = 0.0 # Handle division by zero\n",
        "\n",
        "print(f\"\\n--- Daily Positive Rate in Cohort ---\")\n",
        "print(f\"Total Daily Positive Cases (N): {total_daily_positive_N:,.0f}\")\n",
        "print(f\"Total Daily Tested Cases (N): {total_daily_tested_N:,.0f}\")\n",
        "print(f\"Daily Positive Rate: {daily_positive_percentage:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98d25aed"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Calculate the number of data entries (rows) for each country\n",
        "country_data_counts = df['Country_Region'].value_counts().reset_index()\n",
        "country_data_counts.columns = ['Country_Region', 'Data_Entries_Count']\n",
        "\n",
        "# Calculate the total number of entries in the DataFrame\n",
        "total_entries = len(df)\n",
        "\n",
        "# Calculate the percentage of data entries for each country\n",
        "country_data_counts['Percentage'] = (country_data_counts['Data_Entries_Count'] / total_entries) * 100\n",
        "\n",
        "# Sort by percentage in descending order for better visualization\n",
        "country_data_counts = country_data_counts.sort_values(by='Percentage', ascending=False)\n",
        "\n",
        "# Create a bar chart for percentages\n",
        "plt.figure(figsize=(20, 10)) # Adjust figure size as needed\n",
        "sns.barplot(x='Country_Region', y='Percentage', data=country_data_counts, palette='viridis')\n",
        "plt.title('Percentage of Data Entries per Country/Region (Ratio of Total Entries)')\n",
        "plt.xlabel('Country/Region')\n",
        "plt.ylabel('Percentage of Total Entries')\n",
        "plt.xticks(rotation=90, ha='right') # Rotate labels for better readability\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7cLWsLLoAz5"
      },
      "source": [
        "### Interpretation of the Data Entries Percentage Chart\n",
        "\n",
        "This bar chart, titled 'Percentage of Data Entries per Country/Region (Ratio of Total Entries)', visualizes the proportion of data available for each country or region in the dataset.\n",
        "\n",
        "**Key Observations:**\n",
        "\n",
        "*   **Dominance of the United States**: The 'United States' stands out significantly, accounting for over 50% of the total data entries. This means more than half of your dataset pertains to the United States.\n",
        "*   **Second Tier Countries**: Countries like 'Canada', 'Australia', 'Czechia', and 'Israel' follow, but with substantially lower percentages, around 10% to 1% respectively.\n",
        "*   **Long Tail Distribution**: There's a clear 'long tail' distribution, where a large number of countries contribute a very small percentage of the total data. Many countries have less than 1% of the total entries.\n",
        "\n",
        "**Overall Interpretation:**\n",
        "\n",
        "This chart highlights a significant imbalance in the geographical representation within your dataset. The vast majority of the data is concentrated in the United States, followed by a few other countries, while many regions have very few data entries. This imbalance is crucial to consider for any analysis or modeling, as it could lead to models that are heavily biased towards the regions with more data and may not generalize well to underrepresented countries. We could address this by focusing on specific regions, applying weighting techniques, or acknowledging the data's limitations.\n",
        "\n",
        "I choose to acknowledge data limitations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TP_3Vt4r_9za"
      },
      "source": [
        "We notice that for **hospitalized** and **hospitalizedCurr**, almost half of the values are missing. To many missing values affects trained models. We need to tackle the 'missing values' problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5M5_U7zRKC4"
      },
      "source": [
        "### A Poser:\n",
        "\n",
        "**In a covid dataset if there could be a dependent variable what would it be?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnZSntOfRTOz"
      },
      "source": [
        "When working with a COVID-19 dataset, the potential **dependent** **variable** **is** **typically** **the** **outcome** **or** **response** **being** **studied**, which can vary widely based on the research question. The most common dependent variables often relate to the direct health impacts or the effects of interventions, such as the **number** **of** **new** **cases**, **hospitalizations**, **or** **deaths**.\n",
        "\n",
        "Here are some of the key dependent variables that could be used in a COVID-19 dataset analysis:\n",
        "\n",
        "\n",
        "*   **Incidence** **and** **Prevalence** **of** **Infection**: This is a very frequent dependent variable, often measured as the number of new confirmed cases per day, week, or per capita. Researchers use this to study the effectiveness of containment measures or the impact of environmental factors on the virus's spread.\n",
        "\n",
        "*   **Mortality** **and** **Death** **Rates**: The number of deaths, the cumulative death rate per 100,000 people, or the \"daily mortality change\" are critical dependent variables for studies focused on the lethality of the virus and factors that influence patient survival.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Choice of Dependent variable\n",
        "\n",
        "Let Daily_positive be our dependent variables, also known as \"y\""
      ],
      "metadata": {
        "id": "N9SvgbBaveuu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfSHfRFCZgBC"
      },
      "source": [
        "### Our dataset\n",
        "\n",
        "Our dataset is well-suited for time-series forecasting, especially to predict various COVID-19 related metrics. We can forecast trends for positive cases, active cases, deaths, daily_positive cases, and daily_tested numbers. This can be done at different geographical granularities, such as for specific countries or even provinces/states.\n",
        "\n",
        "Hence we can:\n",
        "\n",
        "**Prepare Data for Time Series:** Convert the 'Date' column to datetime objects and set it as the DataFrame index. This step is crucial for any time-series analysis or forecasting.\n",
        "\n",
        "**Handle Missing Values (if necessary):** This ensures all relevant columns for forecasting are properly handled. Depending on the forecasting model, different imputation strategies might be suitable (e.g., forward-fill, interpolation, or specific statistical imputation methods).\n",
        "\n",
        "**Select Target Metric and Granularity:** We need to decide which specific metric (e.g., 'daily_positive', 'death', 'total_tested') and which geographical level (e.g., 'United States' or a specific 'Province_State') to focus on for forecasting. This may involve filtering the DataFrame.\n",
        "\n",
        "**Visualize Time Series Trends:** Here we plot the selected metric over time for the chosen region to observe patterns, seasonality, and overall trends. This helps in understanding the data's behavior and informs model selection. A good practice is to include proper legends for the plots.\n",
        "\n",
        "**Choose a Forecasting Model:** Based on the observed trends and the nature of the data, we need to select an appropriate time-series forecasting model. Common choices include ARIMA, SARIMA (if seasonality is present), Prophet, or simple exponential smoothing methods.\n",
        "\n",
        "**Develop and Evaluate Forecasting Model:** This involves spliting the data into training and testing sets, training the chosen model on the training data, making predictions on the test set, and evaluating the model's performance using relevant metrics (e.g., RMSE, MAE). Next is to provide a visualization of the predictions against actuals. We ought to include proper legends for the plots.\n",
        "\n",
        "**Then:** we summarize the potential forecasting insights from the dataset, including which metrics can be forecasted and at what level of detail."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5dbuyrNl__O"
      },
      "source": [
        "## Outlier Detection\n",
        "\n",
        "An outlier is that observation in handling datasets, that appears to deviate markedly from other members of the sample in which it occurs (Grubbs, 1969).\n",
        "\n",
        "It is caused by: *human mistakes, errors when using instruments, data processing errors, errors from sampling.* Note that an outlier may also **not be an error** (such that it is but a result of extreme *novelty* in the data).\n",
        "\n",
        "A dataset does **not** need to have dependent and independent variables to find outliers. Outlier detection can be performed on a single variable (univariate data) or multiple variables without an assumed cause-and-effect relationship (multivariate data/unsupervised methods).\n",
        "\n",
        "The concept of dependent and independent variables is specific to regression analysis and other supervised modeling techniques, where the goal is to predict a target variable based on one or more predictor variables\n",
        "\n",
        "### Outlier detection in different data contexts\n",
        "\n",
        "•\t**Univariate** **Data** (**Single** **Variable**): Detecting outliers in a single column or feature of one’s data using methods that assess how far a data point is from the center of that variable's distribution.\n",
        "\n",
        "•\t**Multivariate** **Data** (**Multiple** **Variables**, **No** **Assumed** **Relationship**): In this case, one looks for data points that are unusual in the multidimensional space formed by the variables, even if each individual variable's value is normal on its own.\n",
        "\n",
        "•\t**Regression** **Analysis** (**Dependent** & **Independent** **Variables**): Outliers in regression analysis can occur in the independent variables (high leverage points), the dependent variable (large residuals), or both, and their impact is assessed relative to how well they fit the model's trend.\n",
        "\n",
        "Some common methods that do not require defined dependent and independent variables are Visualization Techniques, Interquartile Range (IQR) Method, Z-score Method, Machine Learning Algorithms:\n",
        "\n",
        "•\t**Visualization** **Techniques**: Simple visual tools like box plots, histograms, and scatter plots (for bivariate data) can effectively highlight outliers.\n",
        "\n",
        "•\t**Interquartile** **Range** (**IQR**) **Method**: This robust statistical technique uses the range of the middle 50% of the data (between the 25th and 75th percentiles) to define boundaries (fences). Any point outside these fences is considered an outlier.\n",
        "\n",
        "•\t**Z-score Method**: For data that is normally distributed, the Z-score measures how many standard deviations away from the mean a data point is. Values typically more extreme than +/- 3 standard deviations are flagged as outliers.\n",
        "\n",
        "•\t**Machine Learning Algorithms:** Unsupervised methods like Isolation Forests, Local Outlier Factor (LOF), and One-Class Support Vector Machines (SVM) are designed specifically for anomaly detection in data with multiple features, without needing a dependent variable.\n",
        "\n",
        "•\t**Mahalanobis Distance:** This method measures the distance of a data point from the centroid of a multivariate distribution, considering the correlations between variables. Points with a large Mahalanobis distance are considered multivariate outliers.\n",
        "\n",
        "**It is good practice to:** *tackle the extreme values (outliers) before dealing with the missing values (nulls) for more reliable data preparation.* One reason is that outliers heavily influence the mean and standard deviation, which are common metrics for imputation (e.g., mean/median imputation). Another reason for this understanding is that by removing or capping outliers first, we get more reliable measures (like a median that isn't skewed) to use for filling in our null values, leading to more accurate imputation\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXZ5htBrR3aB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba15814a"
      },
      "source": [
        "# Visualizations/Bivariate Analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40526dd5"
      },
      "source": [
        "### Calculate Correlation Matrix\n",
        "\n",
        "This will help us know the pairwise correlation between numerical columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37eb26ef"
      },
      "outputs": [],
      "source": [
        "# We want to calculate the corelation matrix\n",
        "# That is the pairwise correlation between numerical columns in the dataframe df\n",
        "numerical_df = df.select_dtypes(include=np.number) #select numerical columns in the dataframe\n",
        "correlation_matrix = numerical_df.corr() #apply the .corr()method and store in a variable\n",
        "print(\"Data Correlation Matrix\")\n",
        "print(correlation_matrix.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bddb06d"
      },
      "source": [
        "### Generate Correlation Heatmap\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bf9372a1"
      },
      "outputs": [],
      "source": [
        "# To shows the strenght and direction of linear relationship between each pair of variables\n",
        "# we use seaborn.heatmap with specified parameters\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Correlation Matrix of COVID-19 Metrics')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7A6Mi4q61Fd"
      },
      "source": [
        "### Observation\n",
        "\n",
        "From the generated heatmap, we can observe several strong positive and negative correlations among the COVID-19 related metrics:\n",
        "\n",
        "Strong Positive Correlations (values close to 1.00):\n",
        "*   **Positive Cases vs. Total Tested**:0.99\n",
        "*   **Positive Cases vs. Deaths**: 0.96\n",
        "*   **Positive Cases vs. Daily Tested**:0.94\n",
        "*   **Deaths vs. Hospitalized**:0.94\n",
        "*   **Deaths vs. Daily Tested**:0.95\n",
        "*   **Total Tested vs. Daily Tested**:0.86\n",
        "\n",
        "Moderate Positive Correlations (values between 0.5 and 0.9):\n",
        "*   **Active Cases vs. Hospitalized**: 0.92\n",
        "*   **Active Cases vs. Positive Cases**: 0.75\n",
        "*   **Recovered vs. Positive Cases**: 0.86\n",
        "\n",
        "Weak to Moderate Correlations:\n",
        "*   **Hospitalized Current (hospitalizedCurr)**: This variable shows weaker correlations with most other metrics compared to hospitalized. For instance, it has a correlation of 0.34 with positive and 0.37 with death.\n",
        "*   **Daily Positive (daily_positive)**: This shows a strong positive correlation with positive (0.88) and daily_tested (0.93) which is reasonable, as new positive cases are likely to be found through daily testing.\n",
        "\n",
        "Overall, the heatmap clearly illustrates the interconnectedness of various COVID-19 metrics, particularly highlighting the direct relationships between testing, cases, hospitalizations, and deaths.\n",
        "\n",
        "The strong correlations highlight the expected epidemiological relationships; for example, increased testing leads to more positive cases, and more positive cases can lead to more hospitalizations and deaths. These strong relationships could be leveraged in predictive models.\n",
        "\n",
        "Further investigation could focus on variables with weaker correlations, such as *hospitalizedCurr*, to understand why their relationships with other key metrics are not as strong as expected or to explore alternative relationships (e.g., lagged correlations)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Forcasting versus Date\n",
        "\n",
        "The 'Date' column will be converted to datetime objects in the next cell for several crucial reasons, primarily related to enabling effective time-series analysis and manipulation:\n",
        "\n",
        "1.\tTime-Series Operations: By converting the 'Date' column to a proper datetime format, it allows for various time-based operations such as:\n",
        "\n",
        "o\tSorting: Correctly ordering data chronologically.\n",
        "\n",
        "o\tFiltering: Selecting data within specific date ranges (e.g., all data from March 2020).\n",
        "\n",
        "o\tResampling: Aggregating data by different time frequencies (e.g., daily, weekly, monthly).\n",
        "\n",
        "o\tCalculating Time Differences: Easily determining durations between events.\n",
        "\n",
        "2.\tTime-Series Forecasting: This conversion is a prerequisite for any time-series forecasting model (like ARIMA, SARIMA, or even sequence-based deep learning models). These models rely on the temporal order and properties of the data.\n",
        "\n",
        "3.\tPlotting and Visualization: Proper datetime objects enable intuitive and accurate plotting of time-series data on an x-axis, allowing for easy identification of trends, seasonality, and significant events over time, as seen in plots like the 'Daily Positive COVID-19 Cases Over Time'.\n",
        "\n",
        "4.\tFeature Engineering: It facilitates the creation of new time-based features, such as day of the week, month, year, or even time lags, which can be valuable predictors for many models.\n"
      ],
      "metadata": {
        "id": "1ctMXCU-tc5w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0d40dfe"
      },
      "outputs": [],
      "source": [
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "print(\"Date column converted to datetime objects.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "578b05ba"
      },
      "source": [
        "### Line Chart: Daily Positive Cases Over Time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64cf5aa0"
      },
      "outputs": [],
      "source": [
        "# To visualize trend and fluctuations\n",
        "# To identify peak periods, changes in infection rates, overall progression of the pandemic across datasets\n",
        "# We can see the temporal evolution ot the metric: increases, decreases, plateaus\n",
        "plt.figure(figsize=(14, 7))\n",
        "sns.lineplot(data=df, x='Date', y='daily_positive', hue='Country_Region', estimator=None, legend=False, alpha=0.5)\n",
        "plt.title('Daily Positive COVID-19 Cases Over Time (All Regions)')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Daily Positive Cases')\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.show()\n",
        "\n",
        "# To see specific country trends more clearly, let's plot for a few major countries.\n",
        "plt.figure(figsize=(14, 7))\n",
        "selected_countries = ['United States', 'India', 'Brazil', 'United Kingdom'] # Example countries\n",
        "for country in selected_countries:\n",
        "    country_df = df[df['Country_Region'] == country]\n",
        "    sns.lineplot(data=country_df, x='Date', y='daily_positive', label=country)\n",
        "\n",
        "plt.title('Daily Positive COVID-19 Cases Over Time (Selected Countries)')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Daily Positive Cases')\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interpretation of the First Line Chart: Daily Positive COVID-19 Cases Over Time (All Regions)\n",
        "\n",
        "This chart, titled 'Daily Positive COVID-19 Cases Over Time (All Regions)' (inline_data_2), visualizes the daily confirmed positive cases of COVID-19 across all countries and regions present in the dataset, from the start of data collection until November 2020. Each light-colored line represents the daily positive cases for a different country or region. The estimator=None and legend=False parameters for sns.lineplot in the code mean that individual lines for each country are plotted without aggregation and without a legend to avoid clutter, giving an overall sense of the global trend.\n",
        "\n",
        "Key Observations:\n",
        "\n",
        "•\t**Overall Trend:** The plot shows a clear upward trend in daily positive cases globally from March 2020 onwards, with significant fluctuations. There are periods of relatively lower cases followed by sharp increases.\n",
        "\n",
        "•\t**Early Peaks**: Some regions show early peaks around April-May 2020.\n",
        "\n",
        "\n",
        "•\t**Sustained Growth:** From approximately June-July 2020, there's a more sustained and widespread increase in daily cases, leading to higher peaks towards the end of the observed period (October-November 2020).\n",
        "\n",
        "•\t**Variability:** The sheer number of overlapping lines, particularly in periods of high case counts, indicates significant variability and differing trajectories among various countries.\n",
        "\n",
        "•\t**Magnitude:** The Y-axis, representing 'Daily Positive Cases', shows figures reaching well over 80,000 to 100,000 daily cases in some instances, indicating the severe scale of the pandemic.\n",
        "\n",
        "**Overall Interpretation:**\n",
        "This chart provides a high-level overview of the pandemic's progression globally, revealing a general acceleration in daily positive cases over the observed period. It highlights the widespread nature of the outbreak and the varying impact across different entities, although individual country trends are not clearly distinguishable in this aggregated view due to the density of lines.\n",
        "\n",
        "\n",
        "### Interpretation of the Second Line Chart: Daily Positive COVID-19 Cases Over Time (Selected Countries)\n",
        "\n",
        "This chart, titled 'Daily Positive COVID-19 Cases Over Time (Selected Countries)' (inline_data_3), is a more focused visualization, displaying the daily positive COVID-19 cases specifically for a few major countries: 'United States', 'India', 'Brazil', and 'United Kingdom'. This allows for a clearer comparison of their individual pandemic trajectories.\n",
        "\n",
        "Key Observations:\n",
        "\n",
        "•\t**United States Dominance:** The 'United States' (blue line) consistently shows the highest numbers of daily positive cases, particularly from mid-2020 onwards, often fluctuating with an upward trend towards the end of the period. The shaded area around the US line might indicate confidence intervals or variations, although in this context, it appears to be part of the visual representation of the line itself within the seaborn lineplot function when hue is used.\n",
        "\n",
        "•\t**Early European Peaks:** The 'United Kingdom' (red line) shows an earlier and more pronounced peak around March-April 2020, followed by a decline and then another rise.\n",
        "\n",
        "•\t**South American Surge:** 'Brazil' (green line) indicates a significant surge in cases around May 2020.\n",
        "\n",
        "•\t**Indian Trend:** 'India' (orange line) also shows an upward trend in daily cases during the middle of the observed period.\n",
        "\n",
        "•\tVaried Progression: Each country exhibits a distinct temporal pattern, reflecting differences in local outbreaks, testing capacities, and public health interventions.\n",
        "\n",
        "**Overall Interpretation:**\n",
        "\n",
        "This plot provides granular insights into the pandemic's progression in key countries. It confirms the significant impact on the United States, with consistently high case numbers. It also demonstrates the wave-like nature of the pandemic, with different countries experiencing their peaks and troughs at various times. The comparison highlights the diverse challenges faced by different nations in managing the spread of COVID-19, and these individual country trends are crucial for more detailed time-series forecasting and policy analysis.\n",
        "\n"
      ],
      "metadata": {
        "id": "4cZ4rdnMxvOm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba26aaa1"
      },
      "source": [
        "### Bar Chart\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8480938a"
      },
      "outputs": [],
      "source": [
        "# Bar Chart: Total Positive Cases by Country/Region\n",
        "# For broad comparisons between different categories\n",
        "# A static snapshot of overall impact across geographical entities\n",
        "total_positive_cases = df.groupby('Country_Region')['positive'].max().sort_values(ascending=False).head(15)\n",
        "\n",
        "plt.figure(figsize=(14, 7))\n",
        "sns.barplot(x=total_positive_cases.index, y=total_positive_cases.values, palette='viridis')\n",
        "plt.title('Top 15 Countries/Regions by Total Positive COVID-19 Cases')\n",
        "plt.xlabel('Country/Region')\n",
        "plt.ylabel('Total Positive Cases')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "122a3e91"
      },
      "source": [
        "### Interpretation of the Bar Chart: Top 15 Countries/Regions by Total Positive COVID-19 Cases\n",
        "\n",
        "This bar chart (inline_data_4) visually represents the cumulative total number of positive COVID-19 cases for the top 15 most affected countries or regions within the dataset's timeframe.\n",
        "\n",
        "**Key Observations:**\n",
        "\n",
        "*   **United States Dominance**: The 'United States' stands out prominently with a significantly higher number of total positive cases compared to all other listed countries. Its bar is substantially taller than any other, indicating it recorded the highest cumulative case count.\n",
        "*   **Second Tier Impact**: Countries like 'Italy', 'Russia', 'Bangladesh', and 'Czechia' form a second tier, showing considerable but much lower total positive cases compared to the United States.\n",
        "*   **Gradual Decrease**: Beyond the top few countries, there's a clear decline in total cases, illustrating a 'long tail' distribution where many countries have progressively fewer cumulative positive cases among the top 15.\n",
        "*   **Relative Scale**: The differences in total cases are very pronounced, highlighting that the pandemic's impact in terms of positive case counts was not uniform across the globe, with certain regions experiencing a much higher burden.\n",
        "\n",
        "**Overall Interpretation:**\n",
        "\n",
        "This chart underscores the significant disparity in the distribution of total positive COVID-19 cases globally, with the United States experiencing the most substantial impact in terms of sheer case numbers. It provides a static snapshot that complements the time-series line plots by showing the cumulative effect over the observed period. This concentration of cases in a few regions is an important factor for data analysis, suggesting that models trained on this data might be heavily influenced by the patterns observed in the most affected countries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d2b5f1d"
      },
      "source": [
        "## Column Chart: Total Deaths by Country/Region\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0f2afc45"
      },
      "outputs": [],
      "source": [
        "# Similar to bar chart or a form of it\n",
        "# Often for vertical comparisons. Here its for comparing the cummulative number of deaths across different countries\n",
        "# Offer a critical perspective on the the most severely affected in terms of fatalities, by geographic distribution\n",
        "total_deaths = df.groupby('Country_Region')['death'].max().sort_values(ascending=False).head(15)\n",
        "\n",
        "plt.figure(figsize=(14, 7))\n",
        "sns.barplot(x=total_deaths.index, y=total_deaths.values, palette='plasma')\n",
        "plt.title('Top 15 Countries/Regions by Total COVID-19 Deaths')\n",
        "plt.xlabel('Country/Region')\n",
        "plt.ylabel('Total Deaths')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88PZV6IMAFom"
      },
      "source": [
        "### Observation\n",
        "\n",
        "This chart is essentially a bar chart where the height of each bar represents the cumulative total number of COVID-19 deaths for a specific country or region, sorted in descending order. Here's what it tells us:\n",
        "\n",
        "Striking Impact in the United States: U.S. recorded a significantly higher number of total COVID-19 deaths compared to any other country listed, highlighting it as the region with the most severe mortality impact.\n",
        "\n",
        "Key Affected Nations: Following the United States, countries like Italy, the United Kingdom, Canada, and Belgium show substantial, though considerably lower, death tolls.\n",
        "\n",
        "Distribution of Impact: The burden of fatalities is not evenly distributed. A few countries account for a large proportion of the total deaths among the top 15, with a 'long tail' of other nations experiencing progressively fewer cumulative deaths."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d5fae12"
      },
      "source": [
        "### Distribution Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9f36c314"
      },
      "outputs": [],
      "source": [
        "# distributionplot visualize shape of the data\n",
        "# Histogram and Kernel Density Estimate (KDE) curve identifies central tendency, spread, and most importantly skewness\n",
        "# Skewness indicates the asymmetry of the probability distribution of a real-valued random variable about its mean.\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.histplot(df['daily_positive'].dropna(), kde=True, color='skyblue', bins=50) # Drop NaNs for plotting\n",
        "plt.title('Distribution of Daily Positive COVID-19 Cases')\n",
        "plt.xlabel('Daily Positive Cases')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.show()\n",
        "\n",
        "# Calculate skewness to quantify it\n",
        "skewness_value = df['daily_positive'].skew()\n",
        "print(f\"Skewness of Daily Positive Cases: {skewness_value:.2f}\")\n",
        "\n",
        "if skewness_value > 0.5:\n",
        "    skew_interpretation = \"The data is positively (right) skewed, meaning the tail of the distribution is longer on the right side and there are more values concentrated on the left. This indicates a large number of days with low daily positive cases and fewer days with very high numbers.\"\n",
        "elif skewness_value < -0.5:\n",
        "    skew_interpretation = \"The data is negatively (left) skewed, meaning the tail of the distribution is longer on the left side and there are more values concentrated on the right. This suggests a large number of days with high daily positive cases and fewer days with very low numbers.\"\n",
        "else:\n",
        "    skew_interpretation = \"The data is relatively symmetrical or only slightly skewed.\"\n",
        "\n",
        "print(skew_interpretation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqcXVfyk03CR"
      },
      "source": [
        "### Observation:\n",
        "\n",
        "The plot clearly shows that the data is positively (right) skewed. This means a large number of days had relatively low daily positive cases, while a smaller number of days experienced very high daily positive case counts, pulling the tail of the distribution to the right.\n",
        "\n",
        "To quantify this, the calculated skewness value is 11.72, which is significantly greater than 0.5, confirming the strong positive skew.\n",
        "\n",
        "This interpretation indicates that: The data is positively (right) skewed, meaning the tail of the distribution is longer on the right side and there are more values concentrated on the left. This indicates a large number of days with low daily positive cases and fewer days with very high numbers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_x8GbxIE0v8"
      },
      "source": [
        "The main time series models used for forecasting the infectious diseases are auto regressive time series models like AR (Auto Regressive), MA (Moving Average), ARMA (Auto Regressive Moving Average), ARIMA (Auto Regressive Integrated Moving Average), and SARIMA (Seasonal Auto Regressive Integrated Moving Average)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ee27e1f7"
      },
      "source": [
        "### Map\n",
        "\n",
        "We aggregate the DataFrame df by Country_Region to find the maximum 'positive' cases for each country, representing the cumulative total. Then, we generate an interactive choropleth map using `plotly.express` to visualize these cumulative positive cases globally. Finally, we summarize the key geographical insights gained from the map, identifying countries with high case numbers and overall global patterns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c209c07"
      },
      "source": [
        "### Aggregate Data for Map\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e84eaf2f"
      },
      "outputs": [],
      "source": [
        "#Aggregate data for map by country_Region and application of max() function\n",
        "total_positive_cases_map = df.groupby('Country_Region')['positive'].max() #Aggregate dataframe to calculate maximum number of positive cases per country\n",
        "print(\"Total positive cases per country for mapping:\\n\", total_positive_cases_map.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9bc1312"
      },
      "source": [
        "Now that the data for the choropleth map has been aggregated, we will generate an interactive choropleth map using `plotly.express to visualize the cumulative positive cases globally.` Before plotting, `we convert the 'total_positive_cases_map' Series to a DataFrame, as plotly choropleth expects a DataFrame as input.` Additionally, we `fill any NaN values in the 'positive' column with 0 before plotting` to ensure they are handled correctly on the map.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e713e04f"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "\n",
        "# Convert Series to DataFrame and rename column for better readability in plotly\n",
        "total_positive_cases_map_df = total_positive_cases_map.reset_index()\n",
        "total_positive_cases_map_df.columns = ['Country_Region', 'Positive Cases']\n",
        "\n",
        "# Handle NaN values for countries where max positive cases might be NaN (no data)\n",
        "total_positive_cases_map_df['Positive Cases'] = total_positive_cases_map_df['Positive Cases'].fillna(0)\n",
        "\n",
        "# Create the choropleth map\n",
        "fig = px.choropleth(\n",
        "    total_positive_cases_map_df,\n",
        "    locations=\"Country_Region\",\n",
        "    locationmode=\"country names\",\n",
        "    color=\"Positive Cases\",\n",
        "    hover_name=\"Country_Region\",\n",
        "    color_continuous_scale=px.colors.sequential.Viridis,  # color scale to Viridis\n",
        "    title=\"Cumulative Positive COVID-19 Cases by Country\"\n",
        ")\n",
        "\n",
        "fig.show()\n",
        "print(\"Interactive choropleth map generated with a new color scale.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfxfcqjeX76l"
      },
      "source": [
        "### Observation (understanding the overall epidemiological landscape)\n",
        "The key geographical insights from the choropleth map are:\n",
        "*   The United States shows the highest number of cumulative positive cases, indicating a dominant impact in North America.\n",
        "*   Brazil in South America also exhibits a substantial number of cases.\n",
        "*   Several countries in Western Europe display high cumulative positive cases.\n",
        "*   Asia shows a mixed impact across its regions. Countries like India (if present and with significant data) would typically show high numbers, but some regions might have lower reported cumulative cases. It's important to note the specific data availability for each country.\n",
        "*   Many countries in Africa generally appear with lower reported cumulative positive cases. Many countries in Africa appear with lighter shades, indicating lower reported cumulative positive cases. This could be due to various factors including testing capacity, data reporting, or actual lower prevalence at the time the data was collected.\n",
        "*   The overall global pattern reveals that while the pandemic was widespread, its intensity varied significantly by region and country.\n",
        "\n",
        "The significant variation in case numbers across continents and specific countries highlights the need for region-specific public health strategies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVzdqeKSzMm0"
      },
      "source": [
        "## Outlier Detection\n",
        "\n",
        "###Should detected outliers be reomoved or not?\n",
        "\n",
        " Outlier detection identifies anomalies in datasets without needing to establish dependent and independent variables. It can occur in univariate data, focusing on a single variable, or multivariate data, analyzing complex interactions among multiple variables. In regression analysis, outliers may arise from independent variables (high leverage points) or dependent variables (large residuals). Common outlier detection methods include Visualization Techniques, Interquartile Range (IQR) Method, Z-score Method, and Machine Learning Algorithms like Isolation Forests and Local Outlier Factor. The Mahalanobis Distance method assesses multivariate distribution anomalies.\n",
        "\n",
        " *Addressing outliers before missing values is crucial since they distort mean and standard deviation used for imputation. Eliminating or adjusting outliers leads to more reliable measures for accurate data preparation and imputation of null values.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNh78gdk0QMp"
      },
      "source": [
        "### Choice of Model\n",
        "\n",
        "We need model(s) that is/are useful for predicting pandemics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc7b78cd"
      },
      "source": [
        "### Scatter Plot for Outlier Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9eac42a5"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(14, 7))\n",
        "sns.scatterplot(data=df, x='Date', y='daily_positive', alpha=0.6, s=20) # Removed hue='Country_Region' for a tighter layout\n",
        "plt.title('Daily Positive COVID-19 Cases Over Time (Outlier Visualization)')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Daily Positive Cases')\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "# Removed legend for clearer visualization, as it was causing layout issues with too many entries.\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f7abe4f"
      },
      "source": [
        "### Observation/Interpretation of the Scatter:\n",
        "\n",
        "This generated plot is a **time-series scatter plot**. It displays individual data points of 'daily_positive' cases over time ('Date'). In the context of outlier detection, this scatter plot serves as a **visualization technique** to visually identify data points that deviate significantly from the general trend or cluster of data. Outliers will appear as points far removed from the main body of data.\n",
        "\n",
        "Looking at the scatter plot for 'daily_positive' cases, especially with the different colored lines for countries, we can observe periods where the spread of cases is much wider than others. For example, during certain peaks, the range of 'daily_positive' cases is very large, while at other times, it's quite narrow. This visual variation in spread suggests the presence of **heteroscedasticity** in the time series of 'daily_positive' cases. This is common in real-world time-series data, especially with phenomena like pandemics where variability can change significantly over time.\n",
        "\n",
        "In summary, this is a **time-series scatter plot used for outlier visualization**, and its visual characteristics suggest **heteroscedasticity** if one were to model the variance of the daily positive cases over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17f34c68"
      },
      "outputs": [],
      "source": [
        "columns_to_handle = ['positive', 'active', 'hospitalized', 'hospitalizedCurr', 'recovered', 'death', 'total_tested', 'daily_tested', 'daily_positive']\n",
        "print(\"Re-initialized 'columns_to_handle' list with specified numerical columns.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32311d55"
      },
      "source": [
        "**Action**:\n",
        "The subtask requires applying `sklearn.preprocessing.RobustScaler` to handle outliers in the numerical columns, replacing the IQR capping method that was used in another project.This robust scaling method is less sensitive to outliers than `MinMaxScaler` and will prepare the data for subsequent steps.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2ab2aaf"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "# Create a copy of the DataFrame to apply RobustScaler\n",
        "df_scaled_robust = df.copy()\n",
        "\n",
        "# Initialize RobustScaler\n",
        "robust_scaler = RobustScaler()\n",
        "\n",
        "# Apply RobustScaler to the specified numerical columns\n",
        "df_scaled_robust[columns_to_handle] = robust_scaler.fit_transform(df_scaled_robust[columns_to_handle])\n",
        "\n",
        "print(\"Numerical features have been scaled using RobustScaler to handle outliers.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b285c689"
      },
      "source": [
        "**Action**:\n",
        "Now that the numerical features have been scaled using RobustScaler, we will address any remaining missing values. We first sort the DataFrame by 'Date' and 'Country_Region' to ensure proper chronological order and group-wise filling, then apply forward-fill (`ffill()`) for each `Country_Region` to propagate previous valid observations forward.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bca4818c"
      },
      "outputs": [],
      "source": [
        "df_scaled_robust = df_scaled_robust.sort_values(by=['Date', 'Country_Region'])\n",
        "\n",
        "# Apply forward-fill grouped by Country_Region\n",
        "for col in columns_to_handle:\n",
        "    df_scaled_robust[col] = df_scaled_robust.groupby('Country_Region')[col].ffill()\n",
        "\n",
        "print(\"DataFrame sorted by Date and Country_Region and forward-filled missing values grouped by country.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac087877"
      },
      "source": [
        "**Action**:\n",
        "We will now apply median imputation for any remaining missing values in the numerical columns of the scaled DataFrame, as this is a robust method to fill NaNs without being heavily influenced by outliers, which have already been addressed by RobustScaler.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9a0f2288"
      },
      "outputs": [],
      "source": [
        "for col in columns_to_handle:\n",
        "    if df_scaled_robust[col].isnull().any():\n",
        "        median_val = df_scaled_robust[col].median()\n",
        "        df_scaled_robust[col] = df_scaled_robust[col].fillna(median_val)\n",
        "        print(f\"Median imputation applied for remaining missing values in column '{col}'.\")\n",
        "\n",
        "print(\"All specified numerical columns have had remaining missing values imputed using their median.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a8cd912"
      },
      "source": [
        "**Action**:\n",
        "Now that missing values have been handled, I will set the 'Date' column as the DataFrame's index to prepare the data for time-series analysis.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74ac1f42"
      },
      "outputs": [],
      "source": [
        "df_scaled_robust = df_scaled_robust.set_index('Date')\n",
        "print(\"DataFrame's 'Date' column set as index for time-series analysis.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a4c95c9"
      },
      "source": [
        "**Action**:\n",
        "I will normalize the numerical features of the dataset using `MinMaxScaler` to scale them to a common range, which is a standard practice for many machine learning models to prevent features with larger values from dominating the learning process. This is the next logical step as per the subtask instructions, and it follows the previous steps of outlier handling and missing value imputation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1514525e"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Initialize the MinMaxScaler\n",
        "mins_scaler = MinMaxScaler()\n",
        "\n",
        "# Apply MinMaxScaler to the numerical columns in df_scaled_robust\n",
        "df_scaled_robust[columns_to_handle] = mins_scaler.fit_transform(df_scaled_robust[columns_to_handle])\n",
        "\n",
        "print(\"Numerical features in df_scaled_robust have been normalized using MinMaxScaler.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fc82e19"
      },
      "source": [
        "### Scatter Plot: Daily Positive Cases After Robust Scaling and MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "865d806a"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(14, 7))\n",
        "# Reset the index to make 'Date' a regular column for plotting, resolving the reindexing error\n",
        "plot_df = df_scaled_robust.reset_index()\n",
        "sns.scatterplot(data=plot_df, x='Date', y='daily_positive', alpha=0.6, s=20)\n",
        "plt.title('Daily Positive COVID-19 Cases Over Time (After Robust Scaling & MinMaxScaler)')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Daily Positive Cases (Robust Scaled & Normalized)')\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpLmelgflKih"
      },
      "source": [
        "# Selection of relevant features (Feature Selection)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Variance Inflation Factor (VIF)** is used as a feature selection technique or, more accurately, a feature reduction method, primarily to detect and remove multicollinearity (highly correlated independent variables) in regression models, leading to more stable and interpretable models. It helps identify which features are redundant by showing how much a variable's variance is inflated due to its correlation with other predictors, often suggesting removal of variables with high VIF (e.g., >5 or >10)."
      ],
      "metadata": {
        "id": "xQkgt7PKEL2u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "939f466b"
      },
      "outputs": [],
      "source": [
        "# Import the necessary library for VIF calculation\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "# Prepare the data for VIF calculation\n",
        "# Drop non-numeric columns and any remaining NaNs for VIF calculation\n",
        "X = df_scaled_robust[columns_to_handle].dropna()\n",
        "\n",
        "# Calculate VIF for each feature\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"feature\"] = X.columns\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))]\n",
        "\n",
        "# Display the VIF values\n",
        "print(\"Variance Inflation Factor (VIF) for each feature:\")\n",
        "print(vif_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a0e802c"
      },
      "source": [
        "### Interpretation of VIF Values\n",
        "\n",
        "- **VIF = 1**: No correlation between the predictor and the other predictors.\n",
        "- **1 < VIF < 5**: Moderate correlation, which is generally considered acceptable.\n",
        "- **VIF > 5-10**: High correlation, indicating potential multicollinearity that might require further investigation or variable selection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JINeOjtEqj4d"
      },
      "source": [
        "We need to drop a correlated feature that has great potential to lead to overfitting and inaccurate performance of my model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c9b67c9"
      },
      "source": [
        "**Action**:\n",
        "To accurately assess multicollinearity, we will define the target variable `y` and the independent features `X` as specified, ensuring `X` only contains numerical columns relevant for VIF calculation and is free of missing values. Then, I will recalculate the VIF for these selected features.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fb2caa44"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "# 1. Define the target variable y\n",
        "y = df_scaled_robust['daily_positive']\n",
        "\n",
        "# 2. Define the independent features X\n",
        "# Select all numerical columns from df_scaled_robust\n",
        "numerical_cols = df_scaled_robust.select_dtypes(include=np.number).columns.tolist()\n",
        "\n",
        "# Remove 'daily_positive' from the list of numerical columns to get independent features\n",
        "independent_features_for_vif = [col for col in numerical_cols if col != 'daily_positive']\n",
        "X = df_scaled_robust[independent_features_for_vif].copy()\n",
        "\n",
        "# Ensure X is free of NaN values before VIF calculation\n",
        "X = X.dropna()\n",
        "\n",
        "# 3. Recalculate VIF for each feature in the newly defined X\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"feature\"] = X.columns\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))]\n",
        "\n",
        "# 4. Display the VIF values\n",
        "print(\"Variance Inflation Factor (VIF) for each feature in the updated independent features:\")\n",
        "print(vif_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e04b956f"
      },
      "source": [
        "**Action**:\n",
        "Based on the latest VIF calculation, the feature with the highest Variance Inflation Factor is 'positive' with a VIF of 15.09. This value is well above the common threshold of 5 or 10, indicating high multicollinearity and making it a strong candidate for removal if the goal is to reduce multicollinearity.Therefore, we will remove 'total_tested' from the independent features and then recalculate VIFs to see the impact.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dd10391c"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
        "\n",
        "\n",
        "# Define columns_to_handle if not already in scope\n",
        "columns_to_handle = ['positive', 'active', 'hospitalized', 'hospitalizedCurr', 'recovered', 'death', 'total_tested', 'daily_tested', 'daily_positive']\n",
        "\n",
        "# Create a copy of the DataFrame to apply RobustScaler\n",
        "df_scaled_robust = df.copy()\n",
        "\n",
        "# Initialize RobustScaler and apply it\n",
        "robust_scaler = RobustScaler()\n",
        "df_scaled_robust[columns_to_handle] = robust_scaler.fit_transform(df_scaled_robust[columns_to_handle])\n",
        "\n",
        "# Sort and apply forward-fill grouped by Country_Region\n",
        "df_scaled_robust = df_scaled_robust.sort_values(by=['Date', 'Country_Region'])\n",
        "for col in columns_to_handle:\n",
        "    df_scaled_robust[col] = df_scaled_robust.groupby('Country_Region')[col].ffill()\n",
        "\n",
        "# Apply median imputation for remaining missing values\n",
        "for col in columns_to_handle:\n",
        "    if df_scaled_robust[col].isnull().any():\n",
        "        median_val = df_scaled_robust[col].median()\n",
        "        df_scaled_robust[col] = df_scaled_robust[col].fillna(median_val)\n",
        "\n",
        "# Set 'Date' column as index\n",
        "df_scaled_robust = df_scaled_robust.set_index('Date')\n",
        "\n",
        "# Apply MinMaxScaler\n",
        "mins_scaler = MinMaxScaler()\n",
        "df_scaled_robust[columns_to_handle] = mins_scaler.fit_transform(df_scaled_robust[columns_to_handle])\n",
        "# --- End of re-executed preprocessing steps ---\n",
        "\n",
        "\n",
        "# 1. Define the target variable y\n",
        "y = df_scaled_robust['daily_positive']\n",
        "\n",
        "# 2. Define the independent features X by excluding 'daily_positive' and 'positive'\n",
        "numerical_cols = df_scaled_robust.select_dtypes(include=np.number).columns.tolist()\n",
        "independent_features_for_vif = [col for col in numerical_cols if col not in ['daily_positive', 'positive']]\n",
        "X_after_drop = df_scaled_robust[independent_features_for_vif].copy()\n",
        "\n",
        "# Ensure X is free of NaN values before VIF calculation\n",
        "X_after_drop = X_after_drop.dropna()\n",
        "\n",
        "# 3. Recalculate VIF for each feature in the modified X\n",
        "vif_data_after_drop = pd.DataFrame()\n",
        "vif_data_after_drop[\"feature\"] = X_after_drop.columns\n",
        "vif_data_after_drop[\"VIF\"] = [variance_inflation_factor(X_after_drop.values, i) for i in range(len(X_after_drop.columns))]\n",
        "\n",
        "# 4. Display the updated VIF values\n",
        "print(\"Variance Inflation Factor (VIF) for features after dropping 'positive':\")\n",
        "print(vif_data_after_drop)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After dropping the 'positive' feature, the Variance Inflation Factors for all remaining independent features are now well below the common threshold of 5 (or even 10), as we can see:\n",
        "\n",
        "active: 1.99\n",
        "\n",
        "hospitalized: 1.89\n",
        "\n",
        "hospitalizedCurr: 1.23\n",
        "\n",
        "recovered: 1.32\n",
        "\n",
        "death: 2.76\n",
        "\n",
        "total_tested: 2.70\n",
        "\n",
        "daily_tested: 1.37\n",
        "\n",
        "This indicates that multicollinearity is **no longer** a significant issue among these features, which is a good outcome for the stability and interpretability of regression models."
      ],
      "metadata": {
        "id": "uVterZCEKnUq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVQw-MI4tFOl"
      },
      "source": [
        "### Filter Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Conditional Mutual Information (CMIM)**\n",
        "\n",
        "We will employ the CMIM technique for filtering because of its distinct capacity to choose features according to their significance to the target variable while also taking into account the redundancy among features. CMIM effectively identifies a concise subset of features by iteratively choosing those that enhance mutual information with the target variable while ensuring conditional independence from previously chosen features, thereby capturing essential predictive information without redundancy.\n"
      ],
      "metadata": {
        "id": "rqmsa08lPHBk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ccfac11"
      },
      "source": [
        "\n",
        "We will install the `scikit-feature` library, then apply the CMIM feature selection method using `skfeature.function.information_theoretical_based.CMIM` on the independent features `X_after_drop` and the dependent feature `y`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1ed94f0"
      },
      "source": [
        "The `scikit-feature` library provides an implementation of the CMIM algorithm for feature selection. This is a prerequisite for applying the CMIM method.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5e43ff3"
      },
      "source": [
        "We will use `pip install` in a code block to perform the installation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3ca8ff6"
      },
      "source": [
        "We will now try installing the package under the name 'skfeature-chappers', which is the common distribution for the 'scikit-feature' library.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5cc8969b"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install skfeature-chappers\n",
        "print(\"Attempted to install skfeature-chappers library.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9c4bf4b"
      },
      "source": [
        "\n",
        "Since the `skfeature-chappers` library has been successfully installed, the next logical step is to apply the CMIM feature selection method as outlined in the main task. This involves importing the `cmim` function and using it with the prepared independent features (`X_after_drop`) and the dependent feature (`y`).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cf556718"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from skfeature.function.information_theoretical_based import CMIM\n",
        "\n",
        "# Define the target variable y\n",
        "y = df_scaled_robust['daily_positive']\n",
        "\n",
        "# Define the independent features X by excluding 'daily_positive' and 'total_tested'\n",
        "numerical_cols = df_scaled_robust.select_dtypes(include=np.number).columns.tolist()\n",
        "independent_features_after_drop = [col for col in numerical_cols if col not in ['daily_positive', 'total_tested']]\n",
        "X_after_drop = df_scaled_robust[independent_features_after_drop].copy()\n",
        "\n",
        "# Ensure X is free of NaN values before VIF calculation\n",
        "X_after_drop = X_after_drop.dropna()\n",
        "\n",
        "# Ensure X_after_drop and y are in the correct format (numpy arrays)\n",
        "X_cmim = X_after_drop.values\n",
        "y_cmim = y.values\n",
        "\n",
        "# Apply CMIM feature selection. k specifies the number of features to select.\n",
        "# For now, let's get scores for all features. The returned 'idx' are the feature indices in descending order of importance.\n",
        "num_features = X_cmim.shape[1]\n",
        "idx = CMIM.cmim(X_cmim, y_cmim, n_selected_features=num_features)\n",
        "\n",
        "# CMIM.cmim returns a numpy array of indices, not a tuple (idx, score).\n",
        "# Convert idx to a flat list if it's nested (which it can be depending on skfeature version/config)\n",
        "if isinstance(idx, np.ndarray) and idx.ndim > 1:\n",
        "    idx = idx.flatten()\n",
        "\n",
        "# Get feature names based on the original columns of X_after_drop\n",
        "cmim_ranked_features = X_after_drop.columns[idx].tolist()\n",
        "\n",
        "print(\"Features ranked by CMIM (descending order of importance):\")\n",
        "for i, feature in enumerate(cmim_ranked_features):\n",
        "    print(f\"{i+1}. {feature}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2-iN061w3dI"
      },
      "source": [
        "We would have also implemented CMIM filter method using the mlr3filters package for its functionality, support for FS techniques, reproducibility, and transparency.\n",
        "\n",
        "However, the mlr3filters package is designed for R, and we're currently operating in a Python environment, which prevents us from using R packages directly. We have already successfully applied the CMIM filter method using the skfeature-chappers Python library, which provides similar functionality for feature selection based on mutual information."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Is CMIM at a 5% significane level?**\n",
        "\n",
        "It's important to clarify that CMIM (Conditional Mutual Information Maximization) does not inherently operate at a '5% significance level' in the same way that statistical tests like ANOVA or OLS regression (used in SBE) do.\n",
        "\n",
        "Here's why:\n",
        "\n",
        "CMIM is an information-theoretical filter method: Instead of calculating p-values to determine statistical significance, CMIM evaluates features based on their mutual information with the target variable, while also considering redundancy among selected features. It ranks features by how much unique information they provide about the target.\n",
        "No p-values involved: The skfeature.function.information_theoretical_based.CMIM function returns feature indices ranked by their importance (how much mutual information they share with the target, conditioned on already selected features). It doesn't output p-values or F-statistics that you would compare against a 0.05 (5%) significance threshold.\n",
        "Thresholding is based on desired number of features: When using CMIM, you typically decide how many top-ranked features you want to select (e.g., n_selected_features parameter). This is a practical choice for model building, not a statistical significance cutoff.\n",
        "So, while CMIM is an effective way to select relevant features, it uses a different mechanism than methods based on statistical hypothesis testing."
      ],
      "metadata": {
        "id": "3dw5Vw4CpIf_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Correlation Filter**\n",
        "\n",
        "Let’s now employ the Correlation filter method because of its straightforward and intuitive approach in recognizing features that are closely linked to the target variable. Calculating correlation coefficients for numeric and categorical variables allows us to identify linear and non-linear relationships that are useful for predictive modeling. **Applying a 0.2 threshold for feature selection** emphasizes features with moderate to strong correlation to the target variable, which may improve our model's predictive performance. This approach is selected for its ease and clarity, allowing for a straightforward FS procedure.\n"
      ],
      "metadata": {
        "id": "APKxaxC-PpA1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abf78194"
      },
      "source": [
        "We select features that have an absolute Pearson correlation coefficient greater than 0.2 with the `daily_positive` target variable, leveraging the already calculated correlation matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d89190e9"
      },
      "source": [
        "\n",
        "We will calculate the Pearson correlation coefficient between each independent numerical feature and the dependent variable (daily_positive). This step will identify features that have a strong linear relationship with the target.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83d83283"
      },
      "source": [
        "**Action**:\n",
        "We need to calculate the Pearson correlation coefficient between each independent numerical feature and the dependent variable ('daily_positive'). We will access the 'daily_positive' row from the already computed `correlation_matrix` and then exclude the correlation of 'daily_positive' with itself before printing the results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7877453"
      },
      "outputs": [],
      "source": [
        "correlation_with_target = correlation_matrix['daily_positive'].drop('daily_positive')\n",
        "print(\"Pearson correlation with 'daily_positive':\")\n",
        "print(correlation_with_target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91b53645"
      },
      "source": [
        "**Action**:\n",
        "Now that the Pearson correlation coefficients between the independent features and 'daily_positive' have been calculated, I will filter these features to select only those with an absolute correlation coefficient greater than 0.2, as specified in the main task. This step helps in identifying features with a strong linear relationship with the target variable.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2c812506"
      },
      "outputs": [],
      "source": [
        "selected_features_correlation = correlation_with_target[abs(correlation_with_target) > 0.2].index.tolist()\n",
        "print(\"Features with absolute Pearson correlation > 0.2 with 'daily_positive':\")\n",
        "print(selected_features_correlation)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Embedded approach: Random Forest utilizing Mean Decrease Gini (MDG)**\n",
        "\n",
        "Mean Decrease Gini (MDG) is a feature selection technique rooted in Random Forest algorithms. It functions by assessing the significance of each attribute in the Random Forest model by calculating the reduction in Gini impurity that each attribute induces when utilized to partition the data. The Gini impurity quantifies the likelihood that a randomly selected element from the set would be misclassified if it were labeled at random based on the distribution of labels within the subset. The greater the reduction in Gini impurity, the more significant the feature is deemed.\n",
        "\n",
        "MDG is selected as a strong and dependable FS technique capable of managing high-dimensional data and identifying non-linear associations between the features and the target variable.\n",
        "\n",
        "Preparing data for Random Forest included normalizing the data, forming a 70%/30% train/test division, tackling dataset imbalance, and equalizing the data. The ideal number of random variables (mtry) was identified by constructing RF classifiers with different quantities of trees (ntree) and observing where the out-of-bag error rate became stable. The most significant variables ought to be determined by employing the mean decrease in node impurity approach.\n"
      ],
      "metadata": {
        "id": "mTzrTImTS__f"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5c0f6867"
      },
      "source": [
        "### Prepare_Data_for_Random_Forest_and_MDG\n",
        "\n",
        "We define `X` (independent features excluding `daily_positive`) and `y` (`daily_positive`) from `df_scaled_robust`. Next, we standardize the `X` data using `StandardScaler`.\n",
        "\n",
        "We split the standardized data next, into training and testing sets using a 70%/30% chronological split, as requested for Random Forest data preparation.\n",
        "\n",
        "Ensure the data is balanced if necessary, though for time series regression, this typically involves handling extreme values, which have been addressed during preprocessing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a1f3247"
      },
      "source": [
        "**Action**:\n",
        "We will define the independent features (X) by selecting all numerical columns from `df_scaled_robust` except for the 'daily_positive' column, and then define the dependent variable (y) as the 'daily_positive' column from the same DataFrame. We will also ensure X is sorted by its index as `df_scaled_robust` is already indexed by Date.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cd21b34b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 1. Define the independent features (X)\n",
        "# Ensure 'df_scaled_robust' is already indexed by 'Date' and sorted as per previous steps\n",
        "independent_features = [col for col in df_scaled_robust.select_dtypes(include=np.number).columns if col != 'daily_positive']\n",
        "X = df_scaled_robust[independent_features].copy()\n",
        "\n",
        "# 2. Define the dependent variable (y)\n",
        "y = df_scaled_robust['daily_positive']\n",
        "\n",
        "print(\"Independent features (X) and Dependent variable (y) defined.\")\n",
        "print(f\"Shape of X: {X.shape}\")\n",
        "print(f\"Shape of y: {y.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2b63c01"
      },
      "source": [
        "**Action**:\n",
        "Now that `X` and `y` are defined, we will standardize `X` using `StandardScaler` and then perform a chronological 70%/30% split for training and testing sets, finally printing the shapes to verify.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3e843a9"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 3. Initialize StandardScaler and apply it to X\n",
        "scaler = StandardScaler()\n",
        "X_scaled_array = scaler.fit_transform(X)\n",
        "\n",
        "# Convert back to DataFrame to retain column names and index\n",
        "X_scaled = pd.DataFrame(X_scaled_array, columns=X.columns, index=X.index)\n",
        "\n",
        "# 5. Determine the size of the training set (70%)\n",
        "train_size = int(len(X_scaled) * 0.7)\n",
        "\n",
        "# 6. Split the standardized independent features into training and testing sets\n",
        "X_train_scaled, X_test_scaled = X_scaled.iloc[:train_size], X_scaled.iloc[train_size:]\n",
        "\n",
        "# 7. Split the dependent variable y into training and testing sets\n",
        "y_train, y_test = y.iloc[:train_size], y.iloc[train_size:]\n",
        "\n",
        "# 8. Print the shapes to verify the splits\n",
        "print(\"Shapes after standardization and splitting:\")\n",
        "print(f\"X_train_scaled shape: {X_train_scaled.shape}\")\n",
        "print(f\"X_test_scaled shape: {X_test_scaled.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cd635fc"
      },
      "source": [
        "### Determine_Optimal_Random_Forest_Hyperparameters\n",
        "\n",
        "Build Random Forest regressors (since `daily_positive` is numerical) with varying numbers of trees (`ntree`) and different numbers of features considered at each split (`mtry`).\n",
        "\n",
        "Evaluate the out-of-bag (OOB) error rate to find the optimal `mtry` where the OOB error stabilizes. This step is crucial for configuring an effective Random Forest model for feature importance calculation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cb9616e"
      },
      "source": [
        "**Action**:\n",
        "I need to build Random Forest regressors with varying `max_features` (mtry) values and evaluate their out-of-bag (OOB) error rates to find the optimal `mtry` that stabilizes the OOB error. This involves importing `RandomForestRegressor`, defining a range for `mtry`, and iterating through it to fit models and collect OOB scores.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1e10a8d"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the range of mtry values. A common strategy is to go up to the number of features.\n",
        "# X_train_scaled.shape[1] gives the total number of features.\n",
        "num_features = X_train_scaled.shape[1]\n",
        "mtry_values = list(range(1, num_features + 1))\n",
        "\n",
        "# Initialize an empty list to store OOB errors\n",
        "oob_errors = []\n",
        "\n",
        "# Fixed parameters for RandomForestRegressor\n",
        "n_estimators = 100\n",
        "random_state = 42\n",
        "\n",
        "print(f\"Evaluating OOB error for mtry values from 1 to {num_features}...\")\n",
        "\n",
        "for mtry in mtry_values:\n",
        "    # Instantiate RandomForestRegressor with current mtry (max_features) and oob_score=True\n",
        "    rf_regressor = RandomForestRegressor(\n",
        "        n_estimators=n_estimators,\n",
        "        max_features=mtry,\n",
        "        oob_score=True,  # Enable out-of-bag error calculation\n",
        "        random_state=random_state,\n",
        "        n_jobs=-1 # Use all available cores\n",
        "    )\n",
        "\n",
        "    # Fit the regressor to the training data\n",
        "    rf_regressor.fit(X_train_scaled, y_train)\n",
        "\n",
        "    # Record the OOB error (1 - oob_score)\n",
        "    oob_error = 1 - rf_regressor.oob_score_\n",
        "    oob_errors.append(oob_error)\n",
        "    print(f\"mtry={mtry}, OOB Error={oob_error:.4f}\")\n",
        "\n",
        "# Plot the OOB error rates against the mtry values\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(mtry_values, oob_errors, marker='o', linestyle='-')\n",
        "plt.title('Out-of-Bag Error vs. max_features (mtry)')\n",
        "plt.xlabel('max_features (mtry)')\n",
        "plt.ylabel('Out-of-Bag Error Rate')\n",
        "plt.xticks(mtry_values) # Ensure all mtry values are shown on x-axis\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Find the optimal mtry value\n",
        "optimal_mtry = mtry_values[oob_errors.index(min(oob_errors))]\n",
        "print(f\"\\nOptimal mtry value based on lowest OOB error: {optimal_mtry}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This graph, titled 'Out-of-Bag Error vs. max_features (mtry)' (inline_data_9), is crucial for tuning Random Forest models. It visualizes how the model's performance, as measured by the Out-of-Bag (OOB) error rate, changes with different settings for the **max_features hyperparameter**, also known as **mtry**.\n",
        "\n",
        "**What is Out-of-Bag (OOB) Error?** In a Random Forest, each tree is trained on a bootstrap sample of the data. This means some data points are left out of each tree's training set; these are the 'out-of-bag' samples. The OOB error is calculated by using these unused samples to evaluate the prediction accuracy of each tree. It's an internal, unbiased estimate of the generalization error, similar to cross-validation but without needing a separate validation set.\n",
        "\n",
        "**Key Observations from the Graph and Code Output:**\n",
        "\n",
        "**Trend of OOB Error:** As max_features (mtry) increases from 1, the OOB error initially decreases, reaching its lowest point when mtry=2 (0.0216). After this point, the OOB error starts to increase progressively as mtry continues to grow, peaking at mtry=8 (0.0314).\n",
        "\n",
        "mtry=1, OOB Error=0.0243\n",
        "mtry=2, OOB Error=0.0216 (Lowest Error)\n",
        "mtry=3, OOB Error=0.0224\n",
        "mtry=4, OOB Error=0.0238\n",
        "mtry=5, OOB Error=0.0253\n",
        "mtry=6, OOB Error=0.0264\n",
        "mtry=7, OOB Error=0.0285\n",
        "mtry=8, OOB Error=0.0314\n",
        "\n",
        "**Optimal mtry Value:** The plot clearly shows that the optimal mtry value is 2, as this is where the Random Forest Regressor achieves the lowest OOB error rate (0.0216). This means that when the Random Forest considers 2 features at each split, it generalizes best to unseen data.\n",
        "\n",
        "**Overall Interpretation:**\n",
        "\n",
        "The max_features parameter controls the number of features the Random Forest considers when looking for the best split at each node. Choosing an optimal mtry is crucial because:\n",
        "\n",
        "**Too Low mtry:** If mtry is too low (e.g., 1 in this case, initially higher error than 2), the trees might be too simple, leading to high bias and underfitting, as they don't have enough options to find good splits.\n",
        "\n",
        "**Too High mtry:** If mtry is too high (e.g., 8 in this case, with increasing error), the trees become more correlated with each other, reducing the benefits of ensemble averaging and increasing the risk of overfitting to the training data, thus leading to higher OOB error on unseen data.\n",
        "\n",
        "**Optimal mtry:** The optimal value of 2 indicates a good balance between bias and variance for this dataset, leading to the most robust and accurate Random Forest model for subsequent feature importance calculations and predictions."
      ],
      "metadata": {
        "id": "HBqLNCKXWnGX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f09b6c03"
      },
      "source": [
        "## Apply_MDG_Feature_Selection\n",
        "\n",
        "Train a Random Forest regressor using the optimal hyperparameters determined in the previous step. Extract feature importances based on the Mean Decrease Gini (MDG) / Mean Decrease in Node Impurity method. This will provide a ranking of features based on their contribution to reducing impurity.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8c34e7c"
      },
      "source": [
        "**Action**:\n",
        "To extract feature importances using the Mean Decrease Gini (MDG) method, we will first instantiate a RandomForestRegressor with the optimal hyperparameters determined previously, then fit it to the training data, and finally extract and display the feature importances.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7aa6a5f2"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Instantiate RandomForestRegressor with optimal hyperparameters\n",
        "# 'optimal_mtry' was determined in the previous step\n",
        "rf_regressor_optimal = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    max_features=optimal_mtry, # Use the optimal mtry found\n",
        "    oob_score=True,\n",
        "    random_state=42,\n",
        "    n_jobs=-1 # Use all available cores\n",
        ")\n",
        "\n",
        "# 2. Fit the regressor to the training data\n",
        "rf_regressor_optimal.fit(X_train_scaled, y_train)\n",
        "\n",
        "# 3. Extract the feature_importances_ from the fitted model\n",
        "feature_importances = rf_regressor_optimal.feature_importances_\n",
        "\n",
        "# 4. Create a Pandas Series to store the feature names along with their importance scores\n",
        "# Ensure the feature names are from X_train_scaled.columns\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'feature': X_train_scaled.columns,\n",
        "    'importance': feature_importances\n",
        "})\n",
        "\n",
        "# Sort in descending order of importance\n",
        "feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)\n",
        "\n",
        "# 5. Print the feature importance scores\n",
        "print(\"Feature Importances (Mean Decrease Gini):\")\n",
        "print(feature_importance_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Just like CMIM, Mean Decrease Gini (MDG) does not operate at a '5% significance level'.**\n",
        "\n",
        "Here's why:\n",
        "\n",
        "MDG is a Feature Importance Metric: MDG is an intrinsic feature of tree-based models like Random Forests. It quantifies how much each feature contributes to the homogeneity (or 'purity') of the nodes in the decision trees. When a feature is used to split a node, the decrease in Gini impurity (for classification) or variance reduction (for regression) is calculated. This decrease is averaged across all trees in the forest and weighted by the number of samples each split handles.\n",
        "\n",
        "**No P-values or Hypothesis Testing:** MDG provides a score for each feature, indicating its relative importance in the model. A higher score means the feature is more impactful in reducing impurity and thus in making predictions. However, this score is not associated with a p-value, nor is it used in hypothesis testing to determine statistical significance against a null hypothesis (like whether a feature's coefficient is significantly different from zero, as in linear regression).\n",
        "\n",
        "**Relative Importance, Not Statistical Significance:** MDG tells you which features the model relies on most, and how much it relies on them, in a relative sense. It doesn't tell you if a feature's contribution is statistically significant at a given confidence level. For that, you would typically need to employ methods like statistical tests (e.g., ANOVA, as used in the ABL method) or permutation importance with statistical testing.\n",
        "\n",
        "\n",
        "So, while MDG is a very valuable tool for understanding feature relevance within a Random Forest model, it doesn't come with a '5% significance level' in the statistical sense."
      ],
      "metadata": {
        "id": "1OFHvPuoqfmO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Neither CMIM nor MDG (Mean Decrease Gini) create a unique dataset to work with in the sense of generating entirely new data points or transforming the entire dataset into a new structure. Instead, they are feature selection techniques that operate on your existing dataset.*\n",
        "\n",
        "Here's how they function in relation to your data:\n",
        "\n",
        "They identify a subset of features: Both CMIM and MDG analyze your existing features (the columns in your X_train_scaled DataFrame in our case) and the target variable (y_train). Their goal is to identify which of these existing features are most relevant or important for predicting the target.\n",
        "\n",
        "They output a ranking or selection of original features: After applying these methods, you get a list or an index of the original features that are considered important. For example, with CMIM, we got a ranked list of features like 'daily_tested', 'positive', etc. With MDG, we got a DataFrame showing the importance scores for each of our existing features.\n",
        "\n",
        "You then use this subset for modeling: Once you have identified the selected features, you would then create a new version of your dataset (or a subset of your original X_train_scaled) that only includes these chosen features. This reduced dataset is what you would then feed into your machine learning models for training and prediction.\n",
        "\n",
        "So, while they don't create a 'unique dataset' from scratch, they do help you define a 'unique feature set' from your existing data, which is then used to form a more streamlined dataset for your actual predictive models."
      ],
      "metadata": {
        "id": "Ta-2zg5qr7a5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:**\n",
        "\n",
        "We would typically create a \"new version\" of your dataset manually. However, it's not about creating entirely new data points, but rather selecting a subset of your existing columns (features).\n",
        "\n",
        "Here's how you'd usually do it programmatically after using methods like CMIM or MDG:\n",
        "\n",
        "**Identify Selected Features:** After running CMIM or MDG, you get a list or an array of the names (or indices) of the features deemed most important. For instance, if CMIM returned ['daily_tested', 'positive', 'death'] as the top features.\n",
        "\n",
        "**Filter Your DataFrame:** You would then use this list to select only those columns from your original X_train_scaled DataFrame (or your full X DataFrame if you're preparing the entire dataset for a new split).\n",
        "\n",
        "For example, if selected_features_by_method is your list of chosen features, you'd create your new dataset like this:\n",
        "\n",
        "X_new = X_train_scaled[selected_features_by_method]\n",
        "\n",
        "This X_new DataFrame would be your \"new version\" of the dataset, containing only the features identified as relevant by your chosen selection method. You would then use X_new (and the corresponding y_train) to train your machine learning models."
      ],
      "metadata": {
        "id": "FxctNSd5scj5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hybrid Models"
      ],
      "metadata": {
        "id": "7PAf2SxzYppv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now apply a **Hybrid of ANOVA, backward selection, and Lasso methods referred to as ABL.**\n",
        "\n",
        "ABL is a hybrid approach that combines ANOVA, backward selection, and Lasso techniques to identify significant features.\n",
        "\n",
        "ANOVA is a parametric method that can be applied to a particular dataset attribute to equalize 'class means'. It evaluates the statistically significant differences among class means for each feature by utilizing the F-test statistic and p-value. The foundation of ANOVA is to determine a score for each attribute indicating “how effectively this attribute differentiates between classes” to eliminate irrelevant and undesired features.\n",
        "\n",
        "Sequential backward elimination (SBE) functions by gradually eliminating the least important features from the model until a defined stopping criterion is achieved. It begins with all features present and then eliminates one feature at a time, reassessing the model's performance after each elimination. The purpose of SBE is to lower the dimensionality of the feature space by removing features that minimally affect the model’s predictive ability, which can enhance model interpretability and decrease overfitting.\n",
        "\n",
        "Lasso regularization functions by incorporating a penalty component into the conventional linear regression cost function, which penalizes the magnitude of the regression coefficients. This penalty promotes the shrinkage of regression coefficients for less significant features to zero, thereby achieving feature selection by removing irrelevant variables from the model. Lasso regularization is chosen to carry out both feature selection and regularization at the same time, aiming to minimize overfitting and improve model interpretability by promoting sparsity in the feature space.\n"
      ],
      "metadata": {
        "id": "bZLHDUCoZTOe"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7809f61f"
      },
      "source": [
        "### Apply_ANOVA_Feature_Selection\n",
        "\n",
        "Apply ANOVA (Analysis of Variance) to the independent features (`X_scaled`) and the dependent variable (`y_train`) to identify features that have a statistically significant relationship with the target. Select features based on p-values (e.g., p < 0.05). This step filters out features that do not significantly contribute to predicting the target variable.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f64a5cf8"
      },
      "source": [
        "**Action**:\n",
        "We will import the `f_regression` function, apply it to `X_train_scaled` and `y_train` to get F-statistics and p-values, create a DataFrame with these results, filter for p-values less than 0.05, and then print the selected features sorted by p-value.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1d18dc69"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import f_regression\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Apply f_regression to the training data\n",
        "# f_regression returns F-statistics and p-values\n",
        "f_stats, p_values = f_regression(X_train_scaled, y_train)\n",
        "\n",
        "# 2. Create a Pandas DataFrame to store the feature names, F-statistics, and p-values\n",
        "anova_results = pd.DataFrame({\n",
        "    'feature': X_train_scaled.columns,\n",
        "    'f_statistic': f_stats,\n",
        "    'p_value': p_values\n",
        "})\n",
        "\n",
        "# 3. Filter for features with a p-value less than 0.05\n",
        "selected_features_anova = anova_results[anova_results['p_value'] < 0.05]\n",
        "\n",
        "# 4. Sort by p-value in ascending order\n",
        "selected_features_anova = selected_features_anova.sort_values(by='p_value', ascending=True)\n",
        "\n",
        "# 5. Print the selected features\n",
        "print(\"Features selected by ANOVA (p-value < 0.05):\")\n",
        "print(selected_features_anova)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f654181"
      },
      "source": [
        "## Apply_SBE_Feature_Selection\n",
        "\n",
        "Using the features selected by ANOVA, apply Sequential Backward Elimination (SBE) to further refine the feature set. This involves building an initial model with all ANOVA-selected features and iteratively removing the least significant feature until a specified criterion (e.g., p-value threshold, adjusted R-squared) is met. This helps in reducing multicollinearity and model complexity.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6b3235f"
      },
      "source": [
        "**Action**:\n",
        "We will import the `statsmodels.api` library, extract feature names from `selected_features_anova`, and then implement a loop to perform Sequential Backward Elimination (SBE) by iteratively building OLS models, identifying features with p-values greater than 0.05, and removing them until all remaining features are statistically significant.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1840026"
      },
      "outputs": [],
      "source": [
        "import statsmodels.api as sm\n",
        "\n",
        "# 2. Extract the list of feature names from the selected_features_anova DataFrame\n",
        "current_features = selected_features_anova['feature'].tolist()\n",
        "\n",
        "print(\"Starting Sequential Backward Elimination with features:\", current_features)\n",
        "\n",
        "# 3. Initialize a while loop\n",
        "while len(current_features) > 0:\n",
        "    # 4. Create a temporary DataFrame X_sbe\n",
        "    X_sbe = X_train_scaled[current_features]\n",
        "\n",
        "    # 5. Add a constant column to X_sbe\n",
        "    X_sbe = sm.add_constant(X_sbe)\n",
        "\n",
        "    # 6. Fit an Ordinary Least Squares (OLS) model\n",
        "    model = sm.OLS(y_train, X_sbe).fit()\n",
        "\n",
        "    # 7. Get the p-values for all features from the model's summary\n",
        "    p_values = model.pvalues.drop('const') # Drop 'const' term p-value\n",
        "\n",
        "    # 8. Identify the feature with the highest p-value\n",
        "    if p_values.empty:\n",
        "        break # No more features to check\n",
        "\n",
        "    max_p_value = p_values.max()\n",
        "    feature_to_remove = p_values.idxmax()\n",
        "\n",
        "    # 9. If the highest p-value is greater than 0.05, remove that feature\n",
        "    if max_p_value > 0.05:\n",
        "        current_features.remove(feature_to_remove)\n",
        "        print(f\"Removing '{feature_to_remove}' with p-value: {max_p_value:.4f}\")\n",
        "    # 10. If no feature has a p-value greater than 0.05, break the loop\n",
        "    else:\n",
        "        print(f\"All remaining features have p-values <= 0.05. Stopping SBE.\")\n",
        "        break\n",
        "\n",
        "# 11. Store the final list of selected features\n",
        "sbe_selected_features = current_features\n",
        "\n",
        "# 12. Print the sbe_selected_features\n",
        "print(\"\\nFinal features selected by SBE:\", sbe_selected_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "051f7c6b"
      },
      "source": [
        "## Apply_Lasso_Feature_Selection\n",
        "\n",
        "Apply Lasso regularization to the features remaining after SBE. Lasso adds a penalty to the regression coefficients, effectively shrinking some to zero and performing automatic feature selection. This step helps in further reducing overfitting and creating a more parsimonious model. The selected features will be those with non-zero coefficients.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bc737d5"
      },
      "source": [
        "**Action**:\n",
        "We will apply Lasso regularization to the features selected by SBE as per the instructions. This involves importing `Lasso`, filtering `X_train_scaled` with `sbe_selected_features`, instantiating and fitting the `Lasso` model, and then identifying and printing the features with non-zero coefficients.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1782a522"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Filter X_train_scaled to include only the sbe_selected_features\n",
        "X_train_lasso = X_train_scaled[sbe_selected_features]\n",
        "\n",
        "# 2. Instantiate a Lasso model\n",
        "# Alpha is the regularization strength. A smaller alpha means less regularization.\n",
        "# We start with a relatively small alpha to allow more features to be selected initially.\n",
        "lasso_model = Lasso(alpha=0.01, random_state=42)\n",
        "\n",
        "# 3. Fit the Lasso model to the filtered X_train_scaled and y_train\n",
        "lasso_model.fit(X_train_lasso, y_train)\n",
        "\n",
        "# 4. Identify the features with non-zero coefficients\n",
        "# The coefficients are stored in lasso_model.coef_\n",
        "# We consider coefficients very close to zero as effectively zero.\n",
        "selected_features_lasso = X_train_lasso.columns[lasso_model.coef_ != 0].tolist()\n",
        "\n",
        "# 5. Print the features selected by Lasso regularization\n",
        "print(\"Features selected by Lasso regularization (non-zero coefficients):\")\n",
        "if selected_features_lasso:\n",
        "    for feature in selected_features_lasso:\n",
        "        print(f\"- {feature}\")\n",
        "else:\n",
        "    print(\"No features were selected by Lasso regularization with the current alpha.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmJcA_bueIJo"
      },
      "source": [
        "## Implement ABL Hybrid Feature Selection\n",
        "\n",
        "### Subtask:\n",
        "Create a function `apply_abl_feature_selection` that encapsulates the ANOVA, Sequential Backward Elimination (SBE), and Lasso feature selection methods. This function will take `X_train_scaled` and `y_train` as input.\n",
        "1.  **ANOVA**: Use `f_regression` to identify features with a p-value less than 0.05.\n",
        "2.  **SBE**: Apply a sequential backward elimination process using `statsmodels.OLS` to the ANOVA-selected features, iteratively removing features with p-values greater than 0.05 until all remaining features are statistically significant.\n",
        "3.  **Lasso**: Apply `sklearn.linear_model.Lasso` regularization to the SBE-selected features, retaining only those with non-zero coefficients (using a suitable `alpha` value like 0.01).\n",
        "Finally, call this function and print the features selected by the ABL hybrid method.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Action**:\n",
        "I need to define a Python function `apply_abl_feature_selection` that encapsulates the ANOVA, SBE, and Lasso feature selection methods as specified. This function will then be called with the available `X_train_scaled` and `y_train` data, and the resulting selected features will be printed.\n",
        "\n"
      ],
      "metadata": {
        "id": "77EhxeOSevr9"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJBBfEGxfBAF"
      },
      "source": [
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from sklearn.feature_selection import f_regression\n",
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "def apply_abl_feature_selection(X_train, y_train, alpha=0.01):\n",
        "    print(\"\\n--- Applying ANOVA Step ---\")\n",
        "    # 1. ANOVA: Identify features with p-value less than 0.05\n",
        "    f_stats, p_values_anova = f_regression(X_train, y_train)\n",
        "    anova_results = pd.DataFrame({\n",
        "        'feature': X_train.columns,\n",
        "        'p_value': p_values_anova\n",
        "    })\n",
        "    anova_selected_features = anova_results[anova_results['p_value'] < 0.05]['feature'].tolist()\n",
        "    if not anova_selected_features:\n",
        "        print(\"No features selected by ANOVA.\")\n",
        "        return []\n",
        "    print(f\"ANOVA selected features: {anova_selected_features}\")\n",
        "\n",
        "    print(\"\\n--- Applying SBE Step ---\")\n",
        "    # 2. SBE: Iteratively remove features with p-values > 0.05 from OLS model\n",
        "    current_features_sbe = list(anova_selected_features) # Make a mutable copy\n",
        "    if not current_features_sbe:\n",
        "        print(\"No features to start SBE with.\")\n",
        "        return []\n",
        "\n",
        "    while True:\n",
        "        X_sbe = X_train[current_features_sbe]\n",
        "        X_sbe_const = sm.add_constant(X_sbe)\n",
        "        model = sm.OLS(y_train, X_sbe_const).fit()\n",
        "\n",
        "        # Get p-values, excluding the constant\n",
        "        p_values = model.pvalues.drop('const', errors='ignore')\n",
        "\n",
        "        if p_values.empty:\n",
        "            break\n",
        "\n",
        "        max_p_value = p_values.max()\n",
        "        feature_to_remove = p_values.idxmax()\n",
        "\n",
        "        if max_p_value > 0.05:\n",
        "            current_features_sbe.remove(feature_to_remove)\n",
        "            print(f\"Removing '{feature_to_remove}' with p-value: {max_p_value:.4f}\")\n",
        "            if not current_features_sbe:\n",
        "                print(\"No features remaining after SBE.\")\n",
        "                break\n",
        "        else:\n",
        "            print(\"All remaining features in SBE have p-values <= 0.05. Stopping SBE.\")\n",
        "            break\n",
        "\n",
        "    sbe_selected_features = current_features_sbe\n",
        "    if not sbe_selected_features:\n",
        "        print(\"No features selected by SBE.\")\n",
        "        return []\n",
        "    print(f\"SBE selected features: {sbe_selected_features}\")\n",
        "\n",
        "    print(\"\\n--- Applying Lasso Step ---\")\n",
        "    # 3. Lasso: Retain features with non-zero coefficients\n",
        "    X_lasso = X_train[sbe_selected_features]\n",
        "    lasso_model = Lasso(alpha=alpha, random_state=42, max_iter=10000) # Increased max_iter for convergence\n",
        "    lasso_model.fit(X_lasso, y_train)\n",
        "\n",
        "    lasso_selected_features = X_lasso.columns[lasso_model.coef_ != 0].tolist()\n",
        "    if not lasso_selected_features:\n",
        "        print(f\"No features selected by Lasso with alpha={alpha}.\")\n",
        "        return []\n",
        "    print(f\"Lasso selected features: {lasso_selected_features}\")\n",
        "\n",
        "    return lasso_selected_features\n",
        "\n",
        "# Call the function with X_train_scaled and y_train\n",
        "abl_selected_features = apply_abl_feature_selection(X_train_scaled, y_train)\n",
        "\n",
        "print(\"\\nFinal features selected by ABL Hybrid Method:\")\n",
        "if abl_selected_features:\n",
        "    for feature in abl_selected_features:\n",
        "        print(f\"- {feature}\")\n",
        "else:\n",
        "    print(\"No features were selected by the ABL Hybrid Method.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's clarify the results of the Hybrid ABL feature selection method.\n",
        "\n",
        "Here's a breakdown:\n",
        "\n",
        "ANOVA Step: In the first step of ABL, ANOVA (Analysis of Variance) was applied. It identified all 8 independent features (positive, active, hospitalized, hospitalizedCurr, recovered, death, total_tested, daily_tested) as having a statistically significant relationship with the target variable (daily_positive), with p-values less than 0.05 (i.e., at a 5% significance level).\n",
        "\n",
        "Sequential Backward Elimination (SBE) Step: Following ANOVA, SBE was performed. Starting with all 8 features identified by ANOVA, the SBE process iteratively removes the least significant features. In this case, no features were removed by SBE because all remaining features maintained p-values less than or equal to 0.05. This means all 8 features were still considered statistically significant by SBE.\n",
        "\n",
        "Lasso Regularization Step: This is the final and most aggressive step of the ABL hybrid method. With an alpha value of 0.01 (which controls the strength of regularization), Lasso performed a strong feature selection. It shrunk the coefficients of all features except one to zero.\n",
        "\n",
        "As a result, only 'death' was retained as a feature with a non-zero coefficient by the Lasso model.\n",
        "\n",
        "Conclusion: While the initial ANOVA and SBE steps indicated that all 8 features were statistically significant at the 5% level, the Lasso regularization component of ABL aggressively selected only 'death' as the most crucial feature, effectively reducing the feature set to a single variable due to its strong penalty. So, yes, 'death' is the only feature deemed significant by the entire ABL hybrid method with the given Lasso alpha value."
      ],
      "metadata": {
        "id": "oGsVKskEmb6T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Boruta-VI hybrid method\n",
        "\n",
        "**Meaning:** a combination of the Boruta FS with variable importance\n",
        "\n",
        "The Boruta-VI technique, executed with the BorutaPy library, is a wrapper-style feature selection algorithm aimed at identifying all pertinent features in a dataset. It operates by conducting a thorough search, repeatedly assessing the significance of each original feature in relation to the significance of randomly shuffled versions of those features, referred to as 'shadow features'\n"
      ],
      "metadata": {
        "id": "QaaO8O-OdMjt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07a6e714"
      },
      "source": [
        "**Action**\n",
        "Install the `boruta_py` library, which provides the Boruta feature selection algorithm. This is a prerequisite for applying the Boruta-VI method.We  will use `pip install` in a code block to perform the installation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96e64ac5"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install boruta_py\n",
        "print(\"Attempted to install boruta_py library.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4ef5016"
      },
      "source": [
        "**Action**:\n",
        "The previous attempt to install `boruta_py` failed because the package name was incorrect. We will re-attempt the installation using the correct package name, `Boruta`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0c2f0a1e"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install Boruta\n",
        "print(\"Attempted to install Boruta library.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ef69c9c"
      },
      "source": [
        "**Action**:\n",
        "Now that the Boruta library has been successfully installed, we will proceed to apply the Boruta-VI feature selection method. This involves importing `BorutaPy`, defining a suitable estimator (`RandomForestRegressor`), initializing BorutaPy with the training data, and then fitting it to determine the relevant features.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92c5d0d7"
      },
      "outputs": [],
      "source": [
        "from boruta import BorutaPy\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# 1. Define the estimator for Boruta\n",
        "# Use RandomForestRegressor as the estimator. Set n_estimators and random_state.\n",
        "estimator = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "\n",
        "# 2. Initialize BorutaPy\n",
        "# verbose=2 will print some details about the feature selection process\n",
        "# max_iter is the number of iterations for the Boruta algorithm. A higher number ensures better convergence.\n",
        "borta_selector = BorutaPy(\n",
        "    estimator=estimator,\n",
        "    n_estimators='auto',\n",
        "    max_iter=100, # Number of iterations for Boruta to perform\n",
        "    random_state=42,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "# 3. Fit BorutaPy to the training data\n",
        "# X_train_scaled and y_train are already prepared from previous steps.\n",
        "# Boruta expects numpy arrays, so convert them.\n",
        "borta_selector.fit(X_train_scaled.values, y_train.values)\n",
        "\n",
        "print(\"Boruta feature selection applied.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A breakdown of how the code block uses the Boruta-VI method to determine feature importance:\n",
        "\n",
        "1.\tBase Estimator Selection: The first step defines a RandomForestRegressor as the estimator. This is the core machine learning model that Boruta uses internally to assess feature importance. Random Forests are chosen because they can inherently measure feature importance (e.g., via Mean Decrease Gini).\n",
        "\n",
        "2.\testimator = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "\n",
        "3.\tBorutaPy Initialization: An instance of BorutaPy is created, passing the estimator (our Random Forest Regressor). Key parameters used are:\n",
        "\n",
        "\n",
        "*   n_estimators='auto': Boruta intelligently decides how many trees to build inthe Random Forest for its internal importance calculations, adapting to the dataset.\n",
        "\n",
        "*   max_iter=100: The algorithm will run for a maximum of 100 iterations. In each iteration, it refines its assessment of feature importance.\n",
        "\n",
        "\n",
        "*   rbose=2: This setting provides detailed output during the fitting process, showing how features are categorized as 'Confirmed', 'Tentative', or 'Rejected' in each iteration.\n",
        "\n",
        "\n",
        "4.\tThe Core Algorithm (Fitting): The borta_selector.fit() method is called with the training data (X_train_scaled.values and y_train.values). During this fitting process, Boruta performs the following iterative steps:\n",
        "\n",
        "\n",
        "*   Creates Shadow Features: For each original feature, a randomly shuffled copy (a 'shadow feature') is created. These shadow features are then added to the dataset.\n",
        "*   Trains Random Forest: A Random Forest model is trained on this extended dataset (original features + shadow features).\n",
        "\n",
        "\n",
        "*   Calculates Importance: Feature importances for all features (original and shadow) are calculated using the Random Forest's internal mechanism (e.g., Gini importance).\n",
        "*   Compares to Shadow Features: The algorithm finds the maximum importance value among all shadow features. This value acts as a threshold.\n",
        "\n",
        "\n",
        "*   Decision Rule: Each original feature's importance is compared to this maximum shadow feature importance:\n",
        "\n",
        "> If an original feature's importance is significantly higher than the max shadow importance, it's considered 'Confirmed' relevant.\n",
        "\n",
        "> If it's significantly lower, it's 'Rejected' as irrelevant.\n",
        "\n",
        "> If its importance is similar, it remains 'Tentative' and is re-evaluated in the next iteration.\n",
        "\n",
        "\n",
        "*   Iterates: This process repeats for max_iter times or until all features are either 'Confirmed' or 'Rejected'.\n",
        "\n",
        "5.\tborta_selector.fit(X_train_scaled.values, y_train.values)\n",
        "\n",
        "6.\tRetrieving Selected Features: After the fit method completes, the relevance status of each feature is stored in borta_selector.support_ (for strongly relevant features) and borta_selector.support_weak_ (for weakly relevant features).\n",
        "\n",
        "7.\tselected_strong_features = X_train_scaled.columns[borta_selector.support_].tolist()\n",
        "selected_weak_features = X_train_scaled.columns[borta_selector.support_weak_].tolist()\n",
        "\n",
        "In this case, the output shows that all independent features (positive, active, hospitalized, hospitalizedCurr, recovered, death, total_tested, daily_tested) were identified as 'Strong' features, meaning they were consistently found to be more important than random features across the iterations of the Boruta algorithm. This indicates that, according to Boruta's rigorous statistical test, all these features significantly contribute to predicting the target variable.\n"
      ],
      "metadata": {
        "id": "ZBednuc2f4ov"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f52a322d"
      },
      "source": [
        "**Action**:\n",
        "Now that BorutaPy has been fitted, we will retrieve the selected features using `borta_selector.support_` and `borta_selector.support_weak_` attributes, then map them back to the original feature names for clear identification.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "716989ad"
      },
      "outputs": [],
      "source": [
        "selected_strong_features = X_train_scaled.columns[borta_selector.support_].tolist()\n",
        "selected_weak_features = X_train_scaled.columns[borta_selector.support_weak_].tolist()\n",
        "\n",
        "print(\"Boruta Selected Features (Strong):\")\n",
        "if selected_strong_features:\n",
        "    for feature in selected_strong_features:\n",
        "        print(f\"- {feature}\")\n",
        "else:\n",
        "    print(\"No strong features selected by Boruta.\")\n",
        "\n",
        "print(\"\\nBoruta Selected Features (Weak/Tentative):\")\n",
        "if selected_weak_features:\n",
        "    for feature in selected_weak_features:\n",
        "        print(f\"- {feature}\")\n",
        "else:\n",
        "    print(\"No weak/tentative features selected by Boruta.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zV86eEmoBeo1"
      },
      "source": [
        "# Step 3: Model Development and Model Specification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "279f3703"
      },
      "source": [
        "# Details\n",
        "Implement and train the following regression models to forecast daily positive COVID-19 cases:\n",
        " Linear Regression (GLM),\n",
        "\n",
        " Lasso Regression (`alpha=0.0014`, `random_state=42`),\n",
        "\n",
        " K-Nearest Neighbors Regressor (`n_neighbors=1`),\n",
        "\n",
        "  Support Vector Regressor (SVR) (`gamma=0.15`, `C=10`),\n",
        "  \n",
        "  Decision Tree Regressor (CART), RandomForestRegressor (`n_estimators=700`, `max_features=3`, `random_state=42`),\n",
        "  \n",
        "  Bagging Regressor (with `DecisionTreeRegressor` as base),\n",
        "  \n",
        "  Gradient Boosting Regressor (`n_estimators=250`, `max_depth=5`, `learning_rate=0.1`, `min_samples_leaf=10`, `random_state=42`),\n",
        "  \n",
        "  and XGBoost Regressor (`n_estimators=250`, `max_depth=5`, `learning_rate=0.4`, `gamma=0`, `colsample_bytree=0.8`, `random_state=42`).\n",
        "  \n",
        "  For all models, we train on `X_train_scaled` and `y_train`, and make predictions on `X_test_scaled`. Afterward, we evaluate each model's performance by calculating and printing the Mean Absolute Error (MAE), Mean Squared Error (MSE), and R-squared on `y_test` and its predictions. Finally, we present a comparative summary of all model performance metrics, create a bar chart visualizing their MAE, MSE, and R-squared scores, and generate a plot showing the predicted vs. actual `daily_positive` values from the best-performing model on the test set.\n",
        "  \n",
        "  Then we conclude by summarizing the model development process, highlighting chosen models, configurations, and initial performance insights for forecasting daily positive COVID-19 cases, explaining why regression models were used and classification models were not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fefa006"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# 1. Instantiate a LinearRegression model (representing GLM with default settings)\n",
        "lm = LinearRegression()\n",
        "\n",
        "# 2. Train the model using X_train_scaled and y_train\n",
        "lm.fit(X_train_scaled, y_train)\n",
        "\n",
        "# 3. Make predictions on X_test_scaled\n",
        "lr_predictions = lm.predict(X_test_scaled)\n",
        "\n",
        "print(\"Linear Regression model implemented, trained, and predictions made.\")\n",
        "print(f\"Shape of predictions: {lr_predictions.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We cannot perform multivariable binary logistic regression analysis.** This is because 'daily_positive' is a continuous numerical variable, not a binary one. Binary logistic regression is typically used when the dependent variable is binary (e.g., 0 or 1).\n",
        "\n",
        "We might still perform the analysis though by creating a binary target variable from 'daily_positive' by setting a threshold at its median value: 0 if 'daily_positive' is below or equal to the median, and 1 if it's above. This will allow us to proceed with binary logistic regression. For each feature selection method, we could then build a logistic regression model and identify predictors that are statistically significant (p-values < 0.05) and those that are not."
      ],
      "metadata": {
        "id": "ullbYNRhXVlY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e3b7234"
      },
      "source": [
        "## Implement and Train Lasso Regression\n",
        "\n",
        "We implement a Lasso Regression model using `sklearn.linear_model.Lasso` with `alpha=0.0014` and `random_state=42`. Train it on `X_final_train` and `y_final_train`, then make predictions on `X_final_test`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4211c073"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "# 1. Instantiate a Lasso model with alpha=0.0014 and random_state=42\n",
        "lasso_model = Lasso(alpha=0.0014, random_state=42)\n",
        "\n",
        "# 2. Train the Lasso model using X_train_scaled and y_train\n",
        "lasso_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# 3. Make predictions on X_test_scaled\n",
        "lasso_predictions = lasso_model.predict(X_test_scaled)\n",
        "\n",
        "print(\"Lasso Regression model implemented, trained, and predictions made.\")\n",
        "print(f\"Shape of predictions: {lasso_predictions.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df85402c"
      },
      "source": [
        "## Implement and Train K-Nearest Neighbors Regressor\n",
        "\n",
        "We implement a K-Nearest Neighbors Regressor using `sklearn.neighbors.KNeighborsRegressor` with `n_neighbors=1`. Train it on `X_final_train` and `y_final_train`, then make predictions on `X_final_test`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4e4573a5"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "# 1. Instantiate a KNeighborsRegressor model with n_neighbors=1\n",
        "knn_model = KNeighborsRegressor(n_neighbors=1)\n",
        "\n",
        "# 2. Train the K-Nearest Neighbors Regressor model using X_train_scaled and y_train\n",
        "knn_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# 3. Make predictions on X_test_scaled\n",
        "knn_predictions = knn_model.predict(X_test_scaled)\n",
        "\n",
        "print(\"K-Nearest Neighbors Regressor model implemented, trained, and predictions made.\")\n",
        "print(f\"Shape of predictions: {knn_predictions.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0fb44ac"
      },
      "source": [
        "## Implement and Train Support Vector Regressor (SVR)\n",
        "\n",
        "We implement a Support Vector Regressor (SVR) using `sklearn.svm.SVR` with `gamma=0.15` and `C=10`. Train it on `X_final_train` and `y_final_train`, then make predictions on `X_final_test`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "feac4d85"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVR\n",
        "\n",
        "# 1. Instantiate an SVR model with gamma=0.15 and C=10\n",
        "svr_model = SVR(gamma=0.15, C=10)\n",
        "\n",
        "# 2. Train the SVR model using X_train_scaled and y_train\n",
        "svr_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# 3. Make predictions on X_test_scaled\n",
        "svr_predictions = svr_model.predict(X_test_scaled)\n",
        "\n",
        "print(\"Support Vector Regressor model implemented, trained, and predictions made.\")\n",
        "print(f\"Shape of predictions: {svr_predictions.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a07863c"
      },
      "source": [
        "## Implement and Train Decision Tree Regressor (CART)\n",
        "\n",
        "### Subtask:\n",
        "Implement a Decision Tree Regressor model (representing CART) using `sklearn.tree.DecisionTreeRegressor`. Train it on `X_final_train` and `y_final_train`, then make predictions on `X_final_test`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "461e34e8"
      },
      "source": [
        "## Implement and Train Decision Tree Regressor (CART)\n",
        "\n",
        "We implement a Decision Tree Regressor model (representing CART) using `sklearn.tree.DecisionTreeRegressor`. Train it on `X_final_train` and `y_final_train`, then make predictions on `X_final_test`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8f23cda3"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# 1. Instantiate a DecisionTreeRegressor model (representing CART)\n",
        "dt_model = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "# 2. Train the Decision Tree Regressor model using X_train_scaled and y_train\n",
        "dt_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# 3. Make predictions on X_test_scaled\n",
        "dt_predictions = dt_model.predict(X_test_scaled)\n",
        "\n",
        "print(\"Decision Tree Regressor model implemented, trained, and predictions made.\")\n",
        "print(f\"Shape of predictions: {dt_predictions.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82d3c2cb"
      },
      "source": [
        "## Implement and Train RandomForestRegressor\n",
        "\n",
        "We implement a RandomForestRegressor model using `sklearn.ensemble.RandomForestRegressor` with `n_estimators=700`, `max_features=3`, and `random_state=42`. Train it on `X_final_train` and `y_final_train`, then make predictions on `X_final_test`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9419ecbc"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# 1. Instantiate a RandomForestRegressor model with n_estimators=700, max_features=3, and random_state=42\n",
        "rf_model = RandomForestRegressor(n_estimators=700, max_features=3, random_state=42, n_jobs=-1)\n",
        "\n",
        "# 2. Train the RandomForestRegressor model using X_train_scaled and y_train\n",
        "rf_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# 3. Make predictions on X_test_scaled\n",
        "rf_predictions = rf_model.predict(X_test_scaled)\n",
        "\n",
        "print(\"RandomForestRegressor model implemented, trained, and predictions made.\")\n",
        "print(f\"Shape of predictions: {rf_predictions.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e121a66"
      },
      "source": [
        "## Implement and Train Bagging Regressor\n",
        "\n",
        "We implement a Bagging Regressor model using `sklearn.ensemble.BaggingRegressor` with `DecisionTreeRegressor` as the base estimator. Train it on `X_final_train` and `y_final_train`, then make predictions on `X_final_test`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1d3c1f8"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# 1. Instantiate a DecisionTreeRegressor as the base estimator\n",
        "base_estimator = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "# 2. Instantiate a BaggingRegressor model with the base estimator\n",
        "bagging_model = BaggingRegressor(estimator=base_estimator, random_state=42, n_jobs=-1)\n",
        "\n",
        "# 3. Train the BaggingRegressor model using X_train_scaled and y_train\n",
        "bagging_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# 4. Make predictions on X_test_scaled\n",
        "bagging_predictions = bagging_model.predict(X_test_scaled)\n",
        "\n",
        "print(\"Bagging Regressor model implemented, trained, and predictions made.\")\n",
        "print(f\"Shape of predictions: {bagging_predictions.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ded86742"
      },
      "source": [
        "## Implement and Train Gradient Boosting Regressor\n",
        "\n",
        "We implement a Gradient Boosting Regressor using `sklearn.ensemble.GradientBoostingRegressor` with `n_estimators=250`, `max_depth=5`, `learning_rate=0.1`, `min_samples_leaf=10`, and `random_state=42`. Train it on `X_final_train` and `y_final_train`, then make predictions on `X_final_test`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ed6ddadb"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "# 1. Instantiate a GradientBoostingRegressor model with specified hyperparameters\n",
        "gb_model = GradientBoostingRegressor(\n",
        "    n_estimators=250,\n",
        "    max_depth=5,\n",
        "    learning_rate=0.1,\n",
        "    min_samples_leaf=10,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# 2. Train the GradientBoostingRegressor model using X_train_scaled and y_train\n",
        "gb_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# 3. Make predictions on X_test_scaled\n",
        "gb_predictions = gb_model.predict(X_test_scaled)\n",
        "\n",
        "print(\"Gradient Boosting Regressor model implemented, trained, and predictions made.\")\n",
        "print(f\"Shape of predictions: {gb_predictions.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5acdcfc6"
      },
      "source": [
        "## Implement and Train XGBoost Regressor\n",
        "\n",
        "We implement an XGBoost Regressor model using `xgboost.XGBRegressor` with `n_estimators=250`, `max_depth=5`, `learning_rate=0.4`, `gamma=0`, `colsample_bytree=0.8`, and `random_state=42`. Train it on `X_final_train` and `y_final_train`, then make predictions on `X_final_test`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "795a5649"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "# 1. Instantiate an XGBoost Regressor model with specified hyperparameters\n",
        "xgb_model = xgb.XGBRegressor(\n",
        "    n_estimators=250,\n",
        "    max_depth=5,\n",
        "    learning_rate=0.4,\n",
        "    gamma=0,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42,\n",
        "    n_jobs=-1 # Use all available cores\n",
        ")\n",
        "\n",
        "# 2. Train the XGBoost Regressor model using X_train_scaled and y_train\n",
        "xgb_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# 3. Make predictions on X_test_scaled\n",
        "xgb_predictions = xgb_model.predict(X_test_scaled)\n",
        "\n",
        "print(\"XGBoost Regressor model implemented, trained, and predictions made.\")\n",
        "print(f\"Shape of predictions: {xgb_predictions.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "830329c1"
      },
      "source": [
        "**Action**:\n",
        "\n",
        "Now that all regression models have been implemented and predictions made, we will calculate the Mean Absolute Error (MAE), Mean Squared Error (MSE), and R-squared for each model's predictions against the actual test values (`y_test`). We will store these metrics in a dictionary for a comparative summary, fulfilling the next part of the main task.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64ec717b"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Dictionary to store predictions from all models\n",
        "all_predictions = {\n",
        "    'Linear Regression': lr_predictions,\n",
        "    'Lasso Regression': lasso_predictions,\n",
        "    'K-Nearest Neighbors': knn_predictions,\n",
        "    'Support Vector Regressor': svr_predictions,\n",
        "    'Decision Tree Regressor': dt_predictions,\n",
        "    'Random Forest Regressor': rf_predictions,\n",
        "    'Bagging Regressor': bagging_predictions,\n",
        "    'Gradient Boosting Regressor': gb_predictions,\n",
        "    'XGBoost Regressor': xgb_predictions\n",
        "}\n",
        "\n",
        "# Dictionary to store evaluation metrics for each model\n",
        "model_metrics = {}\n",
        "\n",
        "print(\"Evaluating model performance:\")\n",
        "for model_name, predictions in all_predictions.items():\n",
        "    mae = mean_absolute_error(y_test, predictions)\n",
        "    mse = mean_squared_error(y_test, predictions)\n",
        "    r2 = r2_score(y_test, predictions)\n",
        "\n",
        "    model_metrics[model_name] = {\n",
        "        'MAE': mae,\n",
        "        'MSE': mse,\n",
        "        'R-squared': r2\n",
        "    }\n",
        "\n",
        "    print(f\"\\n--- {model_name} ---\")\n",
        "    print(f\"MAE: {mae:.4f}\")\n",
        "    print(f\"MSE: {mse:.4f}\")\n",
        "    print(f\"R-squared: {r2:.4f}\")\n",
        "\n",
        "print(\"\\nAll models evaluated and metrics calculated.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d50c7f37"
      },
      "source": [
        "**Action**:\n",
        "Now that the evaluation metrics for all models have been calculated, we will consolidate them into a Pandas DataFrame for a clear comparative summary. This will allow for easier analysis and identification of the best-performing model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8c18a09"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Convert the model_metrics dictionary to a DataFrame for easy comparison\n",
        "model_performance_df = pd.DataFrame(model_metrics).T\n",
        "\n",
        "print(\"Comparative Summary of Model Performance Metrics:\")\n",
        "print(model_performance_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99951ec2"
      },
      "source": [
        "## Compare Model Results and Visualize\n",
        "\n",
        "Present a comparative summary of the performance metrics for all implemented models. Create a bar chart visualizing their MAE, MSE, and R-squared scores. Additionally, generate a plot showing the predicted vs. actual values for `daily_positive` from the best-performing model on the test set, including proper legends.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4371e91a"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Prepare data for plotting\n",
        "metrics_df = model_performance_df.reset_index().melt(id_vars='index', var_name='Metric', value_name='Value')\n",
        "metrics_df = metrics_df.rename(columns={'index': 'Model'})\n",
        "\n",
        "# Create a bar chart for MAE, MSE, and R-squared scores\n",
        "plt.figure(figsize=(16, 8))\n",
        "sns.barplot(data=metrics_df, x='Model', y='Value', hue='Metric', palette='viridis')\n",
        "plt.title('Comparison of Model Performance Metrics (MAE, MSE, R-squared)')\n",
        "plt.xlabel('Regression Model')\n",
        "plt.ylabel('Metric Value')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.legend(title='Metric')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Bar chart visualizing model performance metrics generated.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49661ab1"
      },
      "source": [
        "**Action**:\n",
        "We will identify the best-performing model based on the highest R-squared from the `model_performance_df` and then generate a scatter plot to visualize its predicted values against the actual `y_test` values, including a diagonal line for perfect predictions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dbed449"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Identify the best-performing model based on R-squared\n",
        "best_model_name = model_performance_df['R-squared'].idxmax()\n",
        "best_model_predictions = all_predictions[best_model_name]\n",
        "\n",
        "print(f\"The best-performing model based on R-squared is: {best_model_name}\")\n",
        "\n",
        "# Create a scatter plot of predicted vs. actual values for the best model\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.scatterplot(x=y_test, y=best_model_predictions, alpha=0.6)\n",
        "\n",
        "# Add a diagonal line for perfect predictions (y=x)\n",
        "max_val = max(y_test.max(), best_model_predictions.max())\n",
        "min_val = min(y_test.min(), best_model_predictions.min())\n",
        "plt.plot([min_val, max_val], [min_val, max_val], color='red', linestyle='--', label='Perfect Prediction')\n",
        "\n",
        "plt.title(f'Predicted vs. Actual Daily Positive Cases ({best_model_name})')\n",
        "plt.xlabel('Actual Daily Positive Cases')\n",
        "plt.ylabel('Predicted Daily Positive Cases')\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Scatter plot of predicted vs. actual values for the best model generated.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b2e9301"
      },
      "source": [
        "## Summary of Model Development and Performance\n",
        "\n",
        "### Model Development Process\n",
        "The model development process involved several key stages to prepare the COVID-19 dataset and build robust forecasting models:\n",
        "1.  **Data Preprocessing**: Initial data inspection revealed significant missing values and outliers. Outliers were first addressed using `RobustScaler` to ensure robustness against extreme values. Missing values were then handled through a two-step imputation process: first, forward-filling grouped by country to account for time-series dependencies, and then median imputation for any remaining NaNs. Finally, the numerical features were normalized using `MinMaxScaler` to scale them to a common range (0-1), which is beneficial for many machine learning algorithms.\n",
        "2.  **Feature Selection**: A comprehensive approach was taken to select relevant features, aiming to reduce multicollinearity, improve model performance, and prevent overfitting:\n",
        "    *   **VIF (Variance Inflation Factor)**: Initial VIF calculation identified `daily_positive` and `total_tested` as highly collinear. As `daily_positive` is the target, `total_tested` was dropped, and VIFs were re-calculated to confirm reduction in multicollinearity.\n",
        "    *   **CMIM (Conditional Mutual Information Maximization)**: This information-theoretical filter method ranked features based on their ability to predict the target while minimizing redundancy. `daily_tested` was identified as the top feature.\n",
        "    *   **Random Forest (Mean Decrease Gini - MDG)**: An optimal `max_features` hyperparameter (`mtry=2`) was determined by evaluating out-of-bag (OOB) error rates. An optimal `RandomForestRegressor` was then trained, and feature importances (MDG) were extracted, ranking `positive` and `death` as most important.\n",
        "    *   **ANOVA (Analysis of Variance)**: This statistical method identified all remaining independent features (`positive`, `daily_tested`, `total_tested`, `death`, `hospitalizedCurr`, `active`, `recovered`, `hospitalized`) as having a statistically significant relationship with the target (`daily_positive`) with p-values less than 0.05.\n",
        "    *   **SBE (Sequential Backward Elimination)**: Applied to the ANOVA-selected features, SBE confirmed that all these features remained statistically significant (p-value <= 0.05), indicating their collective importance.\n",
        "    *   **Lasso Regularization**: Lasso was applied to the SBE-selected features. With an `alpha=0.01`, only `death` was retained, indicating a strong penalty and highlighting `death` as the most robust predictor under this regularization.\n",
        "    *   **Boruta-VI**: This wrapper method used a `RandomForestRegressor` and selected all independent features as 'strong' features, suggesting that all of them contribute meaningfully to the target prediction in combination.\n",
        "\n",
        "### Chosen Models and Configurations\n",
        "To forecast `daily_positive` COVID-19 cases, a variety of regression models were implemented and trained on the preprocessed and scaled training data (`X_train_scaled`, `y_train`). Predictions were then made on the test set (`X_test_scaled`). The models and their configurations were:\n",
        "*   **Linear Regression (GLM)**: Default `LinearRegression()`\n",
        "*   **Lasso Regression**: `Lasso(alpha=0.0014, random_state=42)`\n",
        "*   **K-Nearest Neighbors Regressor**: `KNeighborsRegressor(n_neighbors=1)`\n",
        "*   **Support Vector Regressor (SVR)**: `SVR(gamma=0.15, C=10)`\n",
        "*   **Decision Tree Regressor (CART)**: `DecisionTreeRegressor(random_state=42)`\n",
        "*   **RandomForestRegressor**: `RandomForestRegressor(n_estimators=700, max_features=3, random_state=42)`\n",
        "*   **Bagging Regressor**: `BaggingRegressor(estimator=DecisionTreeRegressor(random_state=42), random_state=42)`\n",
        "*   **Gradient Boosting Regressor**: `GradientBoostingRegressor(n_estimators=250, max_depth=5, learning_rate=0.1, min_samples_leaf=10, random_state=42)`\n",
        "*   **XGBoost Regressor**: `xgb.XGBRegressor(n_estimators=250, max_depth=5, learning_rate=0.4, gamma=0, colsample_bytree=0.8, random_state=42)`\n",
        "\n",
        "### Initial Performance Insights\n",
        "The models were evaluated using Mean Absolute Error (MAE), Mean Squared Error (MSE), and R-squared. A comparative summary is provided below:\n",
        "\n",
        "| Model                       | MAE       | MSE       | R-squared |\n",
        "| :-------------------------- | :-------- | :-------- | :-------- |\n",
        "| Linear Regression           | 0.0083    | 0.0010    | 0.4205    |\n",
        "| Lasso Regression            | 0.0074    | 0.0006    | 0.6421    |\n",
        "| K-Nearest Neighbors         | **0.0045**| **0.0004**| **0.7737**|\n",
        "| Support Vector Regressor    | 0.0699    | 0.0056    | -2.3748   |\n",
        "| Decision Tree Regressor     | 0.0087    | 0.0011    | 0.3232    |\n",
        "| Random Forest Regressor     | 0.0066    | 0.0006    | 0.6490    |\n",
        "| Bagging Regressor           | 0.0091    | 0.0015    | 0.1121    |\n",
        "| Gradient Boosting Regressor | 0.0095    | 0.0017    | -0.0342   |\n",
        "| XGBoost Regressor           | 0.0049    | 0.0004    | 0.7731    |\n",
        "\n",
        "**Key Observations:**\n",
        "*   **Best Performer**: The **K-Nearest Neighbors Regressor** achieved the best performance with the lowest MAE (0.0045), lowest MSE (0.0004), and highest R-squared (0.7737). This suggests that for this dataset, local patterns and similarities in the feature space are highly predictive of daily positive cases.\n",
        "*   **Strong Contenders**: XGBoost Regressor also performed exceptionally well, very close to KNN, with an R-squared of 0.7731 and similarly low MAE/MSE. Lasso Regression and Random Forest Regressor also showed decent performance with R-squared values around 0.64.\n",
        "*   **Poor Performers**: Support Vector Regressor (SVR) performed very poorly, evidenced by a significantly high MAE and a negative R-squared, indicating that its predictions are worse than simply predicting the mean. Gradient Boosting Regressor and Bagging Regressor also showed poor R-squared values, suggesting that their current configurations did not capture the underlying patterns effectively for this specific prediction task.\n",
        "\n",
        "### Why Regression Models were Used\n",
        "Regression models were chosen because the target variable, `daily_positive` COVID-19 cases, is a **continuous numerical variable**. The objective is to predict a specific quantity (the number of positive cases each day), not a category or class. If the task were to classify days into categories like"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c6d4070"
      },
      "source": [
        "\n",
        "### Insights or Next Steps\n",
        "*   Further hyperparameter tuning, particularly for the K-Nearest Neighbors and XGBoost models, could potentially enhance their performance even more. Investigating different kernel functions and regularization parameters for SVR might improve its poor performance.\n",
        "*   Explore advanced ensemble methods or stacking approaches using the best-performing models (KNN and XGBoost) to potentially achieve even greater predictive accuracy and robustness for forecasting daily positive COVID-19 cases.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "582b5f01"
      },
      "source": [
        "## Prepare Data for Cross-Validation\n",
        "\n",
        "Ensure the full dataset (`X_scaled` and `y`) is available and in the correct format for cross-validation. This step uses the data that was previously scaled and prepared.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Standard:** The predictive performance of the models should now be rigorously assessed through a thorough 10 times repeated tenfold cross-validation approach. This internal validation technique will provide estimates of model performance on unseen data and allow for fine-tuning of model parameters. The outcomes from cross-validation will be utilized as a benchmark for constructing the final classification model and facilitating model selection. Subsequently, the final model will be determined. The final model undergoes validation against an independent test dataset to confirm its predictive efficacy (external validation).\n",
        "\n",
        "k = 10, as the estimate of prediction error is almost unbiased in 10-fold cross-validation. Since cross-validated performance measures are unreliable for small-sample data sets, I recommend that the true classification error be estimated using Bayesian intervals and a single hold-out test set.\n"
      ],
      "metadata": {
        "id": "XO_KwA6sa2ee"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, considering runtime we instead did for K = 3. Hence:\n",
        "\n",
        "The predictive performance of the models will now be rigorously assessed through a thorough 3 times repeated threefold cross-validation approach. This internal validation technique will provide estimates of model performance on unseen data and allow for fine-tuning of model parameters. The outcomes from cross-validation will be utilized as a benchmark for constructing the final classification model and facilitating model selection. Subsequently, the final model will be determined. The final model undergoes validation against an independent test dataset to confirm its predictive efficacy (external validation)."
      ],
      "metadata": {
        "id": "HVMrRiSgbUyQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**k = 3**, as the estimate of prediction error is almost unbiased in 3-fold cross-validation. Since cross-validated performance measures are unreliable for small-sample data sets, I recommend that the true classification error be estimated using Bayesian intervals and a single hold-out test set.\n",
        "\n",
        "\n",
        "Doing **K=3** cross-validation means splitting your dataset into 3 equal-sized \"folds,\" training your model three separate times, and each time using one-fold as the test set and the other two as the training set, then averaging the results for a robust performance estimate. It's a way to ensure every data point is used for testing once, reducing bias and giving a better idea of how your model generalizes to new data, unlike a single train-test split.\n",
        "How it works with K=3:\n",
        "\n",
        "1.\tSplit Data: Your dataset is divided into three random, roughly equal parts (Fold 1, Fold 2, Fold 3).\n",
        "\n",
        "2.\tIteration 1: Train the model on (Fold 2 + Fold 3) and test its performance on Fold 1.\n",
        "\n",
        "3.\tIteration 2: Train the model on (Fold 1 + Fold 3) and test its performance on Fold 2.\n",
        "\n",
        "4.\tIteration 3: Train the model on (Fold 1 + Fold 2) and test its performance on Fold 3.\n",
        "\n",
        "5.\tFinal Result: Average the performance metrics (like accuracy or error) from the three tests to get a single, more reliable score for your model.\n",
        "\n",
        "Why it's useful:\n",
        "\n",
        "•\tReduces Variance: Gives a more stable performance estimate because you're not relying on just one random split.\n",
        "\n",
        "•\tUses All Data: Every data point gets a chance to be in the test set, ensuring the model is evaluated fairly across the entire dataset.\n",
        "\n",
        "•\tPrevents Overfitting: Helps you see if the model performs well on unseen data, not just the data it was trained on.\n"
      ],
      "metadata": {
        "id": "Fx7xju1KcSqE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0229cda4"
      },
      "source": [
        "**Action**:\n",
        "We need to verify the availability and dimensions of `X_scaled` and `y` for cross-validation by printing their shapes, as per the subtask instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2dbca4c"
      },
      "outputs": [],
      "source": [
        "print(f\"Shape of X_scaled: {X_scaled.shape}\")\n",
        "print(f\"Shape of y: {y.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd3a2e47"
      },
      "source": [
        "**Action**:\n",
        "Now that the data is confirmed to be available, I will define the cross-validation strategy using `RepeatedKFold` with 3 splits and 3 repetitions, as specified in the main task.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fe7f560e"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import RepeatedKFold\n",
        "\n",
        "# Define the cross-validation strategy\n",
        "n_splits = 3\n",
        "n_repeats = 3\n",
        "rkf = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=42)\n",
        "\n",
        "print(f\"Cross-validation strategy defined: RepeatedKFold with {n_splits} splits and {n_repeats} repetitions.\")\n",
        "print(f\"Total number of folds for cross-validation: {n_splits * n_repeats}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1759c952"
      },
      "source": [
        "**Action**:\n",
        "\n",
        "We nowperform cross-validation for each of the previously implemented regression models using the defined `RepeatedKFold` strategy, collecting MAE, MSE, and R-squared scores for each fold. This involves looping through each model, splitting the data into training and testing sets based on the folds, training the model, making predictions, and storing the evaluation metrics.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0637a12"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression, Lasso\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, BaggingRegressor, GradientBoostingRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Dictionary of models to evaluate\n",
        "models = {\n",
        "    'Linear Regression': LinearRegression(),\n",
        "    'Lasso Regression': Lasso(alpha=0.0014, random_state=42),\n",
        "    'K-Nearest Neighbors': KNeighborsRegressor(n_neighbors=1),\n",
        "    'Support Vector Regressor': SVR(gamma=0.15, C=10),\n",
        "    'Decision Tree Regressor': DecisionTreeRegressor(random_state=42),\n",
        "    'Random Forest Regressor': RandomForestRegressor(n_estimators=700, max_features=3, random_state=42, n_jobs=-1),\n",
        "    'Bagging Regressor': BaggingRegressor(estimator=DecisionTreeRegressor(random_state=42), random_state=42, n_jobs=-1),\n",
        "    'Gradient Boosting Regressor': GradientBoostingRegressor(n_estimators=250, max_depth=5, learning_rate=0.1, min_samples_leaf=10, random_state=42),\n",
        "    'XGBoost Regressor': XGBRegressor(n_estimators=250, max_depth=5, learning_rate=0.4, gamma=0, colsample_bytree=0.8, random_state=42, n_jobs=-1)\n",
        "}\n",
        "\n",
        "# Dictionary to store cross-validation results\n",
        "cv_results = {model_name: {'MAE': [], 'MSE': [], 'R-squared': []} for model_name in models.keys()}\n",
        "\n",
        "print(\"Starting cross-validation...\")\n",
        "\n",
        "# Perform cross-validation for each model\n",
        "for model_name, model in models.items():\n",
        "    print(f\"\\nPerforming CV for {model_name}...\")\n",
        "    for fold, (train_index, test_index) in enumerate(rkf.split(X_scaled, y)):\n",
        "        X_train_fold, X_test_fold = X_scaled.iloc[train_index], X_scaled.iloc[test_index]\n",
        "        y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "        # Train the model\n",
        "        model.fit(X_train_fold, y_train_fold)\n",
        "\n",
        "        # Make predictions\n",
        "        predictions = model.predict(X_test_fold)\n",
        "\n",
        "        # Calculate metrics\n",
        "        mae = mean_absolute_error(y_test_fold, predictions)\n",
        "        mse = mean_squared_error(y_test_fold, predictions)\n",
        "        r2 = r2_score(y_test_fold, predictions)\n",
        "\n",
        "        # Store metrics\n",
        "        cv_results[model_name]['MAE'].append(mae)\n",
        "        cv_results[model_name]['MSE'].append(mse)\n",
        "        cv_results[model_name]['R-squared'].append(r2)\n",
        "        # print(f\"  Fold {fold+1}: MAE={mae:.4f}, MSE={mse:.4f}, R-squared={r2:.4f}\")\n",
        "\n",
        "print(\"Cross-validation completed for all models.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec8f2b5b"
      },
      "source": [
        "**Action**:\n",
        "\n",
        "Now that cross-validation has been completed and metrics for each fold are stored in `cv_results`, we will aggregate these results by calculating the mean and standard deviation for MAE, MSE, and R-squared for each model. Finally, we will print this summary to analyze the model performances across multiple folds.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dca4ba1"
      },
      "outputs": [],
      "source": [
        "print(\"--- Cross-Validation Results Summary ---\")\n",
        "\n",
        "aggregated_cv_results = {}\n",
        "for model_name, metrics in cv_results.items():\n",
        "    aggregated_cv_results[model_name] = {\n",
        "        'MAE_mean': np.mean(metrics['MAE']),\n",
        "        'MAE_std': np.std(metrics['MAE']),\n",
        "        'MSE_mean': np.mean(metrics['MSE']),\n",
        "        'MSE_std': np.std(metrics['MSE']),\n",
        "        'R-squared_mean': np.mean(metrics['R-squared']),\n",
        "        'R-squared_std': np.std(metrics['R-squared'])\n",
        "    }\n",
        "\n",
        "    print(f\"\\n{model_name}:\")\n",
        "    print(f\"  MAE (Mean ± Std): {aggregated_cv_results[model_name]['MAE_mean']:.4f} ± {aggregated_cv_results[model_name]['MAE_std']:.4f}\")\n",
        "    print(f\"  MSE (Mean ± Std): {aggregated_cv_results[model_name]['MSE_mean']:.4f} ± {aggregated_cv_results[model_name]['MSE_std']:.4f}\")\n",
        "    print(f\"  R-squared (Mean ± Std): {aggregated_cv_results[model_name]['R-squared_mean']:.4f} ± {aggregated_cv_results[model_name]['R-squared_std']:.4f}\")\n",
        "\n",
        "print(\"\\nCross-validation results aggregated and printed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13129bc0"
      },
      "source": [
        "## Visualize Cross-Validation Performance\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2539ae3e"
      },
      "source": [
        "**Action**:\n",
        "We will convert the `aggregated_cv_results` dictionary into a Pandas DataFrame and then melt it to prepare the data for plotting multiple metrics (MAE, MSE, R-squared) for each model on a single bar chart, as instructed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fb5b4e7"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Convert aggregated_cv_results dictionary to a DataFrame\n",
        "aggregated_cv_df = pd.DataFrame(aggregated_cv_results).T\n",
        "\n",
        "# Prepare data for plotting by melting the DataFrame\n",
        "# We want to plot mean MAE, mean MSE, and mean R-squared\n",
        "plot_df = aggregated_cv_df[['MAE_mean', 'MSE_mean', 'R-squared_mean']]\n",
        "plot_df.columns = ['MAE', 'MSE', 'R-squared'] # Rename columns for clarity in the plot\n",
        "\n",
        "metrics_melted = plot_df.reset_index().melt(id_vars='index', var_name='Metric', value_name='Value')\n",
        "metrics_melted = metrics_melted.rename(columns={'index': 'Model'})\n",
        "\n",
        "# Create a bar chart for MAE, MSE, and R-squared scores\n",
        "plt.figure(figsize=(16, 8))\n",
        "sns.barplot(data=metrics_melted, x='Model', y='Value', hue='Metric', palette='viridis')\n",
        "plt.title('Cross-Validation Performance: Mean MAE, MSE, and R-squared')\n",
        "plt.xlabel('Regression Model')\n",
        "plt.ylabel('Metric Value')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.legend(title='Metric')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Bar chart visualizing cross-validation performance metrics generated.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a1e703f"
      },
      "source": [
        "## Identify Best Model based on Cross-Validation\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acfa93fd"
      },
      "source": [
        "**Action**:\n",
        "To identify the best-performing model, we will access the 'aggregated_cv_results' dictionary, find the model with the highest 'R-squared_mean' value, and then print its name.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0ad2c66"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Find the model with the highest mean R-squared score\n",
        "# Convert aggregated_cv_results to a DataFrame for easier sorting\n",
        "aggregated_cv_df = pd.DataFrame(aggregated_cv_results).T\n",
        "best_model_cv = aggregated_cv_df['R-squared_mean'].idxmax()\n",
        "\n",
        "print(f\"The best-performing model based on mean R-squared from cross-validation is: {best_model_cv}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aed2507b"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "*   **Summarize the cross-validation process:**\n",
        "    The cross-validation process involved preparing the full dataset (`X_scaled` with shape (27641, 8) and `y` with shape (27641,)). A `RepeatedKFold` strategy was employed with 3 splits and 3 repetitions, resulting in a total of 9 folds. Nine different regression models (Linear Regression, Lasso Regression, K-Nearest Neighbors, SVR, Decision Tree, Random Forest, Bagging, Gradient Boosting, XGBoost) were evaluated. For each model and each fold, the model was trained, predictions were made, and Mean Absolute Error (MAE), Mean Squared Error (MSE), and R-squared scores were calculated and stored. Finally, the mean and standard deviation of these metrics were aggregated for each model to provide a comprehensive performance summary.\n",
        "*   **Key findings regarding model performance:**\n",
        "    The cross-validation revealed significant differences in performance among the models. Ensemble models such as Random Forest, Bagging, Gradient Boosting, and XGBoost generally exhibited strong predictive capabilities with high R-squared values (around 0.96-0.97) and relatively low MAE and MSE. Conversely, the Support Vector Regressor (SVR) showed very poor performance, indicated by negative R-squared values.\n",
        "*   **Identified best-performing regression model(s):**\n",
        "    Based on the mean R-squared score from the cross-validation, the **Random Forest Regressor** was identified as the best-performing model.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The dataset for cross-validation consisted of 27,641 samples with 8 features (`X_scaled`) and a corresponding target variable (`y`).\n",
        "*   A `RepeatedKFold` cross-validation strategy with 3 splits and 3 repetitions was used, leading to 9 evaluation folds for each model.\n",
        "*   Ensemble models generally performed very well, with the Random Forest Regressor achieving the highest mean R-squared score.\n",
        "*   The Support Vector Regressor (SVR) performed exceptionally poorly, evidenced by negative mean R-squared values, suggesting it was a very poor fit for the data.\n",
        "*   The Random Forest Regressor was explicitly identified as the best-performing model based on the mean R-squared score from cross-validation.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The Random Forest Regressor, being the best-performing model, should be considered as the primary candidate for deployment or further fine-tuning.\n",
        "*   Investigate the specific reasons for SVR's poor performance (e.g., inappropriate kernel, scaling issues not fully addressed, or simply not suitable for this dataset's characteristics).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4d8675a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Re-define feature_sets if not already in memory (from previous steps)\n",
        "# Ensure these variables are available: features_vif_filtered, features_cmim, features_mdg, features_anova, features_sbe, features_lasso, features_boruta\n",
        "\n",
        "# 1. Define features_vif_filtered (from X_after_drop, which had total_tested removed previously)\n",
        "# X_after_drop was created in cell dd10391c\n",
        "features_vif_filtered = set(['positive', 'active', 'hospitalized', 'hospitalizedCurr', 'recovered', 'death', 'daily_tested'])\n",
        "\n",
        "# 2. Define features_cmim (from cmim_ranked_features)\n",
        "# cmim_ranked_features was created in cell cf556718\n",
        "features_cmim = set(['daily_tested', 'positive', 'hospitalized', 'death', 'hospitalizedCurr', 'active', 'recovered'])\n",
        "\n",
        "# 3. Define features_mdg (from feature_importance_df)\n",
        "# feature_importance_df was created in cell 7aa6a5f2\n",
        "features_mdg = set(['positive', 'death', 'daily_tested', 'total_tested', 'hospitalized', 'active', 'recovered', 'hospitalizedCurr'])\n",
        "\n",
        "# 4. Define features_anova (from selected_features_anova)\n",
        "# selected_features_anova was created in cell 1d18dc69\n",
        "features_anova = set(['positive', 'daily_tested', 'total_tested', 'death', 'hospitalizedCurr', 'active', 'recovered', 'hospitalized'])\n",
        "\n",
        "# 5. Define features_sbe (from sbe_selected_features)\n",
        "# sbe_selected_features was created in cell d1840026\n",
        "features_sbe = set(['positive', 'daily_tested', 'total_tested', 'death', 'hospitalizedCurr', 'active', 'recovered', 'hospitalized'])\n",
        "\n",
        "# 6. Define features_lasso (from selected_features_lasso)\n",
        "# selected_features_lasso was created in cell 1782a522\n",
        "features_lasso = set(['death'])\n",
        "\n",
        "# 7. Define features_boruta (from selected_strong_features)\n",
        "# selected_strong_features was created in cell 716989ad\n",
        "features_boruta = set(['positive', 'active', 'hospitalized', 'hospitalizedCurr', 'recovered', 'death', 'total_tested', 'daily_tested'])\n",
        "\n",
        "# Create a dictionary of features for each method (assuming these variables are defined from previous steps)\n",
        "feature_sets_summary = {\n",
        "    'VIF': list(features_vif_filtered),\n",
        "    'CMIM': list(features_cmim),\n",
        "    'MDG': list(features_mdg),\n",
        "    'ANOVA': list(features_anova),\n",
        "    'SBE': list(features_sbe),\n",
        "    'Lasso': list(features_lasso),\n",
        "    'Boruta': list(features_boruta)\n",
        "}\n",
        "\n",
        "# Convert to DataFrame for better display\n",
        "feature_sets_df = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in feature_sets_summary.items() ])).fillna('')\n",
        "\n",
        "print(\"### Features Selected by Each Method\")\n",
        "print(feature_sets_df.to_markdown(index=False))\n",
        "\n",
        "# Identify common features\n",
        "common_vif_cmim = features_vif_filtered.intersection(features_cmim)\n",
        "common_mdg_anova_sbe_boruta = features_mdg.intersection(features_anova).intersection(features_sbe).intersection(features_boruta)\n",
        "common_across_groups = common_vif_cmim.intersection(common_mdg_anova_sbe_boruta)\n",
        "\n",
        "print(\"\\n### Common Features Across Methods\")\n",
        "common_features_data = {\n",
        "    'Group (VIF, CMIM)': list(common_vif_cmim),\n",
        "    'Group (MDG, ANOVA, SBE, Boruta)': list(common_mdg_anova_sbe_boruta),\n",
        "    'Common to Both Groups': list(common_across_groups),\n",
        "    'Lasso Only': list(features_lasso) # Lasso was distinct and only selected 'death'\n",
        "}\n",
        "\n",
        "common_features_df = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in common_features_data.items() ])).fillna('')\n",
        "print(common_features_df.to_markdown(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe35eccc"
      },
      "source": [
        "# Comparison\n",
        "Compare the feature sets identified by VIF, CMIM, MDG, ANOVA, SBE, Lasso, and Boruta-VI methods for the COVID-19 dataset. This involves consolidating the selected features from each method, calculating the Jaccard Index for all pairwise combinations to quantify their similarity, visualizing these similarities using a heatmap, and interpreting the results to understand which methods yield similar or distinct feature sets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be95e793"
      },
      "source": [
        "**Action**:\n",
        "We need to consolidate the selected features from each feature selection method into distinct sets and then store them in a dictionary as per the subtask instructions. I will retrieve the features from the previously defined variables, convert them to sets, and then combine them into a final dictionary.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aab0dcd5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 1. Define features_vif_filtered\n",
        "features_vif_filtered = set(X_after_drop.columns.to_list())\n",
        "\n",
        "# 2. Define features_cmim\n",
        "features_cmim = set(cmim_ranked_features)\n",
        "\n",
        "# 3. Define features_mdg\n",
        "features_mdg = set(feature_importance_df['feature'].to_list())\n",
        "\n",
        "# 4. Define features_anova\n",
        "features_anova = set(selected_features_anova['feature'].to_list())\n",
        "\n",
        "# 5. Define features_sbe\n",
        "features_sbe = set(sbe_selected_features)\n",
        "\n",
        "# 6. Define features_lasso\n",
        "features_lasso = set(selected_features_lasso)\n",
        "\n",
        "# 7. Define features_boruta\n",
        "features_boruta = set(selected_strong_features)\n",
        "\n",
        "# 8. Create a dictionary named feature_sets\n",
        "feature_sets = {\n",
        "    'VIF': features_vif_filtered,\n",
        "    'CMIM': features_cmim,\n",
        "    'MDG': features_mdg,\n",
        "    'ANOVA': features_anova,\n",
        "    'SBE': features_sbe,\n",
        "    'Lasso': features_lasso,\n",
        "    'Boruta': features_boruta\n",
        "}\n",
        "\n",
        "# 9. Print the feature_sets dictionary\n",
        "print(\"Features selected by each method:\")\n",
        "for method, features in feature_sets.items():\n",
        "    print(f\"{method}: {features}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec938aa4"
      },
      "source": [
        "**Action**:\n",
        "Now that the feature sets for each method are consolidated, we will calculate the Jaccard Index for all pairwise combinations to quantify their similarity, as specified in the main task. This involves defining a function for Jaccard Index, iterating through all unique pairs of feature sets, calculating their similarity, and storing the results in a DataFrame for clear visualization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcb693ef"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Function to calculate Jaccard Index\n",
        "def jaccard_index(set1, set2):\n",
        "    intersection = len(set1.intersection(set2))\n",
        "    union = len(set1.union(set2))\n",
        "    if union == 0:\n",
        "        return 0.0  # Handle case where both sets are empty\n",
        "    return intersection / union\n",
        "\n",
        "# Get the names of the feature selection methods\n",
        "methods = list(feature_sets.keys())\n",
        "\n",
        "# Initialize a dictionary to store Jaccard Index results\n",
        "jaccard_similarity_matrix = pd.DataFrame(index=methods, columns=methods, dtype=float)\n",
        "\n",
        "# Calculate Jaccard Index for all pairwise combinations\n",
        "for i in range(len(methods)):\n",
        "    for j in range(i, len(methods)):\n",
        "        method1_name = methods[i]\n",
        "        method2_name = methods[j]\n",
        "\n",
        "        set1 = feature_sets[method1_name]\n",
        "        set2 = feature_sets[method2_name]\n",
        "\n",
        "        similarity = jaccard_index(set1, set2)\n",
        "\n",
        "        jaccard_similarity_matrix.loc[method1_name, method2_name] = similarity\n",
        "        jaccard_similarity_matrix.loc[method2_name, method1_name] = similarity # Symmetric matrix\n",
        "\n",
        "print(\"Jaccard Similarity Matrix between Feature Selection Methods:\")\n",
        "print(jaccard_similarity_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4191995"
      },
      "source": [
        "**Action**:\n",
        "Now that the Jaccard similarity matrix has been computed, I will visualize it using a heatmap to easily identify which feature selection methods yield similar or distinct feature sets, as specified in the main task. This involves using `seaborn.heatmap` and `matplotlib.pyplot` for visualization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6f247a4"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Visualize the Jaccard similarity matrix as a heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(jaccard_similarity_matrix, annot=True, cmap='viridis', fmt=\".2f\", linewidths=.5)\n",
        "plt.title('Jaccard Similarity Heatmap of Feature Selection Methods')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tz8tww4YC4gk"
      },
      "source": [
        "features_correlation = set(selected_features_correlation)\n",
        "\n",
        "print(\"Defined 'features_correlation' set:\")\n",
        "print(features_correlation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFE-tWsAG3dX"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Ensure all feature sets are correctly defined\n",
        "# (Assuming these variables are available from previous steps)\n",
        "# features_vif_filtered = set(X_after_drop.columns.to_list())\n",
        "# features_cmim = set(cmim_ranked_features)\n",
        "# features_mdg = set(feature_importance_df['feature'].to_list())\n",
        "# features_anova = set(selected_features_anova['feature'].to_list())\n",
        "# features_sbe = set(sbe_selected_features)\n",
        "# features_lasso = set(selected_features_lasso)\n",
        "# features_boruta = set(selected_strong_features)\n",
        "# features_correlation = set(selected_features_correlation)\n",
        "\n",
        "# 1. Re-create the feature_sets dictionary, explicitly including 'Correlation'\n",
        "feature_sets = {\n",
        "    'VIF': features_vif_filtered,\n",
        "    'CMIM': features_cmim,\n",
        "    'MDG': features_mdg,\n",
        "    'ANOVA': features_anova,\n",
        "    'SBE': features_sbe,\n",
        "    'Lasso': features_lasso,\n",
        "    'Boruta': features_boruta,\n",
        "    'Correlation': features_correlation # Add correlation feature set\n",
        "}\n",
        "\n",
        "# Function to calculate Jaccard Index (re-defined for clarity in this block)\n",
        "def jaccard_index(set1, set2):\n",
        "    intersection = len(set1.intersection(set2))\n",
        "    union = len(set1.union(set2))\n",
        "    if union == 0:\n",
        "        return 0.0\n",
        "    return intersection / union\n",
        "\n",
        "# Get the names of the feature selection methods\n",
        "methods = list(feature_sets.keys())\n",
        "\n",
        "# 2. Re-initialize and re-calculate the Jaccard Index matrix\n",
        "jaccard_similarity_matrix = pd.DataFrame(index=methods, columns=methods, dtype=float)\n",
        "\n",
        "for i in range(len(methods)):\n",
        "    for j in range(i, len(methods)):\n",
        "        method1_name = methods[i]\n",
        "        method2_name = methods[j]\n",
        "\n",
        "        set1 = feature_sets[method1_name]\n",
        "        set2 = feature_sets[method2_name]\n",
        "\n",
        "        similarity = jaccard_index(set1, set2)\n",
        "\n",
        "        jaccard_similarity_matrix.loc[method1_name, method2_name] = similarity\n",
        "        jaccard_similarity_matrix.loc[method2_name, method1_name] = similarity # Symmetric matrix\n",
        "\n",
        "print(\"Re-calculated Jaccard Similarity Matrix between Feature Selection Methods:\")\n",
        "print(jaccard_similarity_matrix)\n",
        "\n",
        "# 3. Retrieve and print the requested pairwise Jaccard Index values\n",
        "mdg_boruta_jaccard = jaccard_similarity_matrix.loc['MDG', 'Boruta']\n",
        "correlation_boruta_jaccard = jaccard_similarity_matrix.loc['Correlation', 'Boruta']\n",
        "cmim_correlation_jaccard = jaccard_similarity_matrix.loc['CMIM', 'Correlation']\n",
        "\n",
        "print(f\"\\nJaccard Index (MDG vs. Boruta-VI): {mdg_boruta_jaccard:.2f}\")\n",
        "print(f\"Jaccard Index (Correlation vs. Boruta-VI): {correlation_boruta_jaccard:.2f}\")\n",
        "print(f\"Jaccard Index (CMIM vs. Correlation): {cmim_correlation_jaccard:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "857dd169"
      },
      "source": [
        "### Interpretation of Jaccard Similarity Heatmap\n",
        "\n",
        "The Jaccard similarity heatmap provides a quantitative measure of the overlap between the feature sets selected by different methods. The values range from 0 (no common features) to 1 (identical feature sets).\n",
        "\n",
        "**Key Observations:**\n",
        "\n",
        "*   **High Similarity (Value = 1.00):**\n",
        "    *   **VIF and CMIM**: These two methods identified identical feature sets. This is an interesting finding, suggesting that for this dataset, features with low multicollinearity (VIF criteria) are also highly informative and non-redundant with respect to the target variable (CMIM criteria).\n",
        "    *   **MDG, ANOVA, SBE, and Boruta**: These four methods also show perfect agreement (Jaccard Index of 1.00) among themselves. This indicates that the features deemed important by Random Forest's Mean Decrease Gini (MDG), statistically significant by ANOVA, retained through Sequential Backward Elimination (SBE), and confirmed by the robust Boruta-VI algorithm are largely the same.\n",
        "\n",
        "*   **Moderate to High Similarity (Value = 0.88):**\n",
        "    *   There is a high degree of similarity (0.88) between the group (VIF, CMIM) and the group (MDG, ANOVA, SBE, Boruta). This implies that most features selected by VIF/CMIM are also selected by MDG/ANOVA/SBE/Boruta, and vice versa. The slight difference suggests one or two features might have been included or excluded by one group but not the other.\n",
        "\n",
        "*   **Low Similarity (Value <= 0.14):**\n",
        "    *   **Lasso vs. All Other Methods**: Lasso regularization shows very low similarity (0.12 to 0.14) with all other feature selection methods. This is expected because Lasso, with the chosen `alpha` value (0.01), performed aggressive feature selection, retaining only 'death'. This indicates that while 'death' is a strong predictor, the other methods identified a broader set of relevant features that collectively contribute to the model's predictive power without such strong regularization.\n",
        "\n",
        "**Overall Interpretation:**\n",
        "\n",
        "The heatmap clearly delineates two main clusters of feature selection methods that yield highly similar results: (VIF, CMIM) and (MDG, ANOVA, SBE, Boruta). These two clusters themselves show strong overlap. Lasso, on the other hand, stands out as an outlier due to its inherent aggressive selection mechanism, which is designed to produce sparse models. This comparison highlights the trade-offs and differing philosophies of various feature selection techniques. While filter methods (VIF, CMIM, ANOVA) and wrapper/embedded methods (MDG, SBE, Boruta) tend to agree on a larger set of relevant features, strong regularization methods like Lasso might prioritize a minimal set of the most robust predictors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JlKFLAsHYJl"
      },
      "source": [
        "## Hence:\n",
        "\n",
        "*   **MDG Random Forest vs. Boruta-VI**: The Jaccard Index between the feature set identified by MDG and Boruta-VI is 1.00, indicating perfect overlap. This means both methods selected the exact same set of features.\n",
        "*   **Correlation vs. Boruta-VI**: The Jaccard Index between the feature set identified by the Pearson correlation method and Boruta-VI is 1.00. This also indicates perfect overlap, suggesting that the features selected by these two methods are identical.\n",
        "*   **CMIM vs. Correlation**: The Jaccard Index between the feature set identified by CMIM and the Correlation method is 0.88. This signifies a high degree of overlap, with most features common to both sets, but a slight difference in one or two features.\n",
        "*   **Non-inclusion of 'Hybrid ABL'**: The feature set 'Hybrid ABL' was not included in these comparisons as it was not part of the feature selection methods evaluated in this analysis.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The Jaccard Index between the feature sets identified by MDG (Mean Decrease Gini from Random Forest) and Boruta-VI is 1.00, indicating perfect overlap.\n",
        "*   The Jaccard Index between the feature set identified by the Pearson correlation method and Boruta-VI is 1.00, also showing perfect overlap.\n",
        "*   The Jaccard Index between the feature set identified by CMIM and the Correlation method is 0.88, indicating a high degree of overlap.\n",
        "*   A broader analysis across all methods revealed perfect overlap (Jaccard Index of 1.00) between VIF and CMIM methods, and also among MDG, ANOVA, SBE, and Boruta methods.\n",
        "*   The group of methods (VIF, CMIM) showed a high degree of similarity (Jaccard Index of 0.88) with the group (MDG, ANOVA, SBE, Boruta), suggesting strong consistency.\n",
        "*   Lasso regularization exhibited very low similarity (ranging from 0.12 to 0.14) with all other feature selection methods, primarily retaining only the 'death' feature due to its aggressive selection mechanism.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The perfect or high overlap (Jaccard Index of 1.00 or 0.88) among multiple diverse feature selection methods (MDG, Boruta-VI, ANOVA, SBE, VIF, CMIM, and Correlation) suggests a robust and highly relevant core set of features for this dataset.\n",
        "*   Lasso's distinct, aggressive feature selection, primarily identifying 'death', highlights its utility when model sparsity and interpretability are paramount, potentially indicating 'death' as the single most impactful predictor in a parsimonious model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7318a2a9"
      },
      "source": [
        "## Comparison B\n",
        "\n",
        "Summarize the comparison of feature selection methods using the Jaccard Index, including the visualization and explanation of feature similarity.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e14d8de"
      },
      "source": [
        "\n",
        "The analysis revealed that VIF and CMIM methods yield identical feature sets. Similarly, MDG, ANOVA, SBE, and Boruta methods also produce identical feature sets among themselves. These two groups (VIF/CMIM and MDG/ANOVA/SBE/Boruta) show high similarity to each other, indicating a strong consensus on relevant features.\n",
        "\n",
        "In contrast, the Lasso method yields a distinct and much smaller feature set, showing very low similarity with all other methods.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   **Identical Feature Sets**:\n",
        "    *   VIF and CMIM methods identified identical feature sets, both selecting 7 features including `death`, `hospitalizedCurr`, `active`, `hospitalized`, `positive`, `daily_tested`, and `recovered`.\n",
        "    *   MDG, ANOVA, SBE, and Boruta methods also yielded identical feature sets among themselves, all selecting 8 features, which included all features from the VIF/CMIM set plus `total_tested`.\n",
        "*   **High Inter-Group Similarity**: The Jaccard Index between the (VIF, CMIM) group and the (MDG, ANOVA, SBE, Boruta) group was 0.88, indicating a high degree of overlap and agreement on most selected features across these distinct methodological approaches.\n",
        "*   **Distinct Lasso Selection**: The Lasso regularization method demonstrated very low Jaccard Index values (ranging from 0.12 to 0.14) when compared to all other feature selection methods. Lasso aggressively selected only one feature, `death`, highlighting its parsimonious nature for the chosen regularization strength.\n",
        "*   A heatmap visualization clearly depicted these similarities and distinctions, illustrating clusters of methods with high agreement and isolating Lasso as an outlier.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The strong agreement among diverse methods (filter, wrapper, embedded) regarding a substantial set of features suggests that these common features are robust and highly relevant predictors for the COVID-19 dataset.\n",
        "\n",
        "*   Further investigation could involve training predictive models using the feature set identified by the highly similar groups (VIF/CMIM/MDG/ANOVA/SBE/Boruta) and comparing their performance against a model trained with only the `death` feature selected by Lasso, to evaluate the trade-off between model complexity and predictive power.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4aa4070"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Combine the selected features (X_scaled) and the target variable (y) into a single DataFrame\n",
        "# X_scaled already contains the features selected by Boruta-VI (selected_strong_features list matches X_scaled columns)\n",
        "df_boruta_plus_target = pd.concat([X_scaled, y], axis=1)\n",
        "\n",
        "# Calculate the correlation matrix for this combined DataFrame\n",
        "correlation_matrix_boruta = df_boruta_plus_target.corr()\n",
        "\n",
        "# Generate the correlation heatmap\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(correlation_matrix_boruta, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Correlation Matrix of Boruta-VI Selected Features with Daily Positive Cases')\n",
        "plt.show()\n",
        "\n",
        "print(\"Correlation matrix heatmap for Boruta-VI selected features and daily positive cases generated.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6987964b"
      },
      "source": [
        "### Interpretation of Correlation Matrix for Boruta-VI Selected Features\n",
        "\n",
        "This heatmap visualizes the pairwise Pearson correlation coefficients between the features identified as significant by the Boruta-VI method and the target variable, `daily_positive`.\n",
        "\n",
        "**Key Observations:**\n",
        "\n",
        "*   **Strong Positive Correlations with `daily_positive`:**\n",
        "    *   `daily_tested` (0.93): The strongest positive correlation with daily positive cases, which is highly intuitive as more testing generally leads to more detected cases.\n",
        "    *   `positive` (0.88): High correlation, indicating that the total positive cases are strongly related to daily new positive cases.\n",
        "    *   `death` (0.88): Strong positive correlation, suggesting that an increase in daily positive cases is associated with an increase in deaths, likely with some lag.\n",
        "\n",
        "*   **Moderate Positive Correlations with `daily_positive`:**\n",
        "    *   `hospitalizedCurr` (0.57): Moderate correlation, indicating that current hospitalizations are related to daily new cases.\n",
        "    *   `recovered` (0.49): Moderate correlation, as daily positive cases would eventually lead to recoveries.\n",
        "    *   `active` (0.48): Moderate correlation, as daily new cases contribute to the active case count.\n",
        "    *   `hospitalized` (0.35): A weaker but still positive correlation with total hospitalizations.\n",
        "\n",
        "*   **Inter-Feature Correlations (among Boruta-VI selected features):**\n",
        "    *   Many features exhibit strong inter-correlations, for example, `positive` is highly correlated with `death` (0.96), `total_tested` (0.99), and `daily_tested` (0.94). This indicates a high degree of multicollinearity within the dataset, even among the features deemed important by Boruta-VI. While Boruta selects relevant features, it does not specifically aim to reduce multicollinearity.\n",
        "\n",
        "**Overall Interpretation:**\n",
        "\n",
        "The heatmap confirms that all features selected by Boruta-VI have a positive correlation with `daily_positive`, validating their relevance to the target variable. The highest correlations are, as expected, with `daily_tested`, `positive`, and `death`. The presence of strong inter-feature correlations (e.g., between `positive`, `total_tested`, `death`, and `daily_tested`) suggests that while these features are individually and collectively important for predicting `daily_positive`, they also share a lot of common information. This might impact the interpretability of linear models if not addressed, but for tree-based models (like Random Forest and XGBoost which performed well), it's less of a concern as they can handle some level of multicollinearity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0e52732"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Convert aggregated_cv_results dictionary to a DataFrame\n",
        "aggregated_cv_df = pd.DataFrame(aggregated_cv_results).T\n",
        "\n",
        "# Prepare data for plotting by melting the DataFrame\n",
        "# We want to plot mean MAE, mean MSE, and mean R-squared\n",
        "plot_df = aggregated_cv_df[['MAE_mean', 'MSE_mean', 'R-squared_mean']]\n",
        "plot_df.columns = ['MAE', 'MSE', 'R-squared'] # Rename columns for clarity in the plot\n",
        "\n",
        "metrics_melted = plot_df.reset_index().melt(id_vars='index', var_name='Metric', value_name='Value')\n",
        "metrics_melted = metrics_melted.rename(columns={'index': 'Model'})\n",
        "\n",
        "# Create line plots for MAE, MSE, and R-squared scores\n",
        "plt.figure(figsize=(16, 8))\n",
        "sns.lineplot(data=metrics_melted, x='Model', y='Value', hue='Metric', marker='o')\n",
        "plt.title('Cross-Validation Performance: Mean MAE, MSE, and R-squared (Line Plot)')\n",
        "plt.xlabel('Regression Model')\n",
        "plt.ylabel('Metric Value')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.legend(title='Metric')\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Line plot visualizing cross-validation performance metrics generated.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-WhDV8T0-Xu"
      },
      "source": [
        "### Interpretation of Cross-Validation Performance Line Plot\n",
        "\n",
        "This line plot visualizes the mean Mean Absolute Error (MAE), Mean Squared Error (MSE), and R-squared scores for each regression model across the cross-validation folds. The curves allow for a quick comparison of model stability and overall performance.\n",
        "\n",
        "**Key Observations:**\n",
        "\n",
        "*   **R-squared (Green Line)**:\n",
        "    *   Most ensemble models (Random Forest Regressor, Bagging Regressor, XGBoost Regressor, Gradient Boosting Regressor) and the Decision Tree Regressor show very high and stable R-squared values, consistently close to 1.0. This indicates excellent predictive power and good generalization. Random Forest Regressor and Bagging Regressor are at the top, very close to 1.0.\n",
        "    *   K-Nearest Neighbors also performs very well with a high R-squared.\n",
        "    *   Linear Regression and Lasso Regression show moderate R-squared values, indicating decent but not superior performance compared to ensemble methods.\n",
        "    *   Support Vector Regressor (SVR) stands out with a significantly negative R-squared value (around -1.6), indicating extremely poor performance. This model's predictions are worse than simply predicting the mean of the target variable.\n",
        "\n",
        "*   **MAE (Blue Line) and MSE (Orange Line)**:\n",
        "    *   For MAE and MSE, lower values indicate better performance. The ensemble models (Random Forest, Bagging, XGBoost, Gradient Boosting, Decision Tree, and K-Nearest Neighbors) all exhibit very low MAE and MSE values, generally clustered near 0.0, which aligns with their high R-squared scores.\n",
        "    *   Linear Regression and Lasso Regression have slightly higher MAE and MSE values, correlating with their lower R-squared scores.\n",
        "    *   SVR again shows the worst performance, with MAE and MSE values significantly higher than all other models, confirming its inadequacy for this task under the given configuration.\n",
        "\n",
        "**Overall Interpretation:**\n",
        "\n",
        "The line plot clearly illustrates that **Random Forest Regressor** and other tree-based ensemble methods (**Bagging, XGBoost, Gradient Boosting, Decision Tree**) are the top performers in terms of accuracy (R-squared) and error metrics (MAE, MSE) across the cross-validation folds. Their R-squared curves are high and flat, indicating both high performance and stability. The K-Nearest Neighbors also performs very well. In stark contrast, the Support Vector Regressor (SVR) consistently performs the worst. This visualization reinforces the findings from the numerical summary, making it easier to identify the most robust and accurate models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba6171f3"
      },
      "source": [
        "### Accuracy Outputs and Confidence Intervals from Cross-Validation\n",
        "\n",
        "The following table summarizes the mean and standard deviation for R-squared (as a measure of accuracy), Mean Absolute Error (MAE), and Mean Squared Error (MSE) for all implemented regression models based on the 3-times repeated 3-fold cross-validation. The standard deviation provides an indication of the consistency and confidence in the mean performance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24b53f01"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Ensure aggregated_cv_df is available from previous steps\n",
        "if 'aggregated_cv_df' not in locals():\n",
        "    aggregated_cv_df = pd.DataFrame(aggregated_cv_results).T\n",
        "\n",
        "# Display the aggregated cross-validation results, sorted by R-squared_mean\n",
        "# This table will highlight Random Forest's performance.\n",
        "print(\"**Aggregated Cross-Validation Results (Sorted by Mean R-squared):**\")\n",
        "display(aggregated_cv_df.sort_values(by='R-squared_mean', ascending=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc7c98f5"
      },
      "source": [
        "### Conclusion on Random Forest Performance\n",
        "\n",
        "Based on the cross-validation results, the **Random Forest Regressor** consistently exhibits the highest mean R-squared value of **0.9738**, coupled with a very low standard deviation of **0.0058**. This indicates that not only did Random Forest achieve the best overall accuracy, but its performance was also highly stable and consistent across different folds and repetitions of the cross-validation. This robust performance suggests that Random Forest is indeed the top-performing algorithm among those evaluated for this specific task and feature set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "991d3078"
      },
      "source": [
        "# Task\n",
        "### Interpretation of Cross-Validation Performance Line Plot\n",
        "\n",
        "This line plot visualizes the mean Mean Absolute Error (MAE), Mean Squared Error (MSE), and R-squared scores for each regression model across the cross-validation folds. The curves allow for a quick comparison of model stability and overall performance.\n",
        "\n",
        "**Key Observations:**\n",
        "\n",
        "*   **R-squared (Green Line)**:\n",
        "    *   **Best Performance**: The **Random Forest Regressor** shows the highest R-squared value, very close to 1.0 (specifically 0.9738 from the aggregated results), indicating it explains the largest proportion of variance in the target variable and is the best fit among the models. Closely following are Bagging Regressor (0.9692), XGBoost Regressor (0.9668), Decision Tree Regressor (0.9515), and K-Nearest Neighbors (0.9461), all demonstrating excellent predictive power.\n",
        "    *   **Moderate Performance**: Linear Regression (0.8129) and Lasso Regression (0.7973) exhibit good but comparatively lower R-squared values, suggesting a decent fit but not as strong as the ensemble methods.\n",
        "    *   **Worst Performance**: The **Support Vector Regressor (SVR)** stands out with a significantly negative R-squared value (around -1.6368). This indicates extremely poor performance, implying that its predictions are worse than simply predicting the mean of the target variable. Gradient Boosting Regressor also performed relatively poorly in terms of R-squared (-0.0342 on the single split but 0.9613 in CV, though still lower than others in the ensemble group).\n",
        "\n",
        "*   **MAE (Blue Line) and MSE (Orange Line)**:\n",
        "    *   **Best Performance**: Consistent with their high R-squared values, Random Forest Regressor (MAE: 0.0011, MSE: 0.000026), Bagging Regressor (MAE: 0.0012, MSE: 0.000030), XGBoost Regressor (MAE: 0.0013, MSE: 0.000034), Decision Tree Regressor (MAE: 0.0014, MSE: 0.000047), K-Nearest Neighbors (MAE: 0.0014, MSE: 0.000052), and Gradient Boosting Regressor (MAE: 0.0017, MSE: 0.000056) all exhibit very low MAE and MSE values, clustered near 0.0. This signifies high accuracy with minimal prediction errors.\n",
        "    *   **Moderate Performance**: Linear Regression (MAE: 0.0039, MSE: 0.000181) and Lasso Regression (MAE: 0.0041, MSE: 0.000197) have slightly higher MAE and MSE values, aligning with their lower R-squared scores.\n",
        "    *   **Worst Performance**: The **Support Vector Regressor (SVR)** again shows the worst performance, with MAE (0.0463) and MSE (0.0025) values significantly higher than all other models, further confirming its inadequacy for this task under the given configuration.\n",
        "\n",
        "**Overall Interpretation:**\n",
        "\n",
        "The line plot clearly illustrates that **Random Forest Regressor** and other tree-based ensemble methods (**Bagging, XGBoost, Gradient Boosting, Decision Tree**) are the top performers in terms of accuracy (R-squared) and error metrics (MAE, MSE) across the cross-validation folds. Their R-squared curves are high and flat, indicating both high performance and stability. The K-Nearest Neighbors also performs very well. In stark contrast, the Support Vector Regressor (SVR) consistently performs the worst. This visualization reinforces the findings from the numerical summary, making it easier to identify the most robust and accurate models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "682f0eb9"
      },
      "source": [
        "\n",
        "### Key Findings\n",
        "\n",
        "*   **Top-Performing Models**: The Random Forest Regressor consistently demonstrated the best performance, achieving the highest R-squared value of 0.9738 and the lowest error metrics (MAE: 0.0011, MSE: 0.000026). Other ensemble tree-based methods like Bagging Regressor (R-squared: 0.9692), XGBoost Regressor (R-squared: 0.9668), Decision Tree Regressor (R-squared: 0.9515), and K-Nearest Neighbors (R-squared: 0.9461) also showed excellent and stable performance with very low MAE and MSE values.\n",
        "*   **Moderate Performance**: Linear Regression (R-squared: 0.8129, MAE: 0.0039, MSE: 0.000181) and Lasso Regression (R-squared: 0.7973, MAE: 0.0041, MSE: 0.000197) provided good, but comparatively lower, predictive power compared to the ensemble models.\n",
        "*   **Worst-Performing Model**: The Support Vector Regressor (SVR) exhibited extremely poor performance, indicated by a significantly negative R-squared value (around -1.6368) and the highest error metrics (MAE: 0.0463, MSE: 0.0025), suggesting it is unsuitable for this task under its current configuration.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   Focus further optimization and hyperparameter tuning on Random Forest Regressor and other top-performing ensemble models, as they offer the most promising results for this prediction task.\n",
        "*   Investigate the configuration or suitability of the Support Vector Regressor, as its performance is exceptionally poor, potentially requiring significant parameter adjustments or indicating its unsuitability for the dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7985dc4b"
      },
      "source": [
        "## Residual Plot for Random Forest Regressor\n",
        "\n",
        "We'll plot the residuals (actual - predicted) against the predicted values for the Random Forest Regressor to assess the model's performance and assumptions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cd127e83"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Assuming X_train_scaled, y_train, X_test_scaled, y_test are already defined\n",
        "# and rf_model is the trained Random Forest Regressor from previous steps\n",
        "\n",
        "# Retrain the Random Forest model (if not already done or to ensure consistency)\n",
        "# with the hyperparameters identified earlier\n",
        "rf_model = RandomForestRegressor(n_estimators=700, max_features=3, random_state=42, n_jobs=-1)\n",
        "rf_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Get predictions on the test set\n",
        "rf_predictions = rf_model.predict(X_test_scaled)\n",
        "\n",
        "# Calculate residuals\n",
        "residuals = y_test - rf_predictions\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "sns.scatterplot(x=rf_predictions, y=residuals, alpha=0.6)\n",
        "plt.axhline(y=0, color='red', linestyle='--', linewidth=2, label='Zero Residuals')\n",
        "plt.title('Residual Plot for Random Forest Regressor')\n",
        "plt.xlabel('Predicted Daily Positive Cases')\n",
        "plt.ylabel('Residuals (Actual - Predicted)')\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Residual plot for Random Forest Regressor generated.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f93e8978"
      },
      "source": [
        "### Interpretation of the Residual Plot\n",
        "\n",
        "In an ideal residual plot, the points should be randomly scattered around the horizontal line at zero, with no discernible patterns. This indicates that the model is capturing the underlying patterns in the data well and that the errors are random.\n",
        "\n",
        "**Key Observations:**\n",
        "\n",
        "*   **Random Scatter Around Zero**: If the residuals are scattered randomly around the red dashed line (y=0), it suggests that the model is generally performing well and there's no systematic bias in its predictions.\n",
        "*   **Homoscedasticity**: If the spread of the residuals is consistent across the range of predicted values, it implies homoscedasticity (constant variance of errors). If the spread widens or narrows, it indicates heteroscedasticity.\n",
        "*   **No Obvious Patterns**: The absence of any clear patterns (e.g., a curve, a funnel shape, or cycles) suggests that the model has captured most of the systematic information in the data.\n",
        "\n",
        "Let's analyze the generated plot to draw specific conclusions about the Random Forest Regressor's residuals."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "011dc323"
      },
      "source": [
        "## Prediction Interval Plot for Random Forest Regressor\n",
        "\n",
        "Generating prediction intervals for tree-based ensemble models like Random Forest can be done using various methods. A common approach for visualizing uncertainty is to use quantile regression forests or by bootstrapping. For simplicity in visualization, we'll demonstrate a basic way to approximate prediction intervals for a subset of the test data. A full, robust implementation of prediction intervals for Random Forest often involves more complex methods like quantile regression forests or conformal prediction, which are beyond the scope of a quick visualization. However, we can use the variability from individual trees (if `return_std=True` was supported or by manual bootstrapping) to estimate an interval. For this visualization, we'll focus on displaying the predicted values alongside a simple error band for a clearer visual.\n",
        "\n",
        "To make the plot readable, we will visualize prediction intervals for a smaller, representative subset of the test data (e.g., the first 100 predictions)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03010871"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Assuming X_train_scaled, y_train, X_test_scaled, y_test are already defined\n",
        "# and rf_model is the trained Random Forest Regressor\n",
        "\n",
        "# Function to get predictions and a simple standard deviation estimate\n",
        "# from individual trees for prediction intervals\n",
        "def predict_with_intervals(model, X):\n",
        "    predictions = []\n",
        "    for tree in model.estimators_:\n",
        "        predictions.append(tree.predict(X))\n",
        "    predictions = np.array(predictions)\n",
        "\n",
        "    mean_prediction = np.mean(predictions, axis=0)\n",
        "    std_prediction = np.std(predictions, axis=0) # Standard deviation of tree predictions\n",
        "\n",
        "    # For a 95% interval, we can use approximately 1.96 * std_dev\n",
        "    # This is a simplified approach, more robust methods exist.\n",
        "    lower_bound = mean_prediction - 1.96 * std_prediction\n",
        "    upper_bound = mean_prediction + 1.96 * std_prediction\n",
        "\n",
        "    return mean_prediction, lower_bound, upper_bound\n",
        "\n",
        "# Get predictions and intervals for a subset of the test data\n",
        "subset_size = 100\n",
        "X_test_subset = X_test_scaled.iloc[:subset_size]\n",
        "y_test_subset = y_test.iloc[:subset_size]\n",
        "\n",
        "mean_rf_predictions, lower_bounds, upper_bounds = predict_with_intervals(rf_model, X_test_subset)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(15, 8))\n",
        "plt.plot(y_test_subset.index, y_test_subset, label='Actual Values', color='blue', marker='o', linestyle='-')\n",
        "plt.plot(y_test_subset.index, mean_rf_predictions, label='RF Predicted Values', color='red', marker='x', linestyle='--')\n",
        "plt.fill_between(\n",
        "    y_test_subset.index,\n",
        "    lower_bounds,\n",
        "    upper_bounds,\n",
        "    color='red',\n",
        "    alpha=0.2,\n",
        "    label='95% Prediction Interval (Approx.)'\n",
        ")\n",
        "\n",
        "plt.title(f'Random Forest Regressor: Actual vs. Predicted with Prediction Intervals (First {subset_size} Test Samples)')\n",
        "plt.xlabel('Date (Test Samples)')\n",
        "plt.ylabel('Daily Positive Cases')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Prediction interval plot for Random Forest Regressor on the first {subset_size} test samples generated.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea1b846e"
      },
      "source": [
        "### Interpretation of the Prediction Interval Plot\n",
        "\n",
        "This plot shows the actual `daily_positive` cases, the Random Forest Regressor's point predictions, and an approximate 95% prediction interval for a subset of the test data.\n",
        "\n",
        "**Key Observations:**\n",
        "\n",
        "*   **Interval Coverage**: Ideally, the blue 'Actual Values' line should fall within the shaded red prediction interval most of the time (e.g., 95% of the time for a 95% interval). This indicates that the model's uncertainty estimates are reliable.\n",
        "*   **Interval Width**: The width of the shaded area represents the uncertainty of the prediction. Narrower intervals suggest more precise forecasts, while wider intervals indicate greater uncertainty, which might occur for more volatile or harder-to-predict data points.\n",
        "*   **Bias**: If the actual values consistently fall above or below the predicted values (and the interval), it could suggest a systematic bias in the model's predictions.\n",
        "\n",
        "Let's analyze the generated plot to understand how well our Random Forest model predicts with its estimated uncertainty."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fc98e99"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Ensure feature_importance_df is available\n",
        "# feature_importance_df was created in cell '7aa6a5f2'\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "sns.barplot(x='feature', y='importance', data=feature_importance_df, palette='viridis')\n",
        "plt.title('Top Features by Mean Decrease Gini Importance')\n",
        "plt.xlabel('Feature')\n",
        "plt.ylabel('Gini Importance')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Column chart of top features by Gini value generated.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ecf8b43"
      },
      "source": [
        "### Interpretation of Top Features by Gini Importance\n",
        "\n",
        "This column chart displays the importance of each feature as determined by the Mean Decrease Gini (MDG) metric from the Random Forest Regressor. A higher Gini importance value indicates that the feature plays a more significant role in reducing impurity across the trees in the forest, and thus, is more influential in predicting the target variable (`daily_positive`).\n",
        "\n",
        "**Key Observations:**\n",
        "\n",
        "*   **Dominant Features**: 'positive' and 'death' stand out as the most important features, with significantly higher Gini importance values compared to others. This suggests they are the strongest predictors of daily positive COVID-19 cases in our model.\n",
        "*   **Next Most Important**: 'daily_tested' and 'total_tested' follow as the next most important features. This is intuitive, as testing directly relates to identifying positive cases.\n",
        "*   **Moderate Importance**: 'hospitalized', 'active', and 'recovered' show moderate levels of importance, contributing to the model's predictions but to a lesser extent than the top features.\n",
        "*   **Least Important**: 'hospitalizedCurr' appears to be the least important feature based on its Gini importance, indicating it has the weakest predictive power among the selected features in this model.\n",
        "\n",
        "**Overall Interpretation:**\n",
        "\n",
        "The visualization confirms that the model heavily relies on the overall `positive` case count and `death` toll for predicting `daily_positive` cases. Features related to testing (`daily_tested`, `total_tested`) also hold substantial predictive weight. This aligns with epidemiological understanding, where current and past case/death data, alongside testing efforts, are critical indicators of the pandemic's progression. The ranking provides valuable insight into which factors the Random Forest model considers most relevant for its forecasts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5wRfj518LmL"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef3018d2"
      },
      "source": [
        "# Task\n",
        "The following is a comprehensive summary of the project, including data preprocessing, exploratory data analysis, feature selection, model development and evaluation, a numerical story, and deployment considerations for the best-performing model.\n",
        "\n",
        "**Summary of Data Preprocessing**\n",
        "The preprocessing pipeline began with loading the \"COVID19 Worldwide Testing Data\" from Kaggle. Initial data inspection using `df.info()` and `df.describe(include='all')` revealed 27,641 entries with 12 columns, and significant missing values across several numerical columns (e.g., 'hospitalized' had 19,231 missing values out of 27,641, 'active' had 9,833 missing, 'daily_positive' had 4,557 missing). The 'Date' column was converted to datetime objects.\n",
        "\n",
        "To handle outliers, `RobustScaler` was applied to the identified numerical columns (`positive`, `active`, `hospitalized`, `hospitalizedCurr`, `recovered`, `death`, `total_tested`, `daily_tested`, `daily_positive`). This step transformed the data to be less sensitive to extreme values.\n",
        "\n",
        "Following outlier handling, missing values were addressed through a two-step imputation process:\n",
        "1.  **Forward-fill (`ffill()`):** The DataFrame was sorted by 'Date' and 'Country_Region', and then `ffill()` was applied grouped by 'Country_Region' to propagate the last valid observation forward within each geographical entity, preserving the time-series context.\n",
        "2.  **Median Imputation:** Any remaining missing values (which could occur at the beginning of a country's data or if a country had entirely missing data for a feature) were imputed using the median of their respective columns. This is a robust imputation strategy, especially after robust scaling.\n",
        "\n",
        "Finally, the numerical features in `df_scaled_robust` were normalized using `MinMaxScaler`, scaling them to a common range [0, 1]. This ensures that features with larger original scales do not disproportionately influence machine learning models. The 'Date' column was then set as the DataFrame index.\n",
        "\n",
        "**Summary of Exploratory Data Analysis (EDA)**\n",
        "EDA revealed several key insights into the COVID-19 dataset:\n",
        "*   **Correlation Heatmap (Initial):** A correlation matrix of numerical features showed strong positive correlations. For instance, 'positive' cases had a **0.99** correlation with 'total_tested', a **0.96** correlation with 'death', and a **0.94** correlation with 'daily_tested'. Similarly, 'death' correlated strongly with 'hospitalized' (**0.94**) and 'daily_tested' (**0.95**). 'Active cases' showed a **0.92** correlation with 'hospitalized' and **0.75** with 'positive'. 'Recovered' cases correlated **0.86** with 'positive'. 'hospitalizedCurr' generally showed weaker correlations (e.g., **0.34** with 'positive', **0.37** with 'death'). 'Daily_positive' had a strong correlation with 'daily_tested' (**0.93**).\n",
        "*   **Time-Series Plots of Daily Positive Cases:** Line plots of 'daily_positive' cases over time for all regions highlighted significant fluctuations and peak periods, particularly from early 2020 through late 2020. Specific country trends for 'United States', 'India', 'Brazil', and 'United Kingdom' showed varied patterns, with the US exhibiting a generally higher and increasing trend.\n",
        "*   **Bar Charts of Total Cases/Deaths by Country:**\n",
        "    *   **Total Positive Cases:** The \"Top 15 Countries/Regions by Total Positive COVID-19 Cases\" bar chart showed the 'United States' with the highest cumulative positive cases (nearly 10 million), followed by Italy (around 1 million), Russia, Bangladesh, and Czechia.\n",
        "    *   **Total Deaths:** The \"Top 15 Countries/Regions by Total COVID-19 Deaths\" bar chart indicated the 'United States' had the highest number of cumulative deaths (over 229,000), significantly more than Italy (around 41,000) and the United Kingdom (around 33,000).\n",
        "*   **Distribution Plot of Daily Positive Cases:** A histogram and KDE plot of 'daily_positive' cases revealed a highly positively (right) skewed distribution, with a calculated skewness value of **11.72**. This indicated that most days had low daily positive case counts, while a smaller number of days experienced very high counts.\n",
        "*   **Choropleth Map:** An interactive choropleth map of cumulative positive cases showed the highest impact in North America (United States), followed by South America (Brazil) and several Western European countries. Many African countries showed lower reported cumulative cases.\n",
        "\n",
        "**Summary of Feature Selection Methods**\n",
        "Seven different feature selection methods were applied and compared:\n",
        "1.  **VIF (Variance Inflation Factor):** After an initial calculation, 'total_tested' was dropped due to high multicollinearity with the target variable ('daily_positive'), with an initial VIF of **3.28** compared to 'daily_positive' which had a VIF of **78.15**. After dropping 'total_tested', the remaining features were: `positive` (VIF: **12.41**), `active` (VIF: **2.05**), `hospitalized` (VIF: **1.89**), `hospitalizedCurr` (VIF: **1.23**), `recovered` (VIF: **1.32**), `death` (VIF: **12.72**), and `daily_tested` (VIF: **1.34**).\n",
        "2.  **CMIM (Conditional Mutual Information Maximization):** This information-theoretical method ranked features based on their ability to predict the target while minimizing redundancy. The ranked features were: `daily_tested`, `positive`, `hospitalized`, `death`, `hospitalizedCurr`, `active`, `recovered`.\n",
        "3.  **MDG (Mean Decrease Gini) / Random Forest Feature Importance:** An optimal `max_features` (`mtry`) of **2** was determined for the Random Forest Regressor based on the lowest Out-of-Bag (OOB) error rate of **0.0216**. The MDG feature importances showed: `positive` (**0.288**), `death` (**0.255**), `daily_tested` (**0.165**), `total_tested` (**0.155**), `hospitalized` (**0.047**), `active` (**0.032**), `recovered` (**0.032**), and `hospitalizedCurr` (**0.025**).\n",
        "4.  **ANOVA (Analysis of Variance):** All 8 independent features (`positive`, `active`, `hospitalized`, `hospitalizedCurr`, `recovered`, `death`, `total_tested`, `daily_tested`) were found to have a statistically significant relationship with 'daily_positive' (p-value < 0.05). The strongest F-statistic was for `positive` (**78957.69**).\n",
        "5.  **SBE (Sequential Backward Elimination):** Starting with ANOVA-selected features, SBE retained all 8 features as statistically significant (p-value <= 0.05).\n",
        "6.  **Lasso Regularization:** With an `alpha=0.01`, Lasso performed aggressive selection, retaining only `death` as a feature with a non-zero coefficient.\n",
        "7.  **Boruta-VI:** This wrapper method, using a `RandomForestRegressor`, identified all 8 independent features as 'strong' features, meaning they were consistently important across the Boruta runs.\n",
        "\n",
        "**Jaccard Index Comparison:**\n",
        "*   **VIF and CMIM** showed **100%** similarity, selecting an identical set of 7 features (all except 'total_tested').\n",
        "*   **MDG, ANOVA, SBE, and Boruta** also showed **100%** similarity among themselves, selecting an identical set of 8 features (including 'total_tested').\n",
        "*   The similarity between the (VIF, CMIM) group and the (MDG, ANOVA, SBE, Boruta) group was **88%**, indicating a strong overlap.\n",
        "*   **Lasso** regularization exhibited very low similarity, ranging from **12% to 14%**, with all other methods, as it only selected 'death'.\n",
        "\n",
        "**Summary of Model Development and Evaluation**\n",
        "Nine regression models were implemented and evaluated for forecasting 'daily_positive' cases.\n",
        "*   **Initial Train-Test Split (70/30 chronological split):**\n",
        "    *   **Best Performers:** **K-Nearest Neighbors Regressor** achieved the best results with an MAE of **0.0045**, MSE of **0.0004**, and an R-squared of **0.7737**. **XGBoost Regressor** followed closely with MAE **0.0049**, MSE **0.0004**, and R-squared **0.7731**.\n",
        "    *   **Moderate Performers:** Lasso Regression (MAE: **0.0074**, MSE: **0.0006**, R-squared: **0.6421**) and Random Forest Regressor (MAE: **0.0066**, MSE: **0.0006**, R-squared: **0.6490**) also performed reasonably well.\n",
        "    *   **Worst Performers:** **Support Vector Regressor (SVR)** performed exceptionally poorly, yielding an MAE of **0.0699**, MSE of **0.0056**, and a negative R-squared of **-2.3748**. Gradient Boosting Regressor (MAE: **0.0095**, MSE: **0.0017**, R-squared: **-0.0342**) and Bagging Regressor (MAE: **0.0091**, MSE: **0.0015**, R-squared: **0.1121**) also showed weak performance.\n",
        "*   **3-times Repeated 3-fold Cross-Validation:**\n",
        "    *   **Best Performers:** The **Random Forest Regressor** emerged as the top model, demonstrating the highest mean R-squared of **0.9738** (±0.0058), lowest mean MAE of **0.0011** (±0.0000), and lowest mean MSE of **0.000026** (±0.0000). Other ensemble tree-based models also performed exceptionally well: Bagging Regressor (R-squared: **0.9692**), XGBoost Regressor (R-squared: **0.9668**), Decision Tree Regressor (R-squared: **0.9515**), and K-Nearest Neighbors (R-squared: **0.9461**).\n",
        "    *   **Moderate Performers:** Linear Regression (R-squared: **0.8129**) and Lasso Regression (R-squared: **0.7973**) showed good but comparatively lower performance.\n",
        "    *   **Worst Performer:** **SVR** continued to perform poorly with a mean R-squared of **-1.6368** (±1.0751).\n",
        "\n",
        "**Story by Numbers and Percentages**\n",
        "*   The dataset initially contained up to **69.57%** (19,231 out of 27,641) missing values in the 'hospitalized' column, which were effectively handled by robust scaling, ffill, and median imputation.\n",
        "*   The initial correlation analysis showed 'daily_tested' had a strong **0.93** correlation with 'daily_positive', highlighting its direct relationship. 'Positive' cases exhibited a **0.96** correlation with 'death'.\n",
        "*   The distribution of 'daily_positive' cases was highly positively skewed, with a skewness of **11.72**, indicating a large number of low-case days and fewer, extreme high-case days.\n",
        "*   Mean Decrease Gini (MDG) importance for the Random Forest Regressor identified 'positive' at **0.288** and 'death' at **0.255** as the most influential features.\n",
        "*   The Jaccard Index demonstrated high similarity between various feature selection methods: VIF and CMIM were **100%** similar, and MDG, ANOVA, SBE, and Boruta were also **100%** similar among themselves. A significant **88%** similarity was observed between these two groups, emphasizing a strong consensus on relevant features. Lasso, however, was highly distinct with only **12-14%** similarity, selecting only 'death' as a feature with its aggressive regularization.\n",
        "*   In model evaluation, the **Random Forest Regressor** achieved a mean R-squared of **0.9738** (MAE: **0.0011**, MSE: **0.000026**) during cross-validation, signifying its superior predictive power. In contrast, the SVR had a mean R-squared of **-1.6368**, performing worse than a simple baseline model.\n",
        "*   The stability of the Random Forest model was further evidenced by a very low standard deviation in R-squared (**0.0058**) across cross-validation folds.\n",
        "\n",
        "**Deployment Considerations for Random Forest Model**\n",
        "For deploying the best-performing Random Forest Regressor model, several considerations are crucial:\n",
        "1.  **Model Export:** The trained `rf_model` (or the retrained model on the full dataset) would be serialized using `pickle` or `joblib` (e.g., `joblib.dump(rf_model, 'random_forest_model.pkl')`).\n",
        "2.  **API Integration:** The exported model could be integrated into a web application or microservice using frameworks like Flask or FastAPI. This API would expose an endpoint to accept new input data and return predictions.\n",
        "3.  **Data Input/Output Formats:** The API should expect input data in a standardized format (e.g., JSON) mirroring the `X_train_scaled` structure. Output predictions would also be JSON. Crucially, incoming data would need to undergo the same preprocessing steps (robust scaling and min-max scaling) as the training data using the *fitted* `RobustScaler` and `MinMaxScaler` objects, which must also be exported and loaded during deployment.\n",
        "4.  **Real-time vs. Batch Predictions:** For real-time predictions, the API should be optimized for low latency. For batch predictions, a scheduled job could process larger datasets offline.\n",
        "5.  **Monitoring for Drift:** Continuous monitoring of model performance (e.g., MAE, R-squared on new data) and data drift (changes in input feature distributions compared to training data) is essential. If performance degrades or drift is detected, the model may need to be retrained or updated.\n",
        "6.  **Scalability:** The deployment infrastructure (e.g., cloud platforms like AWS, Google Cloud, Azure) should support scaling resources to handle varying prediction loads.\n",
        "\n",
        "**Final Task: Overall Project Summary**\n",
        "This project aimed to perform a comparative analysis of feature selection techniques for a COVID-19 dataset and develop robust regression models to forecast daily positive cases. The process began with comprehensive data preprocessing, including outlier handling via `RobustScaler`, sequential missing value imputation (forward-fill then median), and feature normalization using `MinMaxScaler`.\n",
        "\n",
        "Exploratory Data Analysis revealed strong correlations among COVID-19 metrics, highlighting the US as significantly affected, and a highly skewed distribution of daily positive cases. Diverse feature selection methods (VIF, CMIM, MDG, ANOVA, SBE, Lasso, Boruta-VI) were applied. While most methods largely agreed on a set of 7-8 relevant features, Lasso's aggressive regularization uniquely selected only 'death'. Jaccard Index comparisons quantified these similarities, showing high agreement among most methods but significant divergence with Lasso.\n",
        "\n",
        "A suite of nine regression models was implemented and rigorously evaluated using both an initial train-test split and 3-times repeated 3-fold cross-validation. The **Random Forest Regressor** consistently emerged as the best-performing model, achieving an impressive mean R-squared of **0.9738** (MAE: **0.0011**, MSE: **0.000026**) during cross-validation, demonstrating high accuracy and stability. The XGBoost Regressor also showed excellent performance. Conversely, the Support Vector Regressor (SVR) performed exceptionally poorly, proving unsuitable for this task. Regression models were chosen due to the continuous numerical nature of the target variable ('daily_positive' cases), as opposed to classification models which are designed for discrete categorical outcomes.\n",
        "\n",
        "Future directions include further hyperparameter tuning for the top-performing models (Random Forest, XGBoost), exploring ensemble methods or stacking, and investigating the specific reasons for SVR's poor performance. Additionally, the insights from feature selection, particularly the consensus on robust features, can guide the development of more parsimonious and interpretable predictive models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36bc3a1e"
      },
      "source": [
        "## Summary of Data Preprocessing\n",
        "\n",
        "Summary of the data loading, initial data inspection, handling of missing values (forward-fill and median imputation), and outlier detection/scaling (RobustScaler and MinMaxScaler) steps.\n",
        "\n",
        "#### Summary:\n",
        "\n",
        "**1. Data Loading and Initial Inspection**\n",
        "\n",
        "The process began by loading the `COVID19 Worldwide Testing Data` dataset from Kaggle into a pandas DataFrame. Initial inspection revealed a dataset with **27,641 entries** and **12 columns**. Key observations included:\n",
        "*   Many columns, such as `positive`, `active`, `hospitalized`, `hospitalizedCurr`, `recovered`, `death`, `total_tested`, `daily_tested`, and `daily_positive`, contained a significant number of missing values. For instance, `hospitalized` had **19,231 missing values**, indicating substantial data gaps.\n",
        "*   The `Date` column was successfully converted to datetime objects, a crucial step for time-series analysis.\n",
        "\n",
        "**2. Outlier Detection and Scaling (RobustScaler)**\n",
        "\n",
        "To address the presence of outliers and prepare the data for modeling, `sklearn.preprocessing.RobustScaler` was applied to the following numerical columns: `positive`, `active`, `hospitalized`, `hospitalizedCurr`, `recovered`, `death`, `total_tested`, `daily_tested`, and `daily_positive`. `RobustScaler` is particularly effective for outlier handling as it scales features using statistics that are robust to outliers (median and interquartile range), ensuring that extreme values do not disproportionately influence the scaling process.\n",
        "\n",
        "**3. Handling Missing Values (Forward-fill and Median Imputation)**\n",
        "\n",
        "Missing values were handled through a two-step imputation process:\n",
        "*   **Forward-fill (ffill())**: The DataFrame (`df_scaled_robust`) was first sorted by `Date` and `Country_Region` to ensure chronological order within each geographical entity. Then, `ffill()` was applied *grouped by Country_Region* for each numerical column. This method propagates the last valid observation forward, which is suitable for time-series data where values tend to be consistent over short periods within the same region.\n",
        "*   **Median Imputation**: For any remaining missing values after the forward-fill step, median imputation was applied to the same set of numerical columns. Median imputation is a robust strategy for filling NaNs, as the median is less sensitive to outliers compared to the mean, further solidifying the robustness established by `RobustScaler`.\n",
        "\n",
        "**4. Data Normalization (MinMaxScaler) and Indexing**\n",
        "\n",
        "Finally, the numerical features in the `df_scaled_robust` DataFrame were normalized using `sklearn.preprocessing.MinMaxScaler`. This scaled all numerical features to a common range of `[0, 1]`, which is beneficial for many machine learning algorithms that are sensitive to the scale of input features. Additionally, the `Date` column was set as the DataFrame's index, officially preparing the dataset for time-series analysis and modeling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70210b70"
      },
      "source": [
        "## Summary of Exploratory Data Analysis (EDA)\n",
        "\n",
        "### Key Findings\n",
        "\n",
        "1.  **Initial Correlation Heatmap**: The correlation heatmap revealed strong positive correlations among several COVID-19 metrics. Notably, 'Positive Cases' showed a correlation of 0.99 with 'Total Tested', 0.96 with 'Deaths', and 0.94 with 'Daily Tested'. Similarly, 'Deaths' had a 0.94 correlation with 'Hospitalized' and 0.95 with 'Daily Tested'. These strong interdependencies highlight the direct relationships between testing efforts, case identification, hospitalizations, and mortality rates.\n",
        "\n",
        "2.  **Time-Series Plots of Daily Positive Cases**: The time-series line plots, both for all regions and selected major countries, showed clear trends and fluctuations in daily positive cases over time. For the selected countries (United States, India, Brazil, United Kingdom), distinct peaks and growth patterns were observed, indicating varied progression and impact of the pandemic across different geographical entities.\n",
        "\n",
        "3.  **Bar Charts of Total Cases/Deaths by Country**: The bar charts for total positive cases and total deaths by country revealed a highly uneven distribution of the pandemic's impact. The **United States** consistently stood out with significantly higher numbers for both total positive cases and total deaths compared to other nations, indicating the most severe impact in this region. Other countries like Italy, the United Kingdom, Brazil, and Russia also showed substantial figures, but none matched the scale observed in the U.S.\n",
        "\n",
        "4.  **Distribution Plot of Daily Positive Cases**: The distribution plot of 'daily_positive' cases, accompanied by its Kernel Density Estimate (KDE), clearly demonstrated a **strong positive (right) skew**. The calculated skewness value was **11.72**, which is significantly greater than 0.5. This indicates that a large number of days had relatively low daily positive case counts, while a smaller number of days experienced very high daily positive case counts, pulling the tail of the distribution to the right. This suggests a pattern of frequent low-incidence days punctuated by occasional large outbreaks or reporting surges."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2744690a"
      },
      "source": [
        "## Summary of Feature Selection Methods\n",
        "\n",
        "Describing each feature selection method applied (VIF, CMIM, MDG, ANOVA, SBE, Lasso, Boruta-VI), their individual outcomes (e.g., features selected, VIF values, Gini importances, p-values), and the results of the Jaccard Index comparison between them, including key percentages of similarity.\n",
        "\n",
        "### 1. Summary of Feature Selection Method Applications and Outcomes\n",
        "\n",
        "#### **VIF (Variance Inflation Factor)**\n",
        "**Application**: Used to detect multicollinearity among independent features. Features with VIF values greater than 5-10 typically indicate high correlation. Initially, `daily_positive` (target) and `total_tested` showed high VIFs. `total_tested` was explicitly dropped due to its high VIF and strong correlation with the target.\n",
        "**Outcomes**:\n",
        "*   **Initial VIFs (before dropping 'total_tested')**:\n",
        "    *   `daily_tested`: 70.09\n",
        "    *   `daily_positive`: 78.16\n",
        "*   **VIFs (after defining X as independent features and before dropping 'total_tested' from X)**:\n",
        "    *   `positive`: 15.09\n",
        "    *   `death`: 12.75\n",
        "    *   `total_tested`: 3.28\n",
        "    *   `daily_tested`: 1.38\n",
        "    *   Other features (`active`, `hospitalized`, `hospitalizedCurr`, `recovered`) had VIFs between 1.23 and 2.05.\n",
        "*   **VIFs (after dropping 'total_tested')**:\n",
        "    *   `positive`: 12.41\n",
        "    *   `death`: 12.72\n",
        "    *   `daily_tested`: 1.34\n",
        "    *   Other features (`active`, `hospitalized`, `hospitalizedCurr`, `recovered`) had VIFs between 1.23 and 2.05.\n",
        "    *   `total_tested` was removed to address multicollinearity, as instructed.\n",
        "    *   `features_vif_filtered`: {'hospitalizedCurr', 'death', 'active', 'hospitalized', 'positive', 'daily_tested', 'recovered'}\n",
        "\n",
        "#### **CMIM (Conditional Mutual Information Maximization)**\n",
        "**Application**: A filter method that selects features that have high mutual information with the target variable while minimizing redundancy among selected features.\n",
        "**Outcomes**:\n",
        "*   **Ranked Features (descending order of importance)**:\n",
        "    1.  `daily_tested`\n",
        "    2.  `positive`\n",
        "    3.  `hospitalized`\n",
        "    4.  `death`\n",
        "    5.  `hospitalizedCurr`\n",
        "    6.  `active`\n",
        "    7.  `recovered`\n",
        "*   `features_cmim`: {'hospitalizedCurr', 'death', 'active', 'hospitalized', 'positive', 'daily_tested', 'recovered'}\n",
        "\n",
        "#### **MDG (Mean Decrease Gini) / Random Forest Feature Importance**\n",
        "**Application**: An embedded method derived from Random Forest, where features contributing more to the decrease in impurity across trees are considered more important.\n",
        "**Outcomes**:\n",
        "*   **Optimal Hyperparameters**: Optimal `max_features` (`mtry`) was found to be **2**, yielding the lowest Out-of-Bag (OOB) error rate of **0.0216**.\n",
        "*   **Feature Importances**:\n",
        "    *   `positive`: 0.2885\n",
        "    *   `death`: 0.2555\n",
        "    *   `daily_tested`: 0.1649\n",
        "    *   `total_tested`: 0.1551\n",
        "    *   `hospitalized`: 0.0465\n",
        "    *   `active`: 0.0323\n",
        "    *   `recovered`: 0.0322\n",
        "    *   `hospitalizedCurr`: 0.0251\n",
        "*   `features_mdg`: {'hospitalizedCurr', 'total_tested', 'death', 'active', 'hospitalized', 'positive', 'daily_tested', 'recovered'}\n",
        "\n",
        "#### **ANOVA (Analysis of Variance)**\n",
        "**Application**: A filter method (`f_regression`) used to assess the linear relationship between each independent numerical feature and the continuous target variable, identifying features with statistically significant relationships (low p-values).\n",
        "**Outcomes**:\n",
        "*   All features (`positive`, `active`, `hospitalized`, `hospitalizedCurr`, `recovered`, `death`, `total_tested`, `daily_tested`) had p-values less than 0.05 (most effectively 0.0), indicating a statistically significant relationship with `daily_positive`.\n",
        "*   `features_anova`: {'hospitalizedCurr', 'total_tested', 'death', 'active', 'hospitalized', 'positive', 'daily_tested', 'recovered'}\n",
        "\n",
        "#### **SBE (Sequential Backward Elimination)**\n",
        "**Application**: A wrapper method that starts with all features and iteratively removes the least statistically significant feature (based on p-value from an OLS model) until all remaining features are significant.\n",
        "**Outcomes**:\n",
        "*   Starting with all ANOVA-selected features, SBE concluded that **no features were removed** as all of them (i.e., `positive`, `daily_tested`, `total_tested`, `death`, `hospitalizedCurr`, `active`, `recovered`, `hospitalized`) maintained p-values <= 0.05.\n",
        "*   `features_sbe`: {'hospitalizedCurr', 'total_tested', 'death', 'active', 'hospitalized', 'positive', 'daily_tested', 'recovered'}\n",
        "\n",
        "#### **Lasso Regularization**\n",
        "**Application**: An embedded method that adds an L1 penalty to the regression model, shrinking some coefficients to zero and effectively performing feature selection. The `alpha` parameter controls the strength of the penalty.\n",
        "**Outcomes**:\n",
        "*   With `alpha=0.01`, Lasso regularization selected only **one feature**:\n",
        "    *   `death`\n",
        "*   `features_lasso`: {'death'}\n",
        "\n",
        "#### **Boruta-VI (Boruta-Variable Importance)**\n",
        "**Application**: A wrapper method that uses a Random Forest classifier to identify all relevant features present in a dataset, considering shadow features to eliminate statistically insignificant ones.\n",
        "**Outcomes**:\n",
        "*   Boruta-VI identified all the original independent features (`positive`, `active`, `hospitalized`, `hospitalizedCurr`, `recovered`, `death`, `total_tested`, `daily_tested`) as **'strong' features**, meaning they are significantly more important than random predictors.\n",
        "*   `features_boruta`: {'hospitalizedCurr', 'total_tested', 'death', 'active', 'hospitalized', 'positive', 'daily_tested', 'recovered'}\n",
        "\n",
        "### 2. Jaccard Index Comparison of Feature Selection Methods\n",
        "\n",
        "The Jaccard Index quantifies the similarity between the feature sets selected by different methods, ranging from 0 (no common features) to 1 (identical sets).\n",
        "\n",
        "**Key Similarities and Distinctions**:\n",
        "*   **VIF and CMIM**: These methods showed a **100% similarity (Jaccard Index = 1.00)**. Both selected the same 7 features: {'hospitalizedCurr', 'death', 'active', 'hospitalized', 'positive', 'daily_tested', 'recovered'}.\n",
        "*   **MDG, ANOVA, SBE, and Boruta**: These four methods also demonstrated **100% similarity (Jaccard Index = 1.00)** among themselves. They all selected the same 8 features: {'hospitalizedCurr', 'total_tested', 'death', 'active', 'hospitalized', 'positive', 'daily_tested', 'recovered'}.\n",
        "*   **Similarity between the two main groups**: The group (VIF, CMIM) had an **88% similarity (Jaccard Index = 0.88)** with the group (MDG, ANOVA, SBE, Boruta). This high overlap indicates a strong consensus on most relevant features, with the difference likely stemming from `total_tested` which was included by the latter group but removed from the former (VIF) and thus not considered for CMIM on the reduced set.\n",
        "*   **Lasso vs. All Other Methods**: Lasso regularization exhibited very low similarity (ranging from **12% to 14%**) with all other methods. This is due to its aggressive feature selection, which, with `alpha=0.01`, reduced the feature set to only 'death'. This highlights Lasso's ability to produce highly sparse models.\n",
        "\n",
        "**Overall**: The comparison reveals two clusters of methods (VIF/CMIM and MDG/ANOVA/SBE/Boruta) that largely agree on a substantial set of influential features, while Lasso stands out as a highly selective method, prioritizing extreme parsimony."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e3565fb"
      },
      "source": [
        "## Summary of Model Development and Evaluation\n",
        "\n",
        "### Overview\n",
        "This summary consolidates the performance metrics of nine regression models applied to forecast `daily_positive` COVID-19 cases. The evaluation considers results from an initial train-test split (70%/30%) and a more robust 3-times repeated 3-fold cross-validation. The goal is to identify the most effective models and understand their stability.\n",
        "\n",
        "### Model Configurations\n",
        "The following models were implemented with their specified configurations:\n",
        "*   **Linear Regression (GLM)**: `LinearRegression()` (default settings)\n",
        "*   **Lasso Regression**: `Lasso(alpha=0.0014, random_state=42)`\n",
        "*   **K-Nearest Neighbors Regressor**: `KNeighborsRegressor(n_neighbors=1)`\n",
        "*   **Support Vector Regressor (SVR)**: `SVR(gamma=0.15, C=10)`\n",
        "*   **Decision Tree Regressor (CART)**: `DecisionTreeRegressor(random_state=42)`\n",
        "*   **RandomForestRegressor**: `RandomForestRegressor(n_estimators=700, max_features=3, random_state=42)`\n",
        "*   **Bagging Regressor**: `BaggingRegressor(estimator=DecisionTreeRegressor(random_state=42), random_state=42)`\n",
        "*   **Gradient Boosting Regressor**: `GradientBoostingRegressor(n_estimators=250, max_depth=5, learning_rate=0.1, min_samples_leaf=10, random_state=42)`\n",
        "*   **XGBoost Regressor**: `xgb.XGBRegressor(n_estimators=250, max_depth=5, learning_rate=0.4, gamma=0, colsample_bytree=0.8, random_state=42)`\n",
        "\n",
        "### Performance Metrics\n",
        "Performance was assessed using Mean Absolute Error (MAE), Mean Squared Error (MSE), and R-squared. Lower MAE and MSE, and higher R-squared values indicate better model performance.\n",
        "\n",
        "#### Initial Train-Test Split Results\n",
        "| Model                       | MAE       | MSE       | R-squared |\n",
        "| :-------------------------- | :-------- | :-------- | :-------- |\n",
        "| Linear Regression           | 0.0083    | 0.0010    | 0.4205    |\n",
        "| Lasso Regression            | 0.0074    | 0.0006    | 0.6421    |\n",
        "| K-Nearest Neighbors         | **0.0045**| **0.0004**| **0.7737**|\n",
        "| Support Vector Regressor    | 0.0699    | 0.0056    | -2.3748   |\n",
        "| Decision Tree Regressor     | 0.0087    | 0.0011    | 0.3232    |\n",
        "| Random Forest Regressor     | 0.0066    | 0.0006    | 0.6490    |\n",
        "| Bagging Regressor           | 0.0091    | 0.0015    | 0.1121    |\n",
        "| Gradient Boosting Regressor | 0.0095    | 0.0017    | -0.0342   |\n",
        "| XGBoost Regressor           | 0.0049    | 0.0004    | 0.7731    |\n",
        "\n",
        "**Initial Split Observations:**\n",
        "*   **Best Performers**: K-Nearest Neighbors Regressor showed the highest R-squared (0.7737) and lowest MAE/MSE. XGBoost Regressor was a close second (R-squared: 0.7731).\n",
        "*   **Worst Performer**: Support Vector Regressor (SVR) demonstrated extremely poor performance with a negative R-squared (-2.3748), indicating predictions worse than a simple mean.\n",
        "\n",
        "#### Cross-Validation Results (Mean \\u00b1 Std)\n",
        "| Model                       | MAE (Mean \\u00b1 Std) | MSE (Mean \\u00b1 Std) | R-squared (Mean \\u00b1 Std) |\n",
        "| :-------------------------- | :-------------------- | :-------------------- | :-------------------------- |\n",
        "| Linear Regression           | 0.0039 \\u00b1 0.0001  | 0.0002 \\u00b1 0.0000  | 0.8129 \\u00b1 0.0297        |\n",
        "| Lasso Regression            | 0.0041 \\u00b1 0.0001  | 0.0002 \\u00b1 0.0000  | 0.7973 \\u00b1 0.0223        |\n",
        "| K-Nearest Neighbors         | 0.0014 \\u00b1 0.0001  | 0.0001 \\u00b1 0.0000  | 0.9461 \\u00b1 0.0220        |\n",
        "| Support Vector Regressor    | 0.0463 \\u00b1 0.0102  | 0.0025 \\u00b1 0.0010  | -1.6368 \\u00b1 1.0751      |\n",
        "| Decision Tree Regressor     | 0.0014 \\u00b1 0.0001  | 0.0000 \\u00b1 0.0000  | 0.9515 \\u00b1 0.0154        |\n",
        "| Random Forest Regressor     | **0.0011 \\u00b1 0.0000**| **0.0000 \\u00b1 0.0000**| **0.9738 \\u00b1 0.0058**    |\n",
        "| Bagging Regressor           | 0.0012 \\u00b1 0.0000  | 0.0000 \\u00b1 0.0000  | 0.9692 \\u00b1 0.0045        |\n",
        "| Gradient Boosting Regressor | 0.0017 \\u00b1 0.0000  | 0.0000 \\u00b1 0.0000  | 0.9613 \\u00b1 0.0040        |\n",
        "| XGBoost Regressor           | 0.0013 \\u00b1 0.0000  | 0.0000 \\u00b1 0.0000  | 0.9668 \\u00b1 0.0066        |\n",
        "\n",
        "**Cross-Validation Observations:**\n",
        "*   **Best Performers**: The **Random Forest Regressor** consistently emerged as the top performer with the highest mean R-squared (0.9738) and lowest mean MAE/MSE, coupled with a very low standard deviation, indicating high stability. Bagging Regressor (0.9692), XGBoost Regressor (0.9668), Decision Tree Regressor (0.9515), and K-Nearest Neighbors (0.9461) also demonstrated excellent performance and stability.\n",
        "*   **Worst Performer**: SVR continued to show very poor and highly unstable performance, with a negative mean R-squared (-1.6368) and a large standard deviation (1.0751).\n",
        "\n",
        "### Overall Conclusion\n",
        "The ensemble tree-based models, particularly **Random Forest Regressor** and **XGBoost Regressor**, consistently delivered superior and stable performance across both evaluation methodologies. K-Nearest Neighbors also performed exceptionally well in cross-validation, indicating its strong predictive capability. The Support Vector Regressor (SVR), conversely, was an outlier with consistently poor results, suggesting it is not suitable for this specific regression task under the given parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1df689a"
      },
      "source": [
        "## Story by Numbers and Percentages\n",
        "\n",
        "This section consolidates key quantitative insights derived throughout the analysis, highlighting critical numbers and their significance in the context of the COVID-19 dataset.\n",
        "\n",
        "### 1. Missing Data Percentage\n",
        "*   The `hospitalized` column had a significant proportion of missing values, specifically **69.58%** (19231 out of 27641 entries). This necessitated robust imputation strategies to ensure data completeness.\n",
        "\n",
        "### 2. Correlation Strengths\n",
        "*   **`daily_tested` with `daily_positive`**: A very strong positive Pearson correlation of **0.93** was observed. This indicates that as daily testing increases, the number of daily positive cases also tends to increase significantly.\n",
        "*   **`positive` with `death`**: A strong positive Pearson correlation of **0.96** was found. This highlights a direct and substantial relationship between the total number of positive cases and the total number of deaths.\n",
        "\n",
        "### 3. Skewness of `daily_positive` Cases\n",
        "*   The `daily_positive` cases exhibited a skewness value of **11.72**. This strong positive (right) skew indicates that most days had relatively low numbers of positive cases, with a few days experiencing extremely high peaks. This distribution suggests the presence of outliers and an uneven spread of the pandemic over time.\n",
        "\n",
        "### 4. Mean Decrease Gini (MDG) Importance\n",
        "From the Random Forest Regressor, the top two features by MDG importance were:\n",
        "*   **`positive`**: Gini Importance of **0.288**\n",
        "*   **`death`**: Gini Importance of **0.255**\n",
        "These values signify that total positive cases and deaths were the most influential factors in reducing impurity across the trees, making them the strongest predictors of daily positive cases.\n",
        "\n",
        "### 5. Jaccard Similarity Percentages\n",
        "*   **VIF and CMIM group**: These two methods showed a perfect Jaccard similarity of **1.00** (100%), indicating they selected identical feature sets (7 features each).\n",
        "*   **MDG, ANOVA, SBE, and Boruta group**: These four methods also displayed a perfect Jaccard similarity of **1.00** (100%) among themselves, selecting identical feature sets (8 features each).\n",
        "*   **Inter-group similarity (VIF/CMIM vs. MDG/ANOVA/SBE/Boruta)**: The Jaccard similarity between these two main clusters was **0.88** (88%), signifying a high degree of overlap in the features deemed important.\n",
        "*   **Lasso vs. Others**: Lasso regularization showed very low Jaccard similarities, ranging from **0.12** to **0.14** (12% to 14%) with all other methods, as it aggressively selected only the `death` feature.\n",
        "\n",
        "### 6. Model Performance (Cross-Validation)\n",
        "\n",
        "| Model                   | Mean MAE | Mean MSE  | Mean R-squared | Std R-squared |\n",
        "| :---------------------- | :------- | :-------- | :------------- | :------------ |\n",
        "| **Random Forest**       | **0.0011** | **0.00003** | **0.9738**     | **0.0058**    |\n",
        "| **Support Vector Regressor** | 0.0463   | 0.0025    | -1.6368        | 1.0751        |\n",
        "\n",
        "*   The **Random Forest Regressor** emerged as the best-performing model with a mean R-squared of **0.9738** and a low standard deviation of **0.0058**, indicating high accuracy and stability. Its mean MAE was **0.0011** and mean MSE was **0.00003**.\n",
        "*   Conversely, the **Support Vector Regressor (SVR)** was the worst-performing model, demonstrating a mean R-squared of **-1.6368**, alongside a high mean MAE of **0.0463** and mean MSE of **0.0025**. The negative R-squared implies that the model performed worse than simply predicting the mean of the target variable.\n",
        "\n",
        "### Significance in COVID-19 Context\n",
        "These numbers collectively paint a clear picture of the pandemic's dynamics and the effectiveness of our analytical approaches:\n",
        "*   **Missing Data**: The substantial missing data, especially in hospitalization figures, underscores the challenges of real-world data collection during a crisis and the necessity of robust imputation methods to proceed with analysis.\n",
        "*   **Correlations**: The strong correlations between testing, positive cases, and deaths reflect the direct epidemiological links. High correlation of `daily_tested` with `daily_positive` emphasizes the importance of testing capacity in understanding the true spread, while `positive` and `death` correlation highlights the severity.\n",
        "*   **Skewness**: The right-skewed distribution of `daily_positive` cases illustrates the episodic nature of outbreaks, with periods of low infection punctuated by sharp surges, a hallmark of pandemic spread.\n",
        "*   **Feature Importance**: The prominence of `positive` and `death` in MDG importance aligns with epidemiological theory—these are the most direct indicators of a disease's impact and spread. `daily_tested` and `total_tested` also being highly important shows the practical impact of surveillance efforts.\n",
        "*   **Feature Selection Consensus**: The high Jaccard similarity among most feature selection methods (excluding Lasso's aggressive selection) suggests a robust set of features consistently identified as critical for understanding COVID-19 dynamics, irrespective of the selection technique's underlying philosophy. This consensus strengthens the confidence in the selected feature set.\n",
        "*   **Model Performance**: The Random Forest Regressor's high R-squared (0.9738) and low errors demonstrate that a well-tuned ensemble model can effectively capture the complex, non-linear relationships in COVID-19 data, providing highly accurate forecasts for daily positive cases. The poor performance of SVR, however, indicates that not all advanced models are universally suitable and careful selection and tuning are crucial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b8e9ede"
      },
      "source": [
        "## Deployment Considerations for Random Forest Model\n",
        "\n",
        "Deploying the best-performing Random Forest Regressor for forecasting daily positive COVID-19 cases requires careful consideration of several aspects to ensure its efficient, reliable, and maintainable operation in a production environment.\n",
        "\n",
        "### 1. Model Export\n",
        "To deploy the trained Random Forest Regressor, the model must be serialized. This involves saving the trained `rf_model` object so it can be loaded later without needing to retrain. Popular Python libraries for this purpose include `pickle` and `joblib`. `joblib` is often preferred for scikit-learn models as it is more efficient with large NumPy arrays, which are common in machine learning models. The saved model file (e.g., `random_forest_model.joblib`) would contain the entire state of the trained regressor, including all its individual decision trees and their configurations.\n",
        "\n",
        "### 2. API Integration\n",
        "For integrating the exported model into a web application or microservice, frameworks like Flask or FastAPI are excellent choices. FastAPI, in particular, offers automatic interactive API documentation (Swagger UI) and high performance. The deployment workflow would typically involve:\n",
        "*   **Loading the Model**: The serialized model would be loaded into memory when the API service starts.\n",
        "*   **Endpoint Creation**: A dedicated API endpoint (e.g., `/predict_covid_cases`) would be created to handle prediction requests.\n",
        "*   **Preprocessing**: Incoming raw data from API requests would need to be preprocessed using the *fitted* `RobustScaler` and `MinMaxScaler` objects (which also need to be serialized and loaded alongside the model). This ensures that the input data to the model is in the same format and scale as the data it was trained on.\n",
        "*   **Prediction**: The preprocessed data would then be fed to the loaded Random Forest model to generate forecasts.\n",
        "\n",
        "### 3. Data Input/Output Formats\n",
        "*   **Input Format**: For ease of integration and interoperability, incoming data for predictions should adhere to a standardized format, typically JSON. The JSON payload would contain the features (`positive`, `active`, `hospitalized`, `hospitalizedCurr`, `recovered`, `death`, `total_tested`, `daily_tested`) as key-value pairs, matching the structure expected by the preprocessing pipeline. For example: `{\"positive\": 1000, \"active\": 500, ...}`.\n",
        "*   **Output Format**: The prediction output should also be a standardized JSON object. This could include the predicted `daily_positive` cases, along with potentially useful information such as prediction intervals or confidence scores. For example: `{\"predicted_daily_positive\": 0.15, \"prediction_interval_lower\": 0.12, \"prediction_interval_upper\": 0.18}`.\n",
        "\n",
        "### 4. Real-time vs. Batch Predictions\n",
        "*   **Real-time Predictions**: For applications requiring immediate responses (e.g., a dashboard showing live forecasts), the API needs to be optimized for low latency. This involves efficient model loading, fast preprocessing, and potentially using asynchronous request handling. The infrastructure might utilize load balancers and auto-scaling groups to handle varying request volumes.\n",
        "*   **Batch Predictions**: For scenarios where predictions are needed less frequently or for large datasets (e.g., daily reports), batch prediction jobs are more suitable. These can be scheduled using tools like Apache Airflow or simple cron jobs. Batch jobs would read large volumes of data, preprocess it, generate predictions, and store the results in a database or data warehouse for later retrieval or analysis. The `n_jobs=-1` parameter used during training of the Random Forest model is also beneficial for leveraging all CPU cores during prediction in a batch context.\n",
        "\n",
        "### 5. Monitoring for Drift\n",
        "Continuous monitoring is crucial for maintaining model performance in production:\n",
        "*   **Model Performance Monitoring**: Track key metrics (MAE, MSE, R-squared) on new, labeled data over time. A decline in performance indicates that the model might be stale.\n",
        "*   **Data Drift Detection**: Monitor the distributions of incoming input features and the target variable to detect changes from the training data. For example, a sudden shift in `daily_tested` patterns could significantly impact predictions.\n",
        "*   **Concept Drift Detection**: Observe changes in the relationship between input features and the target variable. For instance, if the impact of `active` cases on `daily_positive` changes over time due to new variants or interventions.\n",
        "*   **Implications**: Detecting drift implies that the model needs to be retrained or updated. An automated MLOps pipeline can trigger retraining, re-evaluation, and redeployment of the updated model.\n",
        "\n",
        "### 6. Scalability\n",
        "The deployment infrastructure for the prediction service should be designed for scalability:\n",
        "*   **Horizontal Scaling**: Utilize containerization (e.g., Docker) and orchestration platforms (e.g., Kubernetes) to easily spin up multiple instances of the prediction service based on demand.\n",
        "*   **Cloud Services**: Leverage cloud platforms (AWS, Azure, GCP) for managed services that handle scaling, load balancing, and infrastructure management, reducing operational overhead.\n",
        "*   **Resource Management**: Optimize model size and prediction logic to efficiently use computational resources (CPU, memory). Parallel processing with `n_jobs=-1` is beneficial for Random Forest on multi-core machines."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf023246"
      },
      "source": [
        "## Abstract\n",
        "\n",
        "This study addresses the critical challenge of forecasting daily positive COVID-19 cases using a global dataset of **27,641 entries**. Initial data preprocessing involved handling significant missing values, with up to **69.58%** observed in the 'hospitalized' column, addressed via RobustScaler for outlier treatment, followed by country-grouped forward-fill and median imputation. Exploratory Data Analysis revealed a highly positively skewed distribution for 'daily_positive' cases (skewness: **11.72**) and strong correlations, notably between 'daily_tested' and 'daily_positive' (Pearson r: **0.93**), and 'positive' cases and 'death' (Pearson r: **0.96**). Diverse feature selection techniques were employed: VIF, CMIM, MDG (Mean Decrease Gini), ANOVA, SBE (Sequential Backward Elimination), Lasso regularization, and Boruta-VI. A robust consensus emerged among most methods, with MDG, ANOVA, SBE, and Boruta-VI demonstrating **100%** Jaccard similarity and selecting 8 features. VIF and CMIM showed **100%** similarity, identifying 7 features, and a high **88%** similarity with the aforementioned group. Conversely, Lasso regularization (alpha=0.01) exhibited extreme parsimony, selecting only 'death' with low **12-14%** similarity to other methods. Nine regression models were evaluated using 3-times repeated 3-fold cross-validation. The **Random Forest Regressor** consistently outperformed all other models, achieving a superior mean R-squared of **0.9738** (MAE: **0.0011**, MSE: **0.000026**) with high stability (R-squared standard deviation: **0.0058**). In stark contrast, the Support Vector Regressor (SVR) demonstrated exceptionally poor performance, yielding a negative mean R-squared of **-1.6368**. These findings underscore the effectiveness of ensemble tree-based models and the identified robust feature set in accurately predicting COVID-19 case trends, highlighting the importance of comprehensive feature analysis and model validation in public health forecasting."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## More on Model Evaluation\n",
        "\n",
        "\n",
        "\n",
        "1.   ROC Curves: ROC curves can be generated to summarize the trade-of between the true positive and false positive rates. However, ROC curves are primarily used for classification models, where the goal is to assess how well a model distinguishes between different categories (e.g., predicting 'yes' or 'no', 'sick' or 'healthy').\n",
        "\n",
        "\n",
        "> Your task of forecasting daily_positive COVID-19 cases is a regression problem, as you are predicting a continuous numerical value, not a class. For regression models, metrics like MAE, MSE, and R-squared (which we've already used) are the standard ways to evaluate predictive power.\n",
        "\n",
        "> Generating an ROC curve for a regression output directly isn't meaningful because there's no inherent 'true positive rate' or 'false positive rate' in the same way there is for binary classification. We've already explored several relevant metrics for regression, including the predicted vs. actual plot for the best model.\n",
        "\n",
        "2.   calibration plots are specifically designed for classification models, not regression models like the one we're using to predict daily_positive cases (a continuous numerical variable).\n",
        "For regression tasks, the Residual Plot and Prediction Interval Plot are the appropriate tools to assess model performance and uncertainty. We have already generated these for the Random Forest Regressor, which was identified as our best predictive model:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Rv4gVZ6ig8TO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## R\n",
        "The caret package (an R package) can be was employed for model training, comparison, and subsequent analyses.\n"
      ],
      "metadata": {
        "id": "YfLPLAuNiCHh"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1GyHwjYjQTlHHKQt_XHpz4tPML-FS8nfe",
      "authorship_tag": "ABX9TyMtbJt3DZgSIoxXs19mBJ7R",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}