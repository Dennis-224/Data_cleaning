{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dennis-224/Data_cleaning/blob/main/Project1_Working_With_House_Price.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**DSN CAPSTONE 1**\n",
        "#**SCIENTIST: DENNIS OKEREKE**\n",
        "\n",
        "FIRST we were given a house price dataset"
      ],
      "metadata": {
        "id": "9K0RDeB11wMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "gQBNJQKM18uN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ACTION 1: We now load our csv dataset file\n"
      ],
      "metadata": {
        "id": "j_L2341vpKVV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/House_Price - House_Price.csv')"
      ],
      "metadata": {
        "id": "XthR_ABaCSnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ACTION 2: Exploratory Data Analysis (EDA)\n"
      ],
      "metadata": {
        "id": "JutX-BsyvjLh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.describe(include= 'all'))\n",
        "df.info()\n",
        "df.head()"
      ],
      "metadata": {
        "id": "KpVEybfvvn5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Time to detect outliers\n"
      ],
      "metadata": {
        "id": "AmXK4I8vxQDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
        "sns.jointplot(x='n_hot_rooms', y='price', data=df) #sns.jointplot is for detecting relationship btw two variables\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "G2ABNVanxYDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## In like manner for multiple columns...\n"
      ],
      "metadata": {
        "id": "hgMmMiP10Blx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for col in ['rainfall']: # replace or extend with other numeric cols\n",
        "    sns.jointplot(x=col, y='price', data=df)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "CXEc1FIB0JGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Categorical inspection and visualization\n"
      ],
      "metadata": {
        "id": "rICfTv5c1HGb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Unique values in airport column:\", df['airport'].unique())\n",
        "sns.countplot(x='airport', data=df) #sns.countplot for frequency of a single categorical variable\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rbM1ZOzY1Kg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## In like manner... general approach for categorical columns\n",
        "\n"
      ],
      "metadata": {
        "id": "yHoauQ_P1sYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cat_cols = df.select_dtypes(include='object').columns.tolist() #object dtype is for string\n",
        "for col in cat_cols:\n",
        "    print(f\"Column: {col} -- unique values: {df[col].unique()}\") #f string is a modern way to embed python expressions in a string\n",
        "    sns.countplot(x=col, data=df)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "NKy11xMM14rT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Time to takle missing values\n",
        "### And after cleaning we print the data again to confirm\n"
      ],
      "metadata": {
        "id": "8Pr91Jn22iHP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['waterbody'] = df['waterbody'].fillna(df['waterbody'].mode()[0])\n",
        "df['n_hos_beds'] = df['n_hos_beds'].fillna(df['n_hos_beds'].mode()[0])\n",
        "print(df.describe(include= 'all'))\n",
        "df.info()\n",
        "df.head()"
      ],
      "metadata": {
        "id": "RiYqGFr82m1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We continue with dealing with outliers\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TRI1kG-oYW24"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Capping or Winsorization\n",
        "Based on the visualizations, it appears that the outliers are on the higher end of the n_hot_rooms values. In this case, capping would be the appropriate method to handle these outliers. Capping involves setting a maximum value for the outliers, effectively replacing them with this upper limit. In layman terms when data is at higher side, you bring down to lower range\n"
      ],
      "metadata": {
        "id": "oGfyBX7ceqxw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Note: My Finding\n",
        " The quickest and best way to know which columns to cap involves a two-pronged approach: visual inspection using tools like box plots and the application of statistical rules like the IQR or Z-score methods, tailored to the data's distribution."
      ],
      "metadata": {
        "id": "NWnDliW2U4JR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.percentile(df.n_hot_rooms,[99]) #we do the 99th percentile for capping"
      ],
      "metadata": {
        "id": "VZo8HCDyfACi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Where uv is upper value\n",
        "uv = np.percentile(df.n_hot_rooms,[99])[0] #we want it to start from the very first column value.\n",
        "#99th percentile(common capping practice) means only the top 1% of data points is affected, so much of the original data is preseved."
      ],
      "metadata": {
        "id": "LaAr58ZTeWzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[(df.n_hot_rooms>uv)] #selects and displays the rows in the dataframe where value is greater than the calculated upper value.\n",
        "#Used to inspect rows containing outliers based on the 99th percentile.\n",
        "df.n_hot_rooms[(df.n_hot_rooms> 3*uv)] = 3*uv #implements capping. Identifies values greater than 3*uv and replaces them with 3*uv.\n",
        "# This limits maximum values mitigating impact of outliers."
      ],
      "metadata": {
        "id": "TKNoIcTweMZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## So...\n",
        "we check that its been effected...\n"
      ],
      "metadata": {
        "id": "XJqv81cjh52A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
        "sns.jointplot(x='n_hot_rooms', y='price', data=df)\n",
        "plt.show() #Before the outliers was around 80 but now it comes down to 46"
      ],
      "metadata": {
        "id": "1MzZ0y7SiC8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Flooring\n",
        "\n",
        "Is a technique used in data preprocessing to handle outliers that are significantly lower than the majority of the data. Similar to capping, which sets an upper limit for outliers, flooring sets a lower limit.\n",
        "\n",
        "Essentially, any data point below a certain calculated minimum value (often a percentile like the 1st percentile) is replaced with that minimum value. This helps to reduce the impact of extremely low values on your analysis or model\n",
        "\n",
        "Flooring is like an opposite to capping\n"
      ],
      "metadata": {
        "id": "NIUBkzoCh4aA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.percentile(df.rainfall,[1])[0] #1 percentile is a common practice for flooring. Using the 1st percentile means that only the bottom 1% of the data points are affected,\n",
        "#preserving much of the original data while mitigating the impact of these outliers."
      ],
      "metadata": {
        "id": "Rdko3VrSlLdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.percentile(df.rainfall,[1])[0] # calculates the 1st percentile of the 'rainfall' column in your DataFrame df. The result is an array containing percentile value.\n",
        " #[0] accesses the first (and in this case, only) element of that array, which is the calculated 1st percentile value\n",
        "lv = np.percentile(df.rainfall,[1])[0] #assigns calculated 1st percentile value to variable lv\n",
        "df.rainfall[(df.rainfall< 0.3*lv)] = 0.3*lv # the boolean mask df.rainfall<0.3*lv is True for rows where the 'rainfall' value is less than 0.3 (30%) times the lower limit,\n",
        "#and False otherwise. df.rainfall[...] uses this boolean mask to select only the rows in the 'rainfall' column where the condition is True(i.e the outliers at the lower end)"
      ],
      "metadata": {
        "id": "PyljSSa_jFxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
        "sns.jointplot(x='rainfall', y='price', data=df)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "of4W-n8_la1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Seasonality\n",
        "\n",
        "Seasonality can affect our data\n",
        "How could that happen?"
      ],
      "metadata": {
        "id": "yG9uYY4mmqAW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   **Rainfall**: This column directly relates to weather patterns, which are seasonal. High rainfall seasons could correlate with certain property conditions or impact outdoor amenities like parks.\n",
        "*   **Waterbody** **levels**: If the \"waterbody\" column relates to natural water sources, their levels could fluctuate seasonally due to rainfall and evaporation, potentially influencing the desirability or accessibility of properties near them.\n",
        "\n",
        "\n",
        "*   **Parks**: The usage and appeal of parks could be seasonal, affecting nearby property values or interest.\n",
        "*   **Air** **quality**: Air quality can sometimes have seasonal variations depending on factors like temperature inversions, pollution sources, and wind patterns."
      ],
      "metadata": {
        "id": "uFYg1-f1nY_y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Non Usable Variable\n",
        "\n",
        "* If a table has just one value thats non usable bacause it won't relate with anything. An example is Bus_ter that has only YES\n",
        "* if we get an initail data having low fill rate (i.e 60% data is missing), then we drop the entire data.\n",
        "* Imagine we have a variable telling us make of cars. It can't help us in predicting house price so we delete such variable\n",
        "\n"
      ],
      "metadata": {
        "id": "rWEPBTvuT6Um"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.bus_ter.unique().tolist()\n",
        "sns.countplot(x=df.bus_ter)\n",
        "df = df.drop('bus_ter', axis=1) #Now  after this when we check for bus_ter variable we'll get an error message"
      ],
      "metadata": {
        "id": "5YB8TxYmVqjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.bus_ter"
      ],
      "metadata": {
        "id": "tY3NgcSkX3ZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bivariate Analysis\n",
        "\n",
        "Bivariate analysis is a statistical method used to explore the relationship between two variables. This can involve looking at how changes in one variable are associated with changes in the other, and the strength and direction of that association. Common techniques for bivariate analysis include:\n",
        "\n",
        "* Scatter plots: To visualize the relationship between two numerical variables.\n",
        "* Correlation coefficients: To quantify the strength and direction of the linear relationship between two numerical variables.\n",
        "* Cross-tabulation and chi-square tests: To examine the relationship between two categorical variables.\n",
        "* Box plots or violin plots: To compare the distribution of a numerical variable across different categories of a categorical variable.\n",
        "\n",
        "Bivariate Analysis helps in understanding how variables interact with each other, which is crucial for identifying patterns, making predictions, and building models."
      ],
      "metadata": {
        "id": "SirWhF7mYBT-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Histogram\n",
        "Now when we plot a histogram we get to see how skewed our data is.\n",
        "A positively skewed data has the top of the bell on the left hand side. if the top of the bell is on the right hand side, then its negatively skewed.\n"
      ],
      "metadata": {
        "id": "3r6MTKxFZ53c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Skewed data\n",
        "\n",
        "skewed data refers to a situation where the distribution of values in a dataset is asssymetyric, deviating from the normal or bell-shaped distribution. Skewness can have implications for statistical analysis and modeling, as it can affect the accuracy of assumptions made by certain methods.\n",
        "\n",
        "Type\n",
        "\n",
        "There are two common types of skewness:\n",
        "1. Positive skewness: In this case, the tail of the distribution extends towards the right side, indicating a larger number of smaller values and a few extremely large values.\n",
        "2. Negative skewness: Here the tail of the distribution extends towards the left side, indicating a larger number of larger values and a few extremely small values.\n"
      ],
      "metadata": {
        "id": "Pd4nUw71SpkX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Handling skewed data is important because it can affect the performance of certain models and statistical tests that assume a normal distribution. Here are some common methods to handle skewed data:\n",
        "1. Logarithmic transformation: Applying a logarithmic transformation to the skewed variable can compress the larger values and spread out the smaller values, reducing the skewness and making the distribution more symmetrical.\n",
        "2. Square root transformation: Similar to the logarithmic transformation, the square root transformation can help reduce the skewness of the data.\n",
        "3. Box-Cox transformation: The Box-Cox transformation is a more general transformation that can handle various types of skewness. It applies a power transformation to the data, allowing it to become more normally distributed.\n"
      ],
      "metadata": {
        "id": "f_TWPXIyVy6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.distplot(x= df.price, hist=True, kde=True)"
      ],
      "metadata": {
        "id": "FDmPRQDmbPUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logarithm or Square root\n",
        "\n",
        "Logarithm and square root transformations are often used in data preprocessing, particularly when dealing with skewed data or when preparing data for certain statistical models that assume normality."
      ],
      "metadata": {
        "id": "T1JI2f1yceUt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this context, whereby we have already looked at the\n",
        "distribution of the price variable using a histogram (which showed some skewness), here are some potential places where we might consider using logarithm or square root transformations:\n",
        "\n",
        "1. **Handling Skewed Variables**: If any of the numerical features (like crime_rate, resid_area, air_qual, age, poor_prop, or even price itself if you plan to model it) are highly skewed, applying a logarithm or square root transformation can help make their distributions more symmetrical and closer to a normal distribution. This can be beneficial for some regression models.\n",
        "\n",
        "2. **Stabilizing Variance:** For some variables, the variance might not be constant across the range of values. Transformations can sometimes help stabilize the variance, which is another assumption of certain statistical models.\n",
        "\n",
        "3. **Before Modeling:** When you move towards building a predictive model (like linear regression) to predict house prices, transforming skewed independent variables can improve the performance and interpretability of the model.\n",
        "\n",
        "To determine if a transformation is needed and which one to use, we need to:\n",
        "\n",
        "\n",
        "*   Visualize the distribution of the variable (using histograms or density plots) to check for skewness.\n",
        "*   Consider the relationship between the independent variables and the dependent variable (price). Transformations might help linearize non-linear relationships.\n",
        "\n",
        "To apply these transformations we need functions like np.log() or np.sqrt() from NumPy to the relevant columns in your DataFrame.\n",
        "\n"
      ],
      "metadata": {
        "id": "KG1GkZCDdFma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(x=df.crime_rate, y=np.log(df.price)) #notice how we see abnomalities of some values towards 80"
      ],
      "metadata": {
        "id": "jgM1O0Npe51a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.log(df.crime_rate).describe() #The issue with logarithm is that it can lead to negative values. We don't want that!"
      ],
      "metadata": {
        "id": "nvuzd-pzgTKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.log(1+ df.crime_rate).describe() #we rectify it by adding +1 to our code"
      ],
      "metadata": {
        "id": "-n1_GMu9g8zG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# but square root doesn't need plus 1\n",
        "np.sqrt(df.crime_rate).describe()"
      ],
      "metadata": {
        "id": "apAP6L8ShYvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import boxcox\n",
        "pd.Series(boxcox(df['crime_rate'])[0].tolist()).describe()"
      ],
      "metadata": {
        "id": "PBnvhnxRZHyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Flowing from that...\n"
      ],
      "metadata": {
        "id": "3bLbnx9iiG7_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.crime_rate = np.log(1+df.crime_rate)"
      ],
      "metadata": {
        "id": "gmu5RDvPazJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.distplot(x=df.crime_rate, hist = True, kde = True)"
      ],
      "metadata": {
        "id": "jWpGYaCJbXha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(x=df.crime_rate, y=df.price)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CZqSUrsSckIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CONGRATULATIONS!!!!!! âœ¨\n"
      ],
      "metadata": {
        "id": "8gtPniwBcv_A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have successfully cleaned my data\n"
      ],
      "metadata": {
        "id": "bVsK1VZAc_oN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ACTION 3: Feature Engineering\n",
        "Handling categorical columns is an important task in data analysis and modeling as categorical variables contain non-numerical values that require special treatment. Categorical variables provide valuable information about the different groups or categories in a dataset.\n",
        "\n",
        "Categorical variables can be classified into two types:\n",
        "\n",
        "1.   Nominal Variables: These variables have categories with no inherent order or ranking. Examples include gender (male, female)or city(Abuja, Lagos, Benin).\n",
        "2.   Ordinal Variables: These variables have categories with a specific order or ranking. Examples include educational level (high school, college, graduate school) or satisfaction level (low, medium, high)."
      ],
      "metadata": {
        "id": "HmqWUqYtdJfM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Encoding\n",
        "Categorical variables need to be encoded into numerical values for analysis or modeling. Some common encoding techniques include:\n",
        "\n",
        "1.   **One**-**Hot** **Encoding**: This technique creates binary columns for each category, representing the presence or absence of the category in each observation.\n",
        "\n",
        "\n",
        "*   Dummies\n",
        "\n",
        "\n",
        "> In the context of pd.get_dummies(), \"dummies\" refers to these new binary columns that are created. For example, if you have a categorical column named 'Color' with values 'Red', 'Blue', and 'Green', pd.get_dummies() will create three new columns: 'Color_Red', 'Color_Blue', and 'Color_Green'. For each row, one of these new columns will have a value of 1 (or True in newer pandas versions) and the others will have a value of 0 (or False)\n"
      ],
      "metadata": {
        "id": "5TI9R_qNfh3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_df = pd.get_dummies(df) #Now with this code we no longer get YES and NO.But rather True and False.\n",
        "#automatically identifies the categorical columns in your DataFrame (df) and converts them into a numerical format.\n",
        "#For each unique category in a categorical column, it creates a new binary column.\n",
        "encoded_df.head() # displays the first few rows of the new DataFrame encoded_df after the one-hot encoding has been applied."
      ],
      "metadata": {
        "id": "L571HBjv-CRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_df.shape,df.shape"
      ],
      "metadata": {
        "id": "QYHDZm-o_6sl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JCkzBjghAJZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ja7zE5PyAKqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Ordinal** **Encoding**: This technique assigns a unique integer value to each category, preserving the ordinal relationship among categories. It is suitable when the categories have an inherent order or rank.\n",
        "\n"
      ],
      "metadata": {
        "id": "ubOkXZRoAIEx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Fill missing values in 'waterbody' with 'None'\n",
        "df['waterbody'] = df['waterbody'].fillna('None')\n",
        "\n",
        "# Manually specify the order mapping, including 'None'\n",
        "order_mapping = [['None', 'River', 'Lake', 'Lake and River']] # This is a crucial part for ordinal encoding. It defines the specific order you want to assign to the categories in the 'waterbody' column. In this case, you've decided that 'None' comes first, followed by 'River', 'Lake', and finally 'Lake and River'. This order will determine the numerical values assigned during encoding.\n",
        "\n",
        "# Create and fit the OrdinalEncoder with the custom order mapping\n",
        "encoder = OrdinalEncoder(categories=order_mapping) #fit() learns the categories and their order from the data based on the categories provided during initialization\n",
        "encoded_data = encoder.fit_transform(df[['waterbody']]) # The output of this is a Numpy array\n",
        "df['waterbody_encoded'] = encoded_data #selects waterbody column as a Dataframe because fit_transform expects a 2D"
      ],
      "metadata": {
        "id": "tnnGK_3ZsVTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.waterbody.unique().tolist()"
      ],
      "metadata": {
        "id": "z6SRuYrWt9fn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del df['waterbody']"
      ],
      "metadata": {
        "id": "MAZjrz8nuNmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "eXRkGFT5uTef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HZxdkkt7_Rkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Label** **Encoding**: Similar to ordinal encoding, label encoding assigns a unique integer value to each category, but it does not preserve any inherent order. it is suitable when the categories are unordered.\n"
      ],
      "metadata": {
        "id": "o2RDvAlFvB4Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "encoded_data = encoder.fit_transform(df['airport'])\n",
        "df['airport'] = encoded_data\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "3wJMpw1Yvg6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ACTION 4: Multicollinearity\n"
      ],
      "metadata": {
        "id": "UscHx5PIM_Al"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multicollinearity** occurs when two or more independent variables (features) in a regression model are highly correleted with each other. This means one predictor variable can be linearly predicted from the others with a substantial degree of accuracy. *Multicollinearity doesn't necessarily reduce the predictive accuracy of the model as a whole, but it severely impacts the interpretability of its coefficients.*\n",
        "\n",
        "**Why** **is** **it** **a** **problem**?\n",
        "\n",
        "\n",
        "*   **Unreliable** **Coefficients**: it becomes difficult for the model to determine the individual effect of each correlated feature on the target variable. The coefficient estimates can become very sensitive to small changes in the data, making them unstable and untrustworthy.\n",
        "*   **Difficult** **Interpretation**: You can't interpret a coefficient as \"the effect of one-unit change in this feature, holding all others constant,\" because when one feature changes, its correlated partners change with it.\n",
        "\n",
        "**Solution**\n",
        "\n",
        "* Remove highly correlated independent variables by looking at the correlation matrix and VIF (Variance Inflation Factor). VIF quantifies how much the variance of the estimated regression coefficient is increased due to multicollinearity. A high VIF for a variable indicates it is highly correlated with other predictor variables.\n",
        "* Combine correlated variables into a single variable (e.g., create an average or index).\n",
        "* Use regularization techniques (like Lasso or Ridge regression) that can handle multicollinearity.\n",
        "* Collect more data, if possible, as multicollinearity can be a sample-specific issue.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VBLyg-nsXv_h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Correlation** is a statistical measure that indicates the extent to which variables fluctuate together. A positive correlation indicates the extent to which those variables increase or decrease in parallel; a negative correlation indicates the extent to which one variable increases as the other decreases.\n"
      ],
      "metadata": {
        "id": "8fptl9bNNFK6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Examples\n"
      ],
      "metadata": {
        "id": "e_kAkIBXPBry"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some examples of data that have a high correlation:\n",
        "\n",
        "\n",
        "*   Your caloric intake and your weight\n",
        "*   The amount of time you study and your GPA (Me: I'd say debatable)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "69wN2rHMPFp0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Other examples of data that have a low correlation (or none at all):\n",
        "\n",
        "\n",
        "*   A person name and the chances of them commiting violence\n",
        "*   The colour of a house and how much the building cost\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IiytVCqePmmz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The** **Correlation** **Co**-**efficient**\n"
      ],
      "metadata": {
        "id": "bALNQCjnQIXV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definition\n",
        "\n",
        "\n",
        "*   A correlation coefficient is a way to put to the relationship.\n",
        "*   Correlation coefficient have a value between -1 and 1.\n",
        "\n",
        "\n",
        "*   A \"O\" means there is no relationship between the variables at all,\n",
        "*   While -1 or 1 means that there is a perfect negative or positive correlation\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6XjOh0SXQTj_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Examples\n",
        "\n",
        "\n",
        "* A correlation coefficient of 0.9 would indicate a strong positive correlation (e.g., the amount of time you study and your GPA).\n",
        "* A correlation coefficient of -0.8 would indicate a strong negative correlation (e.g., the outside temperature and heating costs).\n",
        "* A correlation coefficient of 0.1 would indicate a very weak positive correlation (e.g., a person's height and their salary).\n",
        "* A correlation coefficient of -0.05 would indicate a very weak negative correlation (e.g., the color of a house and its selling price).\n"
      ],
      "metadata": {
        "id": "mw8S8plKRMwa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The** **Correlation** **matrix**\n"
      ],
      "metadata": {
        "id": "kOVuOZ9sRPZA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definition\n",
        "\n",
        "\n",
        "*   A correlation matrix is a table showing correlation coefficients between variables.\n",
        "*   Each cell in the table shows a correlation between two variables.\n",
        "\n",
        "\n",
        "*   A correlation matrix is used as a way to summarize data, as input into a more advanced analysis, and as a diagnostic for  advanced analysis\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7GOb2IdcRzjy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Application\n",
        "\n",
        "\n",
        "* A correlation coefficient of 0.9 would indicate a strong positive correlation (e.g., the amount of time you study and your GPA).\n",
        "* A correlation coefficient of -0.8 would indicate a strong negative correlation (e.g., the outside temperature and heating costs).\n",
        "* A correlation coefficient of 0.1 would indicate a very weak positive correlation (e.g., a person's height and their salary).\n",
        "* A correlation coefficient of -0.05 would indicate a very weak negative correlation (e.g., the color of a house and its selling price).\n"
      ],
      "metadata": {
        "id": "RTCsKg2-SrUx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize = (13, 13))\n",
        "# Plot the correlation matrix as a heatmap\n",
        "\n",
        "sns.heatmap(df.corr(), annot=True, cmap='RdYlGn', center=0, square=True)"
      ],
      "metadata": {
        "id": "YL-dbdQ1WaVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##Explaining the Heatmap\n",
        "\n",
        "\n",
        "Here's a breakdown of the lines of code:\n",
        "\n",
        "*import matplotlib.pyplot as plt:* This line imports the matplotlib.pyplot module, which is a plotting library in Python, and gives it the alias plt for easier use.\n",
        "\n",
        "*plt.figure(figsize = (13, 13)):* This line creates a new figure for your plot using matplotlib. The figsize parameter sets the size of the figure in inches to 13 inches by 13 inches. This is done to ensure the heatmap is large enough to be readable, especially with many variables.\n",
        "\n",
        "*sns.heatmap(df.corr(), annot=True, cmap='RdYlGn', center=0, square=True)*: This is the core line that generates the heatmap using the seaborn library.\n",
        "\n",
        "*sns.heatmap(...):* This is the function from seaborn used to create heatmaps.\n",
        "\n",
        "\n",
        "*df.corr():* This calculates the pairwise correlation between all columns in your DataFrame df. The result is a correlation matrix, where each cell contains the correlation coefficient between two columns.\n",
        "\n",
        "*annot=True:* This parameter tells seaborn to annotate each cell of the heatmap with the correlation coefficient value. This makes it easy to see the exact correlation between variables.\n",
        "\n",
        "*cmap='RdYlGn':* This sets the colormap for the heatmap. 'RdYlGn' is a common colormap that uses red for negative correlations, yellow/light green for correlations around zero, and green for positive correlations. The intensity of the color indicates the strength of the correlation.\n",
        "\n",
        "*center=0:* This parameter sets the center of the colormap to 0. This is useful for correlation heatmaps as a correlation of 0 means no linear relationship, and the colors will diverge from the center towards the positive and negative extremes.\n",
        "\n",
        "*square=True:* This parameter ensures that each cell in the heatmap is square-shaped. This makes the heatmap visually more appealing and easier to interpret.\n",
        "\n",
        "In summary, this code calculates the correlation matrix of your DataFrame and then visualizes it as a heatmap with annotations and a specific colormap to easily identify the strength and direction of the linear relationships between your variables."
      ],
      "metadata": {
        "id": "z2gtUMQcVkLA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Still on the Heatmap\n",
        "Here's what it's telling us:\n",
        "\n",
        "Each square in the heatmap shows the correlation coefficient between two variables.\n",
        "\n",
        "The color of the square indicates the strength and direction of the correlation:\n",
        "\n",
        "**Red or darker shades of red** typically represent a strong positive correlation (as one variable increases, the other tends to increase).\n",
        "\n",
        "**Blue or darker shades of blue** typically represent a strong negative correlation (as one variable increases, the other tends to decrease).\n",
        "\n",
        "**Lighter shades or colors closer to the center** (often white or light yellow/green depending on the colormap) indicate a weak or no correlation.\n",
        "\n",
        "**The diagonal line** from the top left to the bottom right is always a solid color (usually the strongest positive correlation color) because a variable is perfectly correlated with itself (correlation coefficient of 1).\n",
        "\n",
        "**Symmetry:** The heatmap is symmetrical along the diagonal because the correlation between variable A and variable B is the same as the correlation between variable B and variable A.\n",
        "\n",
        "By looking at this heatmap, we can quickly identify which pairs of variables have strong positive or negative relationships. This is useful for understanding the data and can help in feature selection for machine learning models. For example, if two independent variables are highly correlated with each other (multicollinearity), we might consider removing one of them to improve model performance. We can also see which variables have the strongest correlation with your target variable which may indicate potential strong predictors."
      ],
      "metadata": {
        "id": "VHViXyn6we5G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['avg_dist'] = (df.dist1+df.dist2+df.dist3+df.dist4)/4\n"
      ],
      "metadata": {
        "id": "57JJH8y-bxSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del df['dist1']\n",
        "del df['dist2']\n",
        "del df['dist3']\n",
        "del df['dist4']\n"
      ],
      "metadata": {
        "id": "5L_2i5XdcNl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "TSO2dnOcclw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize = (13, 13))\n",
        "# Plot the correlation matrix as a heatmap\n",
        "sns.heatmap(df.corr(), annot=True, cmap='RdYlGn', center=0, square=True)"
      ],
      "metadata": {
        "id": "GEs-uTLedSWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del df['parks'] #the 'parks' column showed a very low correlation with the target variable 'price. See its negative value is -0.047, so low."
      ],
      "metadata": {
        "id": "Dplu5shJdpNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "UQh1N07ddtr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"Cleaned House Data.csv\", index = False) #this code is saving your cleaned and processed data from the DataFrame into a new CSV file, making it available for future use. The default index=False prevents DataFrame index as the first column"
      ],
      "metadata": {
        "id": "dRi659N6d0FF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('Cleaned House Data.csv')"
      ],
      "metadata": {
        "id": "pXg4Y7WHPjGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ACTION 4: ML Algorithms for Supervised Learning\n"
      ],
      "metadata": {
        "id": "L3LTI8yAeGyA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Note/Remember that:\n",
        "\n",
        "Classification and regression are **the** **two** **main** **types** of supervised learning problems\n",
        "\n",
        "Common Algorithms for machine learning include\n",
        "\n",
        "\n",
        "*   Classification (*which category does this belong to*?): Logistic regression, Support Vector Machines,Decision Trees, k-NN, Naive Bayes\n",
        "\n",
        "\n",
        "*   Regression (*How much? or How many?*): Linear regression, decision trees, random forest, SVR\n"
      ],
      "metadata": {
        "id": "RnxK2bxoeCWm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regression\n",
        "Regression  is a statistical modeling technique used to explore the relationship between a dependent variable and one or more independent variables. Regression predicts **a** **continous** **numerical** **value**.\n",
        "\n",
        "**Questions** **answered** **by** **regression**: *what is the price of this house? How many sales will we have next month? What will the temperature be tommorrow?*\n",
        "\n",
        "Linear regression is a specific type of regression that assumes a linear relationship between the variables.\n"
      ],
      "metadata": {
        "id": "fQ4WwSm1eJjy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Linear Regression:**\n",
        "Linear regression aims to model the relationship between a dependent variable (response variable) and one independent variable (predictor variable) through a linear equation of the form"
      ],
      "metadata": {
        "id": "N_mSDhx_snD-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Y =wx + b,"
      ],
      "metadata": {
        "id": "-0Nq2Djr4fV5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "where **Y** represents the dependent variable,\n",
        "\n",
        " X represents the independent variable,\n",
        "\n",
        " w is the slope coefficient, and\n",
        "\n",
        " b is the y-intercept.\n",
        "\n",
        "Linear regression assumes that the relationship between the variables is additive, linear, and has constant variance (homoscedasticity). It is suitable for continuous numeric variables and can be used for both prediction and inference.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UNiD-5IouHN0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of linear regression is to estimate the coefficients (w and b)that minimizes the sum of the squared differnces between the observed and predicted values.\n"
      ],
      "metadata": {
        "id": "NOq4TvPzvo3E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tO4yXUSJFhKm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is suitable for continuous numeric variables and can be used for both prediction and inference.\n"
      ],
      "metadata": {
        "id": "bnk2YLe_wTrJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Univariate Analysis:**\n"
      ],
      "metadata": {
        "id": "NKjmgQFkwjNI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Univariate analysis involves examining the relationship between the dependent and a single independent variable. In the context of linear regression, univariate analysis helps determine if there is a significant linear relationship between the dependent variable and the predictor variable.\n",
        "\n",
        "Key steps in univariate analysis include data visualization, correlation analysis, and assessing the assumptions of linear regression (linearity, normality, homoscedasticity).\n",
        "\n"
      ],
      "metadata": {
        "id": "eudBTkpKw_2Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multiple** **Linear** **Regression**:\n"
      ],
      "metadata": {
        "id": "cZkl9lpV2obD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multiple linear regression extends the concept of linear regression to incoporate multiple independent variables. It allows for modeling the relationship between a dependent variable and multiple predictor variables through a linear equation of the form. Multiple linear regression enables us to evaluate the unique contribution of each predictor variable while controlling for the effects of other variables.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZNENgqZQ2yM1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##         Y = b + w1X1 + w2X2+...+wn*Xn           "
      ],
      "metadata": {
        "id": "G-7P1Oog3U9J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The coefficients (b, w1,w2,...,wn) are estimated using techniques like ordinary least squares (OLS) to minimize the sum of squared differences between the observed and predicted values.\n"
      ],
      "metadata": {
        "id": "-4sCcdVX5ERU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.regplot (x = \"room_num\", y = \"price\", data=df)"
      ],
      "metadata": {
        "id": "DjkgR8X25rWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Univariate** **analysis** **with** **python**\n"
      ],
      "metadata": {
        "id": "JW0CnDBpzio3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression"
      ],
      "metadata": {
        "id": "zq7NFWOk6H-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = df['price']\n",
        "x = df[['room_num']]"
      ],
      "metadata": {
        "id": "XyJNxUM06W4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lm2 = LinearRegression() #an empty model object named lm2 (of the LinearRegression Class) is ready to be trained\n",
        "lm2.fit(x,y) #trains the model using your data. The fit() method then finds the best-fitting line through your data by minimizing difference between the predicted values and the actual values"
      ],
      "metadata": {
        "id": "9TMOXVTu75Ff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(lm2.intercept_, lm2.coef_)"
      ],
      "metadata": {
        "id": "t0TITDbZ80X9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code print(lm2.intercept_, lm2.coef_) is printing the coefficients of the linear regression model lm2 that we just trained.\n",
        "\n",
        "\n",
        "\n",
        "*   **lm2.intercept_:** This is the y-intercept of the linear regression line. In the equation Y = wx + b, the intercept is b. It represents the predicted value of Y (price) when X (room_num) is zero. While interpreting this value directly might not always be meaningful in a real-world context (as a house can't have 0 rooms), it's a necessary part of the linear equation.\n",
        "\n",
        "\n",
        "*  **lm2.coef_:** This is the coefficient (or slope) of the independent variable x (room_num). In the equation Y = wx + b, the coefficient is w. It represents the change in the predicted value of Y (price) for a one-unit increase in X (room_num), assuming all other variables are held constant (though in this univariate case, there are no other variables). The output [9.09966966] means that for every additional room, the predicted price of the house increases by approximately 9.1 units (assuming the price is in thousands of dollars, this would be $9,100).\n",
        "\n",
        "So, this line of code is giving us the key parameters of the linear equation that the model has learned to represent the relationship between the number of rooms and the house price."
      ],
      "metadata": {
        "id": "0F6kZcMGa0UU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation Metrics\n",
        "\n",
        "Machine learning evaluation metrics are used to assess the performance and quality of machine learning models. These metrics help us understand how well our models are performing and provide insights into their strengths and weaknesses.\n",
        "\n",
        "**Evaluation** **metrics** **for** **regression** **include**:\n",
        "\n",
        "\n",
        "> R-squared, Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and Mean Absolute Error (MAE).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ct9Bdt_shKzV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred1 = lm2.predict(x) #Given the number of rooms in each house in x, what is your best guess for the price of each house?"
      ],
      "metadata": {
        "id": "yTVcRz889D7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score\n",
        "r2_score(y, y_pred1) #r2 tells the % of target value fluctuation that model accounts for"
      ],
      "metadata": {
        "id": "tKcvmOjj9UGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The r2_score function compares the actual values (y) to the predicted values (y_pred1) and returns a single number (the R-squared score) that tells you how well your model's predictions explain the variability in the actual house prices. As you saw in the output of cell tKcvmOjj9UGT, the R-squared score was 0.4848, meaning your model explained about 48.5% of the variation in house prices using only the number of rooms as a predictor."
      ],
      "metadata": {
        "id": "tmWWCR6eeeF4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "FMr6CavW9ztF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multiple Linear Regression with Python"
      ],
      "metadata": {
        "id": "3wphQHwiB_MO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = df['price']\n",
        "x = df.drop(\"price\",axis=1) #This is crucial.\n",
        "#It tells pandas to drop a column (axis 1 refers to columns, while axis 0 refers to rows).\n",
        "#If you omitted axis=1 or set it to axis=0, pandas would try to drop a row with the index \"price\", which would likely result in an error."
      ],
      "metadata": {
        "id": "w7uaWbt2CGH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above code  is preparing your data for a machine learning model, specifically for Multiple Linear Regression. In supervised learning, you typically need to separate your dataset into two parts: the target variable (what you want to predict) and the features (the variables you will use to make the prediction).\n",
        "\n",
        "In essence, this code is correctly splitting your data into the target variable (y) and the features (x), which is a standard first step before training a regression model."
      ],
      "metadata": {
        "id": "c8tfhvxaYiN3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = LinearRegression() #This line creates an instance (or object) of the LinearRegression class.\n",
        "# At this point, lr is an empty linear regression model.\n",
        "# It hasn't been trained on any data yet, and it doesn't know the relationship between your features (x) and your target variable (y).\n"
      ],
      "metadata": {
        "id": "D8Dq-nMXCtT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = x.dropna() #dropna() in pandas removes columns with missing values\n",
        "y = y[x.index] #aligns y with the remaining rows in x.\n",
        "#This ensures that your feature data (x) and target data (y) have the same rows and are aligned correctly for training the model.\n",
        "lr.fit(x,y) #When I put only this line of code it said there were still some missing values so the first line of code above took care of that.\n",
        "# lr.fit(x,y)) will take the lr object and train it using your prepared data."
      ],
      "metadata": {
        "id": "ZBbqKAMyF7B4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr.intercept_, lr.coef_ #Each coefficient represents the change in the predicted value of Y (price) for a one-unit increase in its corresponding independent variable, assuming all other independent variables are held constant."
      ],
      "metadata": {
        "id": "JsUqjYuqGC_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output (np.float64(-6.67166908831053), array([ 1.97802567e-02, -4.06110145e-02, -1.60885060e+01, 4.00300868e+00, -4.40495236e-03, 1.01395220e+00, -5.75016147e-01, 1.17105151e+00, 3.40523671e-01, 9.68761390e-02, 1.43277989e-02, -1.34128907e-01, -1.20709035e+00])) shows the intercept and the coefficients of your multiple linear regression model.\n",
        "\n",
        "Intercept (-6.67166908831053): This is the estimated average price of a house when all the independent variables (features) are zero. In a real-world scenario, it's often not directly interpretable because it's unlikely to have a house with all features equal to zero. However, it's a necessary part of the linear equation that defines the relationship between the features and the price.\n",
        "\n",
        "Coefficients (the array of numbers): Each number in the array corresponds to a coefficient for each of the independent variables in your model, in the order they appear in your x DataFrame. These coefficients represent the estimated change in the house price for a one-unit increase in the corresponding feature, assuming all other features are held constant.\n",
        "\n",
        "For example, the first coefficient 1.97802567e-02 (which is approximately 0.0198) is the coefficient for crime_rate. This means that for every one-unit increase in the crime rate (after the log transformation), the estimated house price increases by approximately 0.0198 units (assuming the price is in thousands of dollars, this would be about $19.80). Similarly, you can interpret the other coefficients in relation to their corresponding features. The sign of the coefficient indicates the direction of the relationship (positive means price increases with the feature, negative means price decreases)."
      ],
      "metadata": {
        "id": "7QE2j3xN8mV4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = lr.predict(x)"
      ],
      "metadata": {
        "id": "ZbmNrkSGIXpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score #the R-squared score is also known as the coefficient of determination.\n",
        "r2_score(y, y_pred) #Now accuracy has moved up to 72% because other facets of the data is now considered as well"
      ],
      "metadata": {
        "id": "MUOiqvKeIkby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The R-squared score is a key metric used to evaluate the performance of regression models (like the linear regression model you've built). It provides a measure of how well your model's predictions fit the actual data.\n",
        "\n",
        "**Interpretation:** The R-squared score ranges from 0 to 1 (or 0% to 100%).\n",
        "An R-squared of 0 means that your model does not explain any of the variability in the dependent variable (price) around its mean. In other words, the model is no better than simply predicting the average price for all houses.\n",
        "\n",
        "An R-squared of 1 means that your model explains 100% of the variability in the dependent variable. This would indicate a perfect fit, where your model's predictions exactly match the actual prices.\n",
        "\n",
        "An R-squared value between 0 and 1 indicates the proportion of the variance in the dependent variable that is predictable from the independent variables. For example, an R-squared of 0.72 (like you got in cell MUOiqvKeIkby) means that your model explains 72% of the variation in house prices.\n",
        "\n",
        "So, from sklearn.metrics import r2_score is simply making the r2_score function available for you to use in your code to calculate how well your linear regression model is performing."
      ],
      "metadata": {
        "id": "uBLv-0d-b-TZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"Cleared House Data.csv\", index = False)"
      ],
      "metadata": {
        "id": "iz-jRdJp_6cD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "from sklearn.linear_model import LinearRegression #Replace with your actual model\n",
        "\n",
        "model = lr\n",
        "\n",
        "x_train = x\n",
        "\n",
        "#Create a dictionary to store the input widgets\n",
        "input_widgets = {}\n",
        "\n",
        "#Create text input widgets for each feature in x_train\n",
        "for feature in x_train.columns:\n",
        "  input_widgets[feature] = widgets.Text(description=feature + ':')\n",
        "prediction = 0\n",
        "#Create a prediction function\n",
        "def make_prediction(b):\n",
        "  input_features = {}\n",
        "\n",
        "  #Retrieve the input values from the widgets\n",
        "  for feature, widget in input_widgets.items():\n",
        "    input_features[feature] = widget.value\n",
        "\n",
        "  #Prepare the input data as a dictionary\n",
        "  input_data = {feature: [value] for feature, value in input_features.items()}\n",
        "\n",
        "  #Create a Dataframe from the input data\n",
        "  input_df = pd.DataFrame(input_data)\n",
        "\n",
        "  #Make a prediction\n",
        "  prediction = model.predict(input_df)[0]  # Get the first element from the prediction array\n",
        "\n",
        "  # Set prediction to zero if it's negative\n",
        "  if prediction < 0:\n",
        "    prediction = 0\n",
        "\n",
        "  #Display the prediction\n",
        "  with output:\n",
        "    print(f\"Predicted Price: {prediction}\")\n",
        "\n",
        "#Create a button for prediction\n",
        "predict_button = widgets.Button(description='Predict', button_style= \"danger\")\n",
        "predict_button.on_click(make_prediction)\n",
        "output = widgets.Output()\n",
        "\n",
        "#Display the widgets and prediction button\n",
        "input_widgets_list = list(input_widgets.values())\n",
        "input_widgets_list.append(predict_button)\n",
        "input_widgets_list.append(output)\n",
        "display(*input_widgets_list)"
      ],
      "metadata": {
        "id": "C_9U63LrAIg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have an interactive way to make predictions, we could explore:\n",
        "\n",
        "Evaluating the model further: While we have the R-squared score, we could look at other evaluation metrics like Mean Absolute Error (MAE) or Mean Squared Error (MSE) to get a more complete picture of our model's performance.\n",
        "\n",
        "Visualizing the model's performance: creating plots to visualize the predicted prices against the actual prices to see how well the model is fitting the data.\n",
        "\n",
        "Other regression models: We could try different regression algorithms (e.g., Ridge, Lasso, or even more complex models) to see if these can improve prediction accuracy.\n",
        "\n",
        "Feature Engineering: One could revisit the feature engineering steps to see if creating new features or transforming existing ones differently could lead to better model performance.\n",
        "\n",
        "Cross-validation: To get a more robust estimate of this model's performance, one could implement cross-validation.\n"
      ],
      "metadata": {
        "id": "IUy45UnjTJ8Z"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOGSXqMkFa0k"
      },
      "source": [
        "## Evaluation Metrics Cont...\n",
        "\n",
        "Machine learning evaluation metrics are used to assess the performance and quality of machine learning models. These metrics help us understand how well our models are performing and provide insights into their strengths and weaknesses.\n",
        "\n",
        "**Evaluation** **metrics** **for** **classification** include:* Accuracy, Precision, Recall, F1 Score, ROC AUC.*\n",
        "\n",
        "**Questions** **answered** **by** **classifacation**: *Is this email spam or not spam? Is this tumor malignant or benign? What animal is this image (cat, dog, bird)?*\n",
        "\n",
        "Let's explore some commonly used evaluation metrics along with their formulas and examples:\n",
        "\n",
        "1. **Accuracy**: Accuracy measures the proportion of correct predictions compared to the total number of predictions. It is suitable for balanced datasets where the classes are equally represented. However, it can be misleading when dealing with imbalanced datasets.\n",
        "- Formula: $$\\frac{TP + TN} {TP + TN + FP + FN}$$\n",
        "\n",
        "        TP: True Positives, TN: True Negatives, FP: False Positives, FN: False Negatives\n",
        "\n",
        "- Example:\n",
        "\n",
        "| Actual   | Predicted |\n",
        "|----------|-----------|\n",
        "| Yes      | Yes       |\n",
        "| No       | No        |\n",
        "| Yes      | No        |\n",
        "| Yes      | Yes       |\n",
        "| Yes      | Yes       |\n",
        "\n",
        " $Accuracy =\\frac{3 + 1} {5} = 0.8$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For instance:\n",
        "\n",
        "In a binary classification system where man is the positive case (1) and woman is the negative case (0), the true/false positive/negative conditions are as follows:\n",
        "\n",
        "\n",
        "*   True Positive (TP): The actual sex is man (1), and the model correctly predicts man (1). The man is correctly identified.\n",
        "\n",
        "\n",
        "*   True Negative (TN): The actual sex is woman (0), and the model correctly predicts woman (0). The woman is correctly identified.\n",
        "*   False Positive (FP): The actual sex is woman (0), but the model incorrectly predicts man (1). A woman is incorrectly identified as a man (also known as a Type I error or false alarm).\n",
        "\n",
        "\n",
        "*   False Negative (FN): The actual sex is man (1), but the model incorrectly predicts woman (0). A man is incorrectly identified as a woman (also known as a Type II error or missed detection).  \n"
      ],
      "metadata": {
        "id": "guMzbtbfFqNA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Precision and Recall**: Precision and recall are metrics used in binary classification problems, particularly when the classes are imbalanced. Precision measures the proportion of correctly predicted positive instances out of all predicted positives. Recall, also known as sensitivity or true positive rate, measures the proportion of correctly predicted positive instances out of all actual positives.\n",
        "\n",
        "**Precision Formula**: $$\\frac{TP}{TP + FP}$$\n",
        "**Recall Formula:** $$\\frac{TP}{TP + FN}$$\n",
        "**Example:**\n",
        "\n",
        "\n",
        "|Actual | Predicted|\n",
        "|-------|-------|\n",
        "|Yes|Yes|\n",
        "|No|\tYes|\n",
        "|Yes|\tNo|\n",
        "|Yes|\tYes|\n",
        "|No|\tNo|\n",
        "\n",
        "$$Precision = \\frac{2}{2 + 1} = 0.67$$\n",
        "$$Recall = \\frac{2}{2 + 1} = 0.67$$\n",
        "\n",
        "\n",
        "3. **F1 Score**: The F1 score is the harmonic mean of precision and recall. It provides a balance between precision and recall and is useful when we want to consider both metrics together.\n",
        "\n",
        "**Formula**: $$ \\frac{2 \\times Precision \\times Recall}{Precision + Recall} $$\n",
        "Example:\n",
        "$$F1 Score =  \\frac {2 \\times 0.67 \\times 0.67 }{0.67 + 0.67} = 0.67$$\n",
        "\n",
        "4. **Mean Squared Error (MSE)**: MSE is commonly used for regression problems. It measures the average squared difference between the predicted and true values. A lower MSE indicates better performance.\n",
        "\n",
        "**Formula** : $$ \\frac{1}{n} \\times Î£(y_true - y_pred)^2$$\n",
        "**Example** :\n",
        "|y_true|\ty_pred|\n",
        "|-------|-------|\n",
        "|2|\t1.5|\n",
        "|3|\t2.0|\n",
        "|5|\t4.5|\n",
        "|4|\t3.5|\n",
        "|6|\t5.5|\n",
        "\n",
        "$$MSE = \\frac{1}{5} \\times ((2 - 1.5)^2 + (3 - 2.0)^2 + (5 - 4.5)^2 + (4 - 3.5)^2 + (6 - 5.5)^2) = 0.25$$\n",
        "5. **R-squared (Coefficient of Determination)**: R-squared measures the proportion of the variance in the dependent variable that is predictable from the independent variables. It ranges from 0 to 1, with 1 indicating a perfect fit and 0 indicating no linear relationship.\n",
        "\n",
        "**Formula**: $ 1 - \\frac{SSR}{SST}$\n",
        "\n",
        "*SSR: Sum of Squared Residuals, SST: Total Sum of Squares*\n",
        "\n",
        "**Example**:\n",
        "$$R-squared = 1 - \\frac{SSR}{SST} = 1 - \\frac{0.25}{10} = 0.975$$\n",
        "\n",
        "6. **ROC-AUC**: Receiver Operating Characteristic (ROC) curve and Area Under the Curve (AUC) are used for binary classification problems. ROC-AUC evaluates the model's ability to distinguish between classes by plotting the true positive rate against the false positive rate.\n",
        "\n",
        "*ROC Curve*: Plots True Positive Rate (TPR) against False Positive Rate (FPR)\n",
        "\n",
        "*AUC*: Area Under the ROC Curve\n",
        "\n",
        "7. **Log Loss (Logistic Loss)**: Log loss is commonly used for probabilistic classification problems. It measures the logarithm of the likelihood of the predicted probabilities matching the true labels. A lower log loss indicates better calibration of probabilities.\n",
        "\n",
        "**Formula**: $$-\\frac{1}{n} \\times Î£(y_true \\times log(y_pred) + (1 - y_true) \\times log(1 - y_pred))$$\n",
        "\n",
        "8. **Confusion Matrix**: A confusion matrix provides a comprehensive summary of the model's performance, especially in multi-class classification problems. It displays the counts of true positives, true negatives, false positives, and false negatives.\n",
        "\n",
        "Provides counts of *True Positives (TP)*, *True Negatives (TN)*, *False Positives (FP)*, and *False Negatives (FN)*\n",
        "\n",
        "**Example**: See the table with counts of each category.\n",
        "\n",
        "It' s important to choose the appropriate evaluation metric based on the specific problem and requirements. Different metrics provide different insights into the model's performance, and it's often recommended to consider multiple metrics together to gain a comprehensive understanding."
      ],
      "metadata": {
        "id": "v10udAB8rTxm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A** **Personal** **Question**\n",
        "\n",
        "Can we use Prediction and recall for the House Price problem?\n",
        "\n",
        "**No**. Because House Price is not a binary classification problem, where we have exact values that we are looking at. If we had a problem where maybe we expect: Apple, Orange, and Bananas, then we can. But the House Price model is not that kind, its a regression problem."
      ],
      "metadata": {
        "id": "oXpS2jjYrVBG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that a multiclass classification is **not** one where we want to know if its apple or not, say. But we want to know if its apple, or cucumber, or banana"
      ],
      "metadata": {
        "id": "wNdTPZCkrgf_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train-Test Split in Machine Learning\n",
        "\n",
        "Train-test split is a common technique used in machine learning to evaluate the performance of a model on unseen data. It involves splitting the available dataset into two independent subsets: one for training the model (the training set) and the other for evaluating the model's performance (the test set). The train-test split helps assess how well the model generalizes to new, unseen data.\n",
        "\n",
        "**Training** **Set**: Majority of our data (typically 70-80%)\n",
        "\n",
        "**Test** **Set**: separate portion of data to evaluate performance (typically 20-30%)\n",
        "\n",
        "we'll be using another library from *SKLearn*\n"
      ],
      "metadata": {
        "id": "XYFyuysnrpFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "OTxsvp0br3nM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the dataset:\n",
        "df = pd.read_csv(\"/content/Cleaned House Data.csv\",)"
      ],
      "metadata": {
        "id": "PPb5nAhHr7-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "neUN-ZQNsCRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop(\"price\", axis=1) #we drop price because it's our target variable.\n",
        "#If we dont the model will end up performing excellently until new data comes in.\n",
        "y = df['price']"
      ],
      "metadata": {
        "id": "_nemBYNusJHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "#random state 42 is by default the degree of randomness in selecting data for training/testing."
      ],
      "metadata": {
        "id": "cJv6vBubsOE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we are splitting the data into a training set (80% of the data) and a test set (20% of the data). The random_state parameter ensures reproducibility of the split."
      ],
      "metadata": {
        "id": "GgmwZ36-sdA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "utG1B8UpshMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "3S75DgUVs6mS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.shape"
      ],
      "metadata": {
        "id": "hRrwtWFEs8Or"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train-test split is crucial to assess the model's performance on unseen data and avoid overfitting. The model is trained on the training set and then evaluated on the test set to measure its ability to generalize. It helps identify potential issues such as underfitting or overfitting.\n",
        "\n",
        "It's important to note that the train-test split should be representative of the original data to ensure reliable evaluation. The choice of the test set size depends on the dataset size, available data, and specific requirements of the problem at hand.\n"
      ],
      "metadata": {
        "id": "VPNOWci2tFRt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "lr = LinearRegression()"
      ],
      "metadata": {
        "id": "l3EYeuUKtGbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "RScRKEDttQvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = lr.predict(X_train)"
      ],
      "metadata": {
        "id": "RfMn-7vXtUjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score\n",
        "r2_score(y_train, y_pred)"
      ],
      "metadata": {
        "id": "6mFEAo0qtY5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pitfalls in Machine Learning\n",
        "\n",
        "### Overfitting and Underfitting\n",
        "\n",
        "In machine learning, overfitting and underfitting are two common problems that occur when training models. Understanding these concepts is crucial for building robust and accurate machine learning models. Let's take a closer look at each:\n",
        "\n",
        "### 1. Overfitting:\n",
        "\n",
        "Overfitting occurs when a model performs exceptionally well on the training data but fails to generalize well on new, unseen data. It happens when the model becomes too complex or overly flexible, effectively \"memorizing\" the training data instead of learning the underlying patterns. Key characteristics of overfitting include:\n",
        "\n",
        "- The model shows low training error but high test error.\n",
        "- The model captures noise and irrelevant details in the training data.\n",
        "- The model may have excessive complexity, such as having too many features or high polynomial degrees.\n",
        "\n",
        "Remedies for overfitting:\n",
        "\n",
        "- Increase the size of the training dataset.\n",
        "- Reduce the complexity of the model, such as by decreasing the number of features or applying feature selection techniques.\n",
        "- Regularize the model using techniques like L1 or L2 regularization.\n",
        "- Use cross-validation for hyperparameter tuning.\n",
        "### 2. Underfitting:\n",
        "Underfitting occurs when a model is too simple or inflexible to capture the underlying patterns in the training data. It fails to learn the relationships between the features and the target variable, resulting in poor performance on both the training and test data. Key characteristics of underfitting include:\n",
        "\n",
        "- The model shows high training error and high test error.\n",
        "- The model is too simplistic to capture the complexity of the data.\n",
        "- The model may have insufficient features or inadequate training time.\n",
        "\n",
        "Remedies for underfitting:\n",
        "\n",
        "- Increase the complexity of the model, such as by adding more features or increasing the model's capacity.\n",
        "- Perform feature engineering to extract more meaningful features from the data.\n",
        "- Increase the training time or adjust the learning rate for iterative algorithms.\n",
        "\n",
        "Balancing between overfitting and underfitting is a crucial aspect of model training. The goal is to find the right level of complexity that allows the model to generalize well to new, unseen data. Techniques like *cross-validation, regularization, and hyperparameter tuning* play important roles in mitigating overfitting and underfitting.\n",
        "\n",
        "It's essential to monitor the model's performance on both the training and test data and make adjustments accordingly. Regular evaluation and fine-tuning of the model help strike the right balance between complexity and generalization, leading to more accurate and reliable predictions."
      ],
      "metadata": {
        "id": "Xadtei6Ptlu8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_pred = lr.predict(X_test)"
      ],
      "metadata": {
        "id": "NBeuJr_OuRWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score\n",
        "r2_score(y_train, y_pred)"
      ],
      "metadata": {
        "id": "uokMpHj_uTuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r2_score(y_test, y_test_pred)"
      ],
      "metadata": {
        "id": "tV7YlLkJuZ8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Selection\n",
        "Feature selection is an important step in machine learning and data analysis that involves selecting a subset of relevant features from the original set of available features. It aims to improve model performance, reduce computational complexity, and enhance interpretability by focusing on the most informative and influential features. Feature selection helps mitigate the curse of dimensionality and can lead to more accurate and efficient models.\n",
        "Subset and shrinkage methods are two common approaches to feature selection. Let's take a closer look at each:\n",
        "\n",
        "### 1.  Subset Selection Methods:\n",
        "Subset selection methods aim to find the best subset of features that maximizes the model's performance. There are two main types of subset selection methods:\n",
        "\n",
        "- Forward Selection: This method starts with an empty set of features and iteratively adds the most significant feature that improves the model's performance the most until a stopping criterion is met.\n",
        "- Backward Elimination: This method starts with the full set of features and iteratively removes the least significant feature that has the least impact on the model's performance until a stopping criterion is met.\n",
        "\n",
        "Subset selection methods evaluate the performance of different feature subsets using metrics like cross-validation error or information criteria. They can be computationally expensive, especially for large feature sets, but they provide an optimal subset of features based on the evaluation criterion.\n",
        "\n",
        "### 2. Shrinkage Methods:\n",
        "Shrinkage methods, also known as regularization methods, add a penalty term to the objective function during model training. These methods encourage sparse solutions, effectively shrinking less important feature coefficients towards zero. Two popular shrinkage methods are:\n",
        "\n",
        "- Lasso Regression: Lasso regression applies L1 regularization, resulting in sparse solutions where some feature coefficients become exactly zero. It automatically performs feature selection by effectively excluding irrelevant features from the model.\n",
        "- Ridge Regression: Ridge regression applies L2 regularization, which reduces the magnitude of feature coefficients without excluding any feature entirely. While ridge regression does not perform explicit feature selection, it can still reduce the impact of less important features.\n",
        "\n",
        "Shrinkage methods provide a trade-off between model complexity and overfitting. They can effectively handle high-dimensional datasets and multicollinearity by reducing the impact of less relevant features.\n",
        "\n"
      ],
      "metadata": {
        "id": "sju8uZQkuixY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the feature names\n",
        "feature_names = X_train.columns  # Replace <your_feature_names> with the actual feature names from your dataset\n",
        "\n",
        "# Get the coefficients\n",
        "coefficients = lr.coef_\n",
        "\n",
        "# Print feature names and coefficients\n",
        "for feature, coef in zip(feature_names, coefficients):\n",
        "    print(f\"{feature}:\\t\\t\\t {coef}\")"
      ],
      "metadata": {
        "id": "Pcutkyseuj4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_features = [feature for feature, coef in zip(feature_names, coefficients) if coef > 0.5 or coef < -0.5]"
      ],
      "metadata": {
        "id": "muPcp0FVusfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_features\n"
      ],
      "metadata": {
        "id": "U5Nd8KLRuuAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_selected = X_train[selected_features]\n"
      ],
      "metadata": {
        "id": "kT4j9m50uyCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg_model_selected = LinearRegression()"
      ],
      "metadata": {
        "id": "874nmua1u2xZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg_model_selected.fit(X_selected, y_train)"
      ],
      "metadata": {
        "id": "IS2J2nFhu-XG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_selected_pred = reg_model_selected.predict(X_selected)"
      ],
      "metadata": {
        "id": "xMHu2u6AvEtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r2_score(y_train, y_pred)"
      ],
      "metadata": {
        "id": "sF5Xc3PhvGbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r2_score(y_train, y_selected_pred)"
      ],
      "metadata": {
        "id": "g1WMnd-DvKm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_selected_pred = reg_model_selected.predict(X_test[selected_features])"
      ],
      "metadata": {
        "id": "baemk5LJvPSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r2_score(y_test, y_test_pred)"
      ],
      "metadata": {
        "id": "_ARml3G6vS9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r2_score(y_test, y_test_selected_pred)"
      ],
      "metadata": {
        "id": "ASh-kePavW8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Subset selection and shrinkage methods offer different approaches to feature selection, each with its own advantages and considerations. The choice between them depends on the specific problem, the size of the feature set, and the desired balance between model complexity and interpretability. Experimentation and evaluation of different feature selection methods are important to identify the most relevant subset of features for a given problem."
      ],
      "metadata": {
        "id": "6bBwN5Q-viPw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Other Regression Models\n",
        "\n",
        "Linear regression is a widely used regression model, but there are several other regression models that can be applied to different types of data and scenarios. Here are a few examples:\n",
        "\n",
        "#### 1. Ridge Regression:\n",
        "Ridge regression is a regularization technique that adds an L2 penalty term to the linear regression objective function. The penalty term is the sum of the squared values of the model's coefficients. It helps reduce overfitting by shrinking the coefficients only very close to zero. Ridge regression is particularly useful when dealing with multicollinearity in the data.\n",
        "\n",
        "#### 2. Lasso Regression:\n",
        "Lasso regression is another regularization technique that adds an L1 penalty term to the linear regression objective function. The penalty term is the sum of the absolute values of the model's coefficients. It encourages sparsity by setting some coefficients to exactly zero, effectively performing feature selection. Lasso regression is helpful when you want to identify the most relevant features for prediction.\n",
        "\n",
        "#### 3. ElasticNet Regression:\n",
        "ElasticNet regression combines both L1 and L2 regularization terms in the linear regression objective function. It offers a balance between the feature selection capability of Lasso regression and the coefficient shrinkage of Ridge regression. ElasticNet regression is useful when dealing with high-dimensional datasets and multicollinearity.\n",
        "\n",
        "#### 4. Decision Tree Regression:\n",
        "Decision tree regression builds a regression model by recursively partitioning the data based on feature values. It predicts the target variable based on the average target value of the training instances within each leaf node. Decision tree regression can capture complex relationships and handle both numerical and categorical features.\n",
        "\n",
        "#### 5. Random Forest Regression:\n",
        "Random forest regression is an ensemble method that combines multiple decision trees to make predictions. It improves the predictive accuracy and handles overfitting by averaging the predictions of multiple trees. Random forest regression is robust, can handle high-dimensional data, and provides feature importance rankings.\n",
        "\n",
        "\n",
        "These are just a few examples of regression models beyond linear regression. Each model has its own strengths and assumptions, and the choice of model depends on the specific problem, the nature of the data, and the desired trade-offs between accuracy, interpretability, and computational complexity. It's important to understand the characteristics of different regression models and experiment with them to find the most suitable one for your specific task.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MFdlemSmvn3y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><h1>Ridge Regression:</h1></center>"
      ],
      "metadata": {
        "id": "QB6JEiDzvr4z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code syntax used to write \"Ridge Regression\" above, is a snippet of HTML (HyperText Markup Language) used for defining a primary heading and horizontally centering it within the web page"
      ],
      "metadata": {
        "id": "ngXybRXLv4Ir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "# Train the ridge regression model\n",
        "ridge_reg = Ridge(alpha=0.5)  # Specify the regularization strength (alpha)\n",
        "ridge_reg.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "kGGqY3bhv9Oy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_ridge = ridge_reg.predict(X_train)\n",
        "y_test_ridge = ridge_reg.predict(X_test)\n"
      ],
      "metadata": {
        "id": "XaAX1rKRwByN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r2_score(y_train, y_ridge)"
      ],
      "metadata": {
        "id": "ogNtj7pXwDTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r2_score(y_test,y_test_ridge)"
      ],
      "metadata": {
        "id": "4VVI0qDtwGhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lasso Regression:"
      ],
      "metadata": {
        "id": "vaArcQ7xwN6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "# Train the lasso regression model\n",
        "lasso_reg = Lasso(alpha=0.1)  # Specify the regularization strength (alpha)\n",
        "lasso_reg.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "q82WhMHVwUnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_lasso = lasso_reg.predict(X_train)\n",
        "y_test_lasso = lasso_reg.predict(X_test)\n"
      ],
      "metadata": {
        "id": "hweG0pdJwY-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r2_score(y_train, y_lasso)"
      ],
      "metadata": {
        "id": "Hbcqwnc5wcRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r2_score(y_test, y_test_lasso)"
      ],
      "metadata": {
        "id": "SN6uz0NLwgTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ElasticNet Regression:"
      ],
      "metadata": {
        "id": "iu0R-0b9wohT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import ElasticNet\n",
        "\n",
        "# Assuming X and y are your feature matrix and target variable, respectively\n",
        "\n",
        "# Train the elastic net regression model\n",
        "elastic_net = ElasticNet(alpha=0.5, l1_ratio=0.5)  # Specify the regularization strengths (alpha, l1_ratio)\n",
        "elastic_net.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "vQiiDUhHwrxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_elast = elastic_net.predict(X_train)\n",
        "y_test_elast = elastic_net.predict(X_test)"
      ],
      "metadata": {
        "id": "JUmuY5hAwwwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r2_score(y_train, y_elast)\n"
      ],
      "metadata": {
        "id": "VblRGh-nw2kU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r2_score(y_test, y_test_elast)\n"
      ],
      "metadata": {
        "id": "VyedjJs1w54R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree Regression:"
      ],
      "metadata": {
        "id": "AvtXCySfxABa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# Assuming X and y are your feature matrix and target variable, respectively\n",
        "\n",
        "# Train the decision tree regression model\n",
        "tree_reg = DecisionTreeRegressor()\n",
        "tree_reg.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "HRQN4Wn3xD92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7MVsAOdLtZ1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;Logistic regression is a popular machine learning algorithm used for binary classification tasks. Unlike linear regression, which is suitable for continuous target variables, logistic regression predicts the probability of an instance belonging to a specific class. This note provides an overview of logistic regression, its advantages, limitations, key concepts, and evaluation metrics."
      ],
      "metadata": {
        "id": "Xv0YHT-8tajJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XRjM0p90wdxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code **&nbsp;** is an HTML entity that stands for \"non-breaking space\". It was used in the text block above."
      ],
      "metadata": {
        "id": "V0Xu1RKatcQW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understanding Binary Classification\n",
        "Binary classification involves predicting one of two possible outcomes: \"yes\" or \"no,\" \"true\" or \"false,\" or simply 0 and 1. Logistic regression is specifically designed for such problems, where the dependent variable is categorical."
      ],
      "metadata": {
        "id": "Lijz8EZ3wmta"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression Intuition\n",
        "Logistic regression builds upon the concept of linear regression but introduces a nonlinear transformation to restrict the output to a range between 0 and 1. This is achieved using the logistic function, also known as the sigmoid function. The sigmoid function maps any real-valued number to a value between 0 and 1.\n",
        "\n",
        "The equation for logistic regression can be represented as:"
      ],
      "metadata": {
        "id": "yo2jCjv4wn3c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pause\n",
        "\n",
        "To know if a problem requires logistic regression we need to Think OUTPUT. For instance when asking who should recieve a loan or who shouldn't. We do not expect decimal answers (something like 3.567...). We want Yes/No (as in 1 or 0).\n"
      ],
      "metadata": {
        "id": "hkdCCadcFnc4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pause\n",
        "\n",
        "###Activation Functions in neural networks\n",
        "\n",
        "Activation functions are important because they:\n",
        "\n",
        "**Introduce** **non**-**linearity**: Real-world data is often non-linear. Activation functions enable neural networks to model these complex relationships by introducing non-linear transformations.\n",
        "\n",
        "**Enable** **complex** **learning**: They allow the network to learn and recognize complex patterns in data, which is crucial for tasks like object detection, speech recognition, and natural language processing.\n",
        "\n",
        "**Determine** **neuron** **output**: They act as a gatekeeper, deciding if a neuron's input is relevant and how strongly its signal should be passed to the next layer.\n",
        "\n",
        "**Facilitate** **backpropagation**: By providing gradients, activation functions make backpropagation possible, which is how neural networks learn from their errors to update their weight.\n",
        "\n",
        "###Common Types\n",
        "\n",
        "**Sigmoid**: A smooth, S-shaped function that squashes values into a range between 0 and 1. It was historically popular but can suffer from the vanishing gradient problem.\n",
        "\n",
        "**ReLU** (Rectified Linear Unit): Outputs the input directly if it is positive and zero otherwise. It is computationally efficient and helps mitigate the vanishing gradient problem.\n",
        "\n",
        "**Tanh** (Hyperbolic Tangent): Similar to sigmoid but squashes values between -1 and 1, often leading to faster convergence compared to sigmoid.\n",
        "\n",
        "**Softmax**: Typically used in the output layer for multi-class classification, it converts a vector of values into a probability distribution where the sum of all values is 1.\n",
        "\n",
        "**Leaky ReLU**: A variation of ReLU that allows a small, non-zero gradient when the unit is not active, helping to address the \"dying ReLU\" problem.\n",
        "\n",
        "**ELU** (Exponential Linear Unit): Similar to ReLU but can produce negative outputs, which helps with learning, though it is computationally more expensive"
      ],
      "metadata": {
        "id": "Q77c9-KuFrl7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pl4teBBJGAaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kg8ZwAx8-Sph"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"loan-train.csv\")"
      ],
      "metadata": {
        "id": "VGgxcTH9GU0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "8zGVaMdUGYwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "eHgV4bvyGj3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.describe()"
      ],
      "metadata": {
        "id": "elUWqb7mGpv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(x=data.Gender)"
      ],
      "metadata": {
        "id": "7rlkj-74Gt-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(x =data[\"Dependents\"])"
      ],
      "metadata": {
        "id": "xOlbqNGQGzLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(x =data[\"Married\"])"
      ],
      "metadata": {
        "id": "7S-9jMY8G8pk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(x =data[\"Self_Employed\"])"
      ],
      "metadata": {
        "id": "_BqMJfwOHBmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"Gender\"] = data[\"Gender\"].fillna(data[\"Gender\"].mode()[0])\n",
        "data[\"Married\"] = data[\"Married\"].fillna(data[\"Married\"].mode()[0])\n",
        "data[\"Dependents\"] = data[\"Dependents\"].fillna(data[\"Dependents\"].mode()[0])\n",
        "data[\"Self_Employed\"] = data[\"Self_Employed\"].fillna(data[\"Self_Employed\"].mode()[0])"
      ],
      "metadata": {
        "id": "DMjHzMB5HEhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "4uEjh90PHKpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"LoanAmount\"] =data[\"LoanAmount\"].fillna(data[\"LoanAmount\"].mean())\n",
        "data[\"Loan_Amount_Term\"] =data[\"Loan_Amount_Term\"].fillna(data[\"Loan_Amount_Term\"].mean())\n",
        "data[\"Credit_History\"] =data[\"Credit_History\"].fillna(data[\"Credit_History\"].mean())\n"
      ],
      "metadata": {
        "id": "C_1sdX1bHPOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(x = data[\"LoanAmount\"])"
      ],
      "metadata": {
        "id": "3NLMMBjPHT3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(x = data[\"Loan_Amount_Term\"]) #majority is less than 400. Only one is close to 500."
      ],
      "metadata": {
        "id": "EfbzBgrIHXxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6gzlC7X4HdJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pause\n",
        "Scatter plots and box plots do not perform the same function, though both are used for data visualization. They each serve different purposes and highlight different aspects of data:\n",
        "\n",
        "**Scatter** **Plot**: A scatter plot displays individual data points on a two-dimensional graph. It is primarily used to observe and show relationships between two quantitative variables. Each dot represents an observation, and its position on the x and y axes corresponds to the values of the two variables. Scatter plots are excellent for identifying correlations, clusters, outliers, and patterns (e.g., linear, non-linear relationships).\n",
        "\n",
        "**Box** **Plot** (or Box-and-Whisker Plot): A box plot is a standardized way of displaying the distribution of data based on a five-number summary: minimum, first quartile (Q1), median (Q2), third quartile (Q3), and maximum. It's often used to compare the distributions of a quantitative variable across different categories or groups. Box plots are good for showing central tendency, variability, skewness, and potential outliers in a dataset, but they do not show individual data points or direct relationships between two different variables in the same way a scatter plot does.\n",
        "\n",
        "If we wanted to see how LoanAmount relates to ApplicantIncome, for example, a scatter plot would be appropriate. Since the goal was *to understand the individual distributions of LoanAmount and Loan_Amount_Term, box plots were the right choice*"
      ],
      "metadata": {
        "id": "qDL-3qLUHiIW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_IfOA5ZzuqGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "scXAlLIIHjPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['Loan_Status'].unique().tolist()"
      ],
      "metadata": {
        "id": "7n_svnYkHogj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder"
      ],
      "metadata": {
        "id": "gjPv6GjXHuK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = LabelEncoder()\n",
        "\n",
        "data['Gender'] = encoder.fit_transform(data['Gender'])"
      ],
      "metadata": {
        "id": "FOUT_5nHHyBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "order_mapping = [['No', 'Yes']]\n",
        "encoder = OrdinalEncoder(categories=order_mapping)\n",
        "encoded_data = encoder.fit_transform(data[['Married']])\n",
        "data['Married'] = encoded_data"
      ],
      "metadata": {
        "id": "k9GRmX9qH3DB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "order_map = [['0', '1', '2', '3+']]\n",
        "encoder = OrdinalEncoder(categories=order_map)\n",
        "encoded_data = encoder.fit_transform(data[['Dependents']])\n",
        "data['Dependents'] = encoded_data"
      ],
      "metadata": {
        "id": "U-hm4QmGH6uo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "order_map = [['Not Graduate','Graduate']]\n",
        "encoder = OrdinalEncoder(categories=order_map)\n",
        "encoded_data = encoder.fit_transform(data[['Education']])\n",
        "data['Education'] = encoded_data"
      ],
      "metadata": {
        "id": "acwDVHUwIBS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "order_map = [['Rural', 'Semiurban', 'Urban']]\n",
        "encoder = OrdinalEncoder(categories=order_map)\n",
        "encoded_data = encoder.fit_transform(data[['Property_Area']])\n",
        "data['Property_Area'] = encoded_data"
      ],
      "metadata": {
        "id": "kB6YFYKUIDI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "order_map = [['Y', 'N'][::-1]]\n",
        "encoder = OrdinalEncoder(categories=order_map)\n",
        "encoded_data = encoder.fit_transform(data[['Loan_Status']])\n",
        "data['Loan_Status'] = encoded_data"
      ],
      "metadata": {
        "id": "Cs_lhqFHIJzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "order_map = [['No', 'Yes']]\n",
        "encoder = OrdinalEncoder(categories=order_map)\n",
        "encoded_data = encoder.fit_transform(data[['Self_Employed']])\n",
        "data['Self_Employed'] = encoded_data"
      ],
      "metadata": {
        "id": "o-y6DIABIPd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"Self_Employed\"].unique()"
      ],
      "metadata": {
        "id": "WvuwBP0eIRWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "ennoGNMdIU1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "sxcHkn-cvONb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del data['Loan_ID']"
      ],
      "metadata": {
        "id": "zPJShEn2IYgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize = (13, 13))\n",
        "# Plot the correlation matrix as a heatmap\n",
        "sns.heatmap(data.corr(), annot=True, cmap='RdYlGn', center=0, square=True)\n"
      ],
      "metadata": {
        "id": "DaZQSp-ZIdGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = data.drop(\"Loan_Status\", axis=1) #X is data without loan status\n",
        "y = data['Loan_Status'] #y is data with loan status"
      ],
      "metadata": {
        "id": "esj_UZyZIm3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Splitting the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "8z29G9B0IoLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.info()"
      ],
      "metadata": {
        "id": "qsQ_GkarIrhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "# Creating an instance of the Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Training the model on the training data\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "k3y0OJiAIy4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYGtdfCs-SqC"
      },
      "outputs": [],
      "source": [
        "# Making predictions on the test data\n",
        "y_pred = model.predict(X_train)\n",
        "y_pred_test = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "# Evaluating the model's accuracy\n",
        "accuracy = accuracy_score(y_train, y_pred)\n",
        "print(\"Train Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "mgsB_JxRJAJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "# Evaluating the model's accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred_test)\n",
        "print(\"Test Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "jIjEbOKpJBsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred_test)"
      ],
      "metadata": {
        "id": "PMSi53N7JFts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Fa-IU_SDJM2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r9qDg0UVJS09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lets dig deeper\n",
        "\n",
        "The confusion matrix cm = array([[18, 25], [1, 79]]) represents the performance of your logistic regression model on the test data. To interpret it, we need to recall how the Loan_Status was encoded:\n",
        "\n",
        "'Y' (Loan Approved) was encoded as 1.\n",
        "\n",
        "'N' (Loan Not Approved) was encoded as 0.\n",
        "\n",
        "In a confusion matrix generated by sklearn, the rows represent the true labels, and the columns represent the predicted labels. So, in this context:\n",
        "\n",
        "\n",
        "###True Positive (TP): cm[1,1] = 79\n",
        "\n",
        "These are the cases where the actual loan status was 'Y' (Approved), and the model correctly predicted 'Y' (Approved). (79 instances)\n",
        "\n",
        "###True Negative (TN): cm[0,0] = 18\n",
        "\n",
        "These are the cases where the actual loan status was 'N' (Not Approved), and the model correctly predicted 'N' (Not Approved). (18 instances)\n",
        "\n",
        "###False Positive (FP): cm[0,1] = 25\n",
        "\n",
        "These are the cases where the actual loan status was 'N' (Not Approved), but the model incorrectly predicted 'Y' (Approved). This is a Type I error. (25 instances)\n",
        "\n",
        "###False Negative (FN): cm[1,0] = 1\n",
        "\n",
        "These are the cases where the actual loan status was 'Y' (Approved), but the model incorrectly predicted 'N' (Not Approved). This is a Type II error. (1 instance)\n",
        "\n",
        "###What the output means in this context:\n",
        "\n",
        "Your model correctly identified 79 applicants who would have their loans approved (True Positives) and 18 applicants who would not (True Negatives). However, it made 25 mistakes by predicting approval for applicants who would actually be rejected (False Positives). Conversely, it only failed to approve 1 applicant who should have received a loan (False Negative). Depending on the business context, the cost of False Positives versus False Negatives can vary significantly."
      ],
      "metadata": {
        "id": "yUPxFSV8JW79"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9i7hB32RJanj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## More digging\n",
        "\n",
        "To reduce False Positives (FP), you can consider several strategies. In the context of your logistic regression model, a primary approach involves adjusting the decision threshold.\n",
        "\n",
        "1. Adjusting the Decision Threshold\n",
        "Logistic regression models typically output a probability score (between 0 and 1) for an instance belonging to the positive class. A default threshold of 0.5 is often used, meaning if the probability is >= 0.5, it's classified as positive (loan approved), otherwise negative (loan not approved). To reduce False Positives (classifying 'N' as 'Y' incorrectly):\n",
        "\n",
        "\n",
        "> Increase the threshold: If you increase the threshold (e.g., to 0.6 or 0.7), the model will require a higher probability to predict 'Y'. This will make the model more conservative in predicting 'Y', thereby reducing the number of False Positives. However, this might increase False Negatives (classifying 'Y' as 'N' incorrectly).\n",
        "\n",
        "2. Feature Engineering\n",
        "Review your features and consider if there are other relevant variables or ways to transform existing ones that could help the model better distinguish between actual 'N' and 'Y' cases. For example, creating interaction terms or polynomial features.\n",
        "\n",
        "3. Different Algorithms\n",
        "While logistic regression is a good baseline, other classification algorithms might perform better in terms of reducing FPs, depending on your dataset's characteristics. Examples include:\n",
        "\n",
        "\n",
        "> Support Vector Machines (SVMs)\n",
        "Random Forests\n",
        "Gradient Boosting Machines (e.g., XGBoost, LightGBM)\n",
        "\n",
        "4. Cost-Sensitive Learning\n",
        "If the cost of a False Positive is significantly higher than a False Negative in your application (which seems to be the case here, as lending money to someone who won't repay can be costly), you can incorporate cost-sensitive learning techniques. This can involve:\n",
        "\n",
        "\n",
        "> Assigning different weights to misclassification errors during model training.\n",
        "Resampling techniques (e.g., oversampling the minority class or undersampling the majority class) if your dataset is imbalanced (fewer 'N's than 'Y's).\n",
        "\n",
        "5. Collecting More Data\n",
        "Sometimes, having more diverse and representative data can help the model learn the patterns better and reduce errors.\n",
        "\n",
        "Choice of strategy depends on the specific trade-offs we are willing to make between False Positives and False Negatives. We'll often need to iterate and evaluate your model's performance using metrics like precision, recall, F1-score, or ROC curves to find the optimal balance."
      ],
      "metadata": {
        "id": "93bi-JpmJdyD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred"
      ],
      "metadata": {
        "id": "6xUgXQskJksA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}