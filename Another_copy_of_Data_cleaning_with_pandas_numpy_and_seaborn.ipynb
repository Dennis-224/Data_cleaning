{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dennis-224/Data_cleaning/blob/main/Another_copy_of_Data_cleaning_with_pandas_numpy_and_seaborn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTjTOeFBovBx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#We now load our csv dataset file\n"
      ],
      "metadata": {
        "id": "j_L2341vpKVV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " df = pd.read_csv('/content/House_Price - House_Price.csv')"
      ],
      "metadata": {
        "id": "XthR_ABaCSnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# We now do a snapy EDD\n"
      ],
      "metadata": {
        "id": "JutX-BsyvjLh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.describe(include= 'all'))\n",
        "df.info()\n",
        "df.head()"
      ],
      "metadata": {
        "id": "KpVEybfvvn5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Time to detect outliers\n"
      ],
      "metadata": {
        "id": "AmXK4I8vxQDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
        "sns.jointplot(x='n_hot_rooms', y='price', data=df)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "G2ABNVanxYDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#In like manner for multiple columns...\n"
      ],
      "metadata": {
        "id": "hgMmMiP10Blx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for col in ['rainfall']: # replace or extend with other numeric cols\n",
        "    sns.jointplot(x=col, y='price', data=df)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "CXEc1FIB0JGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Categorical inspection and visualization\n"
      ],
      "metadata": {
        "id": "rICfTv5c1HGb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Unique values in airport column:\", df['airport'].unique())\n",
        "sns.countplot(x='airport', data=df)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rbM1ZOzY1Kg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#General approach for categorical columns\n",
        "\n"
      ],
      "metadata": {
        "id": "yHoauQ_P1sYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cat_cols = df.select_dtypes(include='object').columns.tolist()\n",
        "for col in cat_cols:\n",
        "    print(f\"Column: {col} -- unique values: {df[col].unique()}\")\n",
        "    sns.countplot(x=col, data=df)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "NKy11xMM14rT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Time to takle missing values\n",
        "#After cleaning we print the data again to confirm\n"
      ],
      "metadata": {
        "id": "8Pr91Jn22iHP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['waterbody'] = df['waterbody'].fillna(df['waterbody'].mode()[0])\n",
        "df['n_hos_beds'] = df['n_hos_beds'].fillna(df['n_hos_beds'].mode()[0])\n",
        "print(df.describe(include= 'all'))\n",
        "df.info()\n",
        "df.head()"
      ],
      "metadata": {
        "id": "RiYqGFr82m1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We continue with dealing with outliers\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TRI1kG-oYW24"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Capping**: Based on the visualizations, it appears that the outliers are on the higher end of the n_hot_rooms values. In this case, capping would be the appropriate method to handle these outliers. Capping involves setting a maximum value for the outliers, effectively replacing them with this upper limit. In layman terms when data is at higher side, you bring down to lower range\n"
      ],
      "metadata": {
        "id": "oGfyBX7ceqxw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.percentile(df.n_hot_rooms,[99]) #we do the 99th percentile for capping"
      ],
      "metadata": {
        "id": "VZo8HCDyfACi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3vbVUI38enCp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "uv = np.percentile(df.n_hot_rooms,[99])[0] #we want it to start from the very first column value"
      ],
      "metadata": {
        "id": "LaAr58ZTeWzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[(df.n_hot_rooms>uv)]\n",
        "df.n_hot_rooms[(df.n_hot_rooms> 3*uv)] = 3*uv"
      ],
      "metadata": {
        "id": "TKNoIcTweMZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we check that its been effected...\n"
      ],
      "metadata": {
        "id": "XJqv81cjh52A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
        "sns.jointplot(x='n_hot_rooms', y='price', data=df)\n",
        "plt.show() #Before the outliers was around 80 but now it comes down to 46"
      ],
      "metadata": {
        "id": "1MzZ0y7SiC8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Flooring** is a technique used in data preprocessing to handle outliers that are significantly lower than the majority of the data. Similar to capping, which sets an upper limit for outliers, flooring sets a lower limit.\n",
        "\n",
        "Essentially, any data point below a certain calculated minimum value (often a percentile like the 1st percentile) is replaced with that minimum value. This helps to reduce the impact of extremely low values on your analysis or model\n",
        "\n",
        "Flooring is like an opposite to capping\n"
      ],
      "metadata": {
        "id": "NIUBkzoCh4aA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.percentile(df.rainfall,[1])[0]"
      ],
      "metadata": {
        "id": "Rdko3VrSlLdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.percentile(df.rainfall,[1])[0]\n",
        "lv = np.percentile(df.rainfall,[1])[0]\n",
        "df.rainfall[(df.rainfall< 0.3*lv)] = 0.3*lv"
      ],
      "metadata": {
        "id": "PyljSSa_jFxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
        "sns.jointplot(x='rainfall', y='price', data=df)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "of4W-n8_la1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Seasonality** can affect our data\n",
        "Lets ask AI how that could happen here..."
      ],
      "metadata": {
        "id": "yG9uYY4mmqAW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Rainfall**: This column directly relates to weather patterns, which are seasonal. High rainfall seasons could correlate with certain property conditions or impact outdoor amenities like parks.\n",
        "**Waterbody** **levels**: If the \"waterbody\" column relates to natural water sources, their levels could fluctuate seasonally due to rainfall and evaporation, potentially influencing the desirability or accessibility of properties near them.\n",
        "**Parks**: The usage and appeal of parks could be seasonal, affecting nearby property values or interest.\n",
        "**Air** **quality**: Air quality can sometimes have seasonal variations depending on factors like temperature inversions, pollution sources, and wind patterns\n"
      ],
      "metadata": {
        "id": "uFYg1-f1nY_y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Non Usable Variable.**\n",
        "* If a table has just one value thats non usable bacause it won't relate with anything. An example is Bus_ter that has only YES\n",
        "* if we get an initail data having low fill rate (i.e 60% data is missing), then we drop the entire data.\n",
        "* Imagine we have a variable telling us make of cars. It can't help us in predicting house price so we delete such variable\n",
        "\n"
      ],
      "metadata": {
        "id": "rWEPBTvuT6Um"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.bus_ter.unique().tolist()\n",
        "sns.countplot(x=df.bus_ter)\n",
        "df = df.drop('bus_ter', axis=1) #Now  after this when we check for bus_ter variable we'll get an error message"
      ],
      "metadata": {
        "id": "5YB8TxYmVqjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.bus_ter"
      ],
      "metadata": {
        "id": "tY3NgcSkX3ZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bivariate Analysis according to AI**\n",
        "Bivariate analysis is a statistical method used to explore the relationship between two variables. This can involve looking at how changes in one variable are associated with changes in the other, and the strength and direction of that association. Common techniques for bivariate analysis include:\n",
        "* Scatter plots: To visualize the relationship between two numerical variables.\n",
        "* Correlation coefficients: To quantify the strength and direction of the linear relationship between two numerical variables.\n",
        "* Cross-tabulation and chi-square tests: To examine the relationship between two categorical variables.\n",
        "* Box plots or violin plots: To compare the distribution of a numerical variable across different categories of a categorical variable.\n",
        "\n",
        "It helps in understanding how variables interact with each other, which is crucial for identifying patterns, making predictions, and building models."
      ],
      "metadata": {
        "id": "SirWhF7mYBT-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Histogram.**(AI) Now when we plot a histogram we get to see how skewed our data is.\n",
        "A positively skewed data has the top of the bell on the left hand side. if the top of the bell is on the right hand side, then its negatively skewed.\n"
      ],
      "metadata": {
        "id": "3r6MTKxFZ53c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Skewed** **data**\n",
        "skewed data refers to a situation where the distribution of values in a dataset is asssymetyric, deviating from the normal or bell-shaped distribution. Skewness can have implications for statistical analysis and modeling, as it can affect the accuracy of assumptions made by certain methods.\n"
      ],
      "metadata": {
        "id": "Pd4nUw71SpkX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Type\n"
      ],
      "metadata": {
        "id": "dUXHVC5YUBMO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are two common types of skewness:\n",
        "1. Positive skewness: In this case, the tail of the distribution extends towards the right side, indicating a larger number of smaller values and a few extremely large values.\n"
      ],
      "metadata": {
        "id": "61ACbNRnUEnU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Negative skewness: Here the tail of the distribution extends towards the left side, indicating a larger number of larger values and a few extremely small values\n"
      ],
      "metadata": {
        "id": "OmQPNqdIU76s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Handling skewed data is important because it can affect the performance of certain models and statistical tests that assume a normal distribution. Here are some common methods to handle skewed data:\n"
      ],
      "metadata": {
        "id": "f_TWPXIyVy6A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Logarithmic transformation: Applying a logarithmic transformation to the skewed variable can compress the larger values and spread out the smaller values, reducing the skewness and making the distribution more symmatrical.\n"
      ],
      "metadata": {
        "id": "-QA2S6sKWNCn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Square root transformation: Similar to the logarithmic transformation, the square root transformation can help reduce the skewness of the data.\n"
      ],
      "metadata": {
        "id": "6WENCkEdWsNj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Box-Cox transformation: The Box-Cox transformation is a more general transformation that can handle various types of skewness. It applies a power transformation to the data, allowing it to become more normally distributed.\n"
      ],
      "metadata": {
        "id": "GxrIhmDeW--E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.distplot(x= df.price, hist=True, kde=True)"
      ],
      "metadata": {
        "id": "FDmPRQDmbPUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Logarithm or Square root** according to AI\n",
        "Logarithm and square root transformations are often used in data preprocessing, particularly when dealing with skewed data or when preparing data for certain statistical models that assume normality."
      ],
      "metadata": {
        "id": "T1JI2f1yceUt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this context (according to AI), where you have already looked at the distribution of the price variable using a histogram (which showed some skewness), here are some potential places where you might consider using logarithm or square root transformations:\n",
        "**Handling Skewed Variables**: If any of the numerical features (like crime_rate, resid_area, air_qual, age, poor_prop, or even price itself if you plan to model it) are highly skewed, applying a logarithm or square root transformation can help make their distributions more symmetrical and closer to a normal distribution. This can be beneficial for some regression models.\n",
        "**Stabilizing Variance:** For some variables, the variance might not be constant across the range of values. Transformations can sometimes help stabilize the variance, which is another assumption of certain statistical models.\n",
        "**Before Modeling:** When you move towards building a predictive model (like linear regression) to predict house prices, transforming skewed independent variables can improve the performance and interpretability of the model.\n",
        "\n",
        "To determine if a transformation is needed and which one to use, you would typically:\n",
        "\n",
        "Visualize the distribution of the variable (using histograms or density plots) to check for skewness.\n",
        "Consider the relationship between the independent variables and the dependent variable (price). Transformations might help linearize non-linear relationships.\n",
        "\n",
        "You could apply these transformations using functions like np.log() or np.sqrt() from NumPy to the relevant columns in your DataFrame.\n",
        "\n"
      ],
      "metadata": {
        "id": "KG1GkZCDdFma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(x=df.crime_rate, y=np.log(df.price)) #notice how we see abnomalities of some values towards 80"
      ],
      "metadata": {
        "id": "jgM1O0Npe51a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.log(df.crime_rate).describe() #The issue with logarithm is that it can lead to negative values. And imagine having a negative value for crime rate. Its not feasible!"
      ],
      "metadata": {
        "id": "nvuzd-pzgTKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.log(1+ df.crime_rate).describe() #we rectify it by adding +1 to our code"
      ],
      "metadata": {
        "id": "-n1_GMu9g8zG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# but square root doesn't need plus 1\n",
        "np.sqrt(df.crime_rate).describe()"
      ],
      "metadata": {
        "id": "apAP6L8ShYvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import boxcox\n",
        "pd.Series(boxcox(df['crime_rate'])[0].tolist()).describe()"
      ],
      "metadata": {
        "id": "PBnvhnxRZHyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we finish it up\n"
      ],
      "metadata": {
        "id": "3bLbnx9iiG7_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.crime_rate = np.log(1+df.crime_rate)"
      ],
      "metadata": {
        "id": "gmu5RDvPazJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.distplot(x=df.crime_rate, hist = True, kde = True)"
      ],
      "metadata": {
        "id": "jWpGYaCJbXha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(x=df.crime_rate, y=df.price)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CZqSUrsSckIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CONGRATULATIONS**!!!!!!\n"
      ],
      "metadata": {
        "id": "8gtPniwBcv_A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You have successfully cleaned the data\n"
      ],
      "metadata": {
        "id": "bVsK1VZAc_oN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Handling categorical columns\n"
      ],
      "metadata": {
        "id": "HmqWUqYtdJfM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Handling categorical columns is an important task in data analysis and modeling as categorical variables contain non-numerical values that require special treatment. Categorical variables provide valuable information about the different groups or categories in a dataset. Here's a note on handling categorical columns:\n"
      ],
      "metadata": {
        "id": "0Jikmga3dbaU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Types of Categorical Columns\n"
      ],
      "metadata": {
        "id": "QlthAot3eF5w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Categorical variables can be classified into two types:\n",
        "\n",
        "\n",
        "1.   Nominal Variables: These variables have categories with no inherent order or ranking. Examples include gender (male, female)or city(Abuja, Lagos, Benin).\n",
        "2.   Ordinal Variables: These variables have categories with a specific order or ranking. Examples include educational level (high school, college, graduate school) or satisfaction level (low, medium, high).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_hHdDSNCeLp3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Encoding\n"
      ],
      "metadata": {
        "id": "5TI9R_qNfh3b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Categorical variables need to be encoded into numerical values for analysis or modeling. Some common encoding techniques include:\n",
        "\n",
        "\n",
        "1.   One-Hot Encoding: This technique creates binary columns for each category, representing the presence or absence of the category in each observation.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nWCqQTaPfm-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_df = pd.get_dummies(df) #Now with this code we no longer get YES and NO.But rather True and False\n",
        "encoded_df.head()"
      ],
      "metadata": {
        "id": "L571HBjv-CRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_df.shape,df.shape"
      ],
      "metadata": {
        "id": "QYHDZm-o_6sl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Ordinal Encoding: This technique assigns a unique integer value to each category, preserving the ordinal relationship among categories. It is suitable when the categories have an inherent order or rank.\n",
        "\n"
      ],
      "metadata": {
        "id": "ubOkXZRoAIEx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Fill missing values in 'waterbody' with 'None'\n",
        "df['waterbody'] = df['waterbody'].fillna('None')\n",
        "\n",
        "# Manually specify the order mapping, including 'None'\n",
        "order_mapping = [['None', 'River', 'Lake', 'Lake and River']]\n",
        "\n",
        "# Create and fit the OrdinalEncoder with the custom order mapping\n",
        "encoder = OrdinalEncoder(categories=order_mapping)\n",
        "encoded_data = encoder.fit_transform(df[['waterbody']])\n",
        "df['waterbody_encoded'] = encoded_data"
      ],
      "metadata": {
        "id": "tnnGK_3ZsVTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.waterbody.unique().tolist()"
      ],
      "metadata": {
        "id": "z6SRuYrWt9fn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del df['waterbody']"
      ],
      "metadata": {
        "id": "MAZjrz8nuNmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "eXRkGFT5uTef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HZxdkkt7_Rkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Label Encoding: Similar to ordinal encoding, label encoding assigns a unique integer value to each category, but it does not preserve any inherent order. it is suitable when the categories are unordered.\n"
      ],
      "metadata": {
        "id": "o2RDvAlFvB4Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "encoded_data = encoder.fit_transform(df['airport'])\n",
        "df['airport'] = encoded_data\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "3wJMpw1Yvg6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Correlation\n"
      ],
      "metadata": {
        "id": "UscHx5PIM_Al"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation is a statistical measure that indicates the extent to which variables fluctuate together. A positive correlation indicates the extent to which those variables increase or decrease in parallel; a negative correlation indicates the extent to which one variable increases as the other decreases.\n"
      ],
      "metadata": {
        "id": "8fptl9bNNFK6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Examples\n"
      ],
      "metadata": {
        "id": "e_kAkIBXPBry"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some examples of data that have a high correlation:\n",
        "\n",
        "\n",
        "*   Your caloric intake and your weight\n",
        "*   The amount of time you study and your GPA (Me: I'd say debatable)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "69wN2rHMPFp0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some examples of data that have a low correlation (or none at all):\n",
        "\n",
        "\n",
        "*   A person name and the chances of them commiting violence\n",
        "*   The colour of a house and how much the building cost\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IiytVCqePmmz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Correlation Co-efficient\n"
      ],
      "metadata": {
        "id": "bALNQCjnQIXV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definition\n",
        "\n",
        "\n",
        "*   A correlation coefficient is a way to put to the relationship.\n",
        "*   Correlation coefficient have a value between -1 and 1.\n",
        "\n",
        "\n",
        "*   A \"O\" means there is no relationship between the variables at all,\n",
        "*   While -1 or 1 means that there is a perfect negative or positive correlation\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6XjOh0SXQTj_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Examples\n"
      ],
      "metadata": {
        "id": "mw8S8plKRMwa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e29641c"
      },
      "source": [
        "For instance(AI)\n",
        "\n",
        "* A correlation coefficient of 0.9 would indicate a strong positive correlation (e.g., the amount of time you study and your GPA).\n",
        "* A correlation coefficient of -0.8 would indicate a strong negative correlation (e.g., the outside temperature and heating costs).\n",
        "* A correlation coefficient of 0.1 would indicate a very weak positive correlation (e.g., a person's height and their salary).\n",
        "* A correlation coefficient of -0.05 would indicate a very weak negative correlation (e.g., the color of a house and its selling price)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Correlation matrix\n"
      ],
      "metadata": {
        "id": "kOVuOZ9sRPZA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definition\n",
        "\n",
        "\n",
        "*   A correlation matrix is a table showing correlation coefficients between variables.\n",
        "*   Each cell in the table shows a correlation between two variables.\n",
        "\n",
        "\n",
        "*   A correlation matrix is used as a way to summarize data, as input into a more advanced analysis, and as a diagnostic for  advanced analysis\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7GOb2IdcRzjy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Application\n"
      ],
      "metadata": {
        "id": "RTCsKg2-SrUx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   To summarize a larger amount of data when the goal is to see patterns.\n",
        "*   To identify colinearity in the data\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5bjdH9PRS9cY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multicolinearity\n"
      ],
      "metadata": {
        "id": "6N7iLu8pTV7G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definition\n"
      ],
      "metadata": {
        "id": "tt4f8B4RTdrC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Multicolinearity exists whenever two or more of the predictors in a  regression model are moderately or highly correlated.\n"
      ],
      "metadata": {
        "id": "5Eqk6EB7TjyO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Effects\n"
      ],
      "metadata": {
        "id": "JpZBypN6UNuX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Multicolinearity results in a change in the signs as well as in the partial regression coefficients from one sample to another sample.\n",
        "*   Multicolinearity makes it tedious to assess the relative importance of the independent variables in explaining the variation caused by the dependent variable.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rnqjRA2DUVsA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solution\n"
      ],
      "metadata": {
        "id": "C51s1dSbWAxF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Remove highly correlated independent variables by looking at the correlation matrix and VIF\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rXt49VzkWEUa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3914fcc6"
      },
      "source": [
        "Solution (AI)\n",
        "\n",
        "* Remove highly correlated independent variables by looking at the correlation matrix and VIF (Variance Inflation Factor). VIF quantifies how much the variance of the estimated regression coefficient is increased due to multicollinearity. A high VIF for a variable indicates it is highly correlated with other predictor variables.\n",
        "* Combine correlated variables into a single variable (e.g., create an average or index).\n",
        "* Use regularization techniques (like Lasso or Ridge regression) that can handle multicollinearity.\n",
        "* Collect more data, if possible, as multicollinearity can be a sample-specific issue."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize = (13, 13))\n",
        "# Plot the correlation matrix as a heatmap\n",
        "sns.heatmap(df.corr(), annot=True, cmap='RdYlGn', center=0, square=True)"
      ],
      "metadata": {
        "id": "YL-dbdQ1WaVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['avg_dist'] = (df.dist1+df.dist2+df.dist3+df.dist4)/4\n"
      ],
      "metadata": {
        "id": "57JJH8y-bxSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del df['dist1']\n",
        "del df['dist2']\n",
        "del df['dist3']\n",
        "del df['dist4']\n"
      ],
      "metadata": {
        "id": "5L_2i5XdcNl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "TSO2dnOcclw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize = (13, 13))\n",
        "# Plot the correlation matrix as a heatmap\n",
        "sns.heatmap(df.corr(), annot=True, cmap='RdYlGn', center=0, square=True)"
      ],
      "metadata": {
        "id": "GEs-uTLedSWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del df['parks']"
      ],
      "metadata": {
        "id": "Dplu5shJdpNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "UQh1N07ddtr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"Cleaned House Data.csv\", index = False)"
      ],
      "metadata": {
        "id": "dRi659N6d0FF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# REGRESSION\n"
      ],
      "metadata": {
        "id": "L3LTI8yAeGyA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regression analysis is a statistical modeling technique used to explore the relationship between a dependent variable and one or more independent variables. Linear regression is a specific type of regression analysis that assumes a linear relationship between the variables. Here's a short note on linear regression, starting with univariate analysis before considering multiple linear regression:\n"
      ],
      "metadata": {
        "id": "fQ4WwSm1eJjy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Linear Regression:**"
      ],
      "metadata": {
        "id": "N_mSDhx_snD-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear regression aims to model the relationship between a dependent variable (response variable) and one independent variable (predictor variable) through a linear equation of the form\n",
        "\n"
      ],
      "metadata": {
        "id": "QicJnBiltIQE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Y =wx + b,"
      ],
      "metadata": {
        "id": "-0Nq2Djr4fV5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "where Y represents the dependent variable,\n"
      ],
      "metadata": {
        "id": "UNiD-5IouHN0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "X represents the independent variable,\n"
      ],
      "metadata": {
        "id": "sFlDCs7lvQrY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "b is the y-intercept, and\n"
      ],
      "metadata": {
        "id": "gOWvrLM_vb2p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "w is the slope coefficient.\n"
      ],
      "metadata": {
        "id": "3In2Yh--vggt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of linear regression is to estimate the coefficients (w and b)that minimizes the sum of the squared differnces between the observed and predicted values.\n"
      ],
      "metadata": {
        "id": "NOq4TvPzvo3E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear regression assumes that the relationship between the variables is additive, linear, and has constant variance (homoscedasticity).\n"
      ],
      "metadata": {
        "id": "DcG8wM44v-S4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is suitable for continuous numeric variables and can be used for both prediction and inference.\n"
      ],
      "metadata": {
        "id": "bnk2YLe_wTrJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Univariate Analysis:**\n"
      ],
      "metadata": {
        "id": "NKjmgQFkwjNI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Univariate analysis involves examining the relationship between the dependent and a single independent variable. In the context of linear regression, univariate analysis helps determine if there is a significant linear relationship between the dependent variable and the predictor variable.\n"
      ],
      "metadata": {
        "id": "eudBTkpKw_2Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key steps in univariate analysis include data visualization, correlation analysis, and assessing the assumptions of linear regression (linearity, normality, homoscedasticity).\n"
      ],
      "metadata": {
        "id": "--BxYCpn2QgK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multiple Linear Regression:\n"
      ],
      "metadata": {
        "id": "cZkl9lpV2obD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multiple linear regression extends the concept of linear regression to incoporate multiple independent variables. It allows for modeling the relationship between a dependent variable and multiple predictor variables through a linear equation of the form\n"
      ],
      "metadata": {
        "id": "ZNENgqZQ2yM1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#            Y = b + w1X1 + w2X2+...+wn*Xn           "
      ],
      "metadata": {
        "id": "G-7P1Oog3U9J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multiple linear regression enables us to evaluate the unique contribution of each predictor variable while controlling for the effects of other variables.\n"
      ],
      "metadata": {
        "id": "jzesGDe73yOt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The coefficients (b, w1,w2,...,wn) are estimated using techniques like ordinary least squares (OLS) to minimize the sum of squared differences between the observed and predicted values.\n"
      ],
      "metadata": {
        "id": "-4sCcdVX5ERU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.regplot (x = \"room_num\", y = \"price\", data=df)"
      ],
      "metadata": {
        "id": "DjkgR8X25rWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Univariant analysis with python\n"
      ],
      "metadata": {
        "id": "JW0CnDBpzio3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression"
      ],
      "metadata": {
        "id": "zq7NFWOk6H-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = df['price']\n",
        "x = df[['room_num']]"
      ],
      "metadata": {
        "id": "XyJNxUM06W4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lm2 = LinearRegression()\n",
        "lm2.fit(x,y)"
      ],
      "metadata": {
        "id": "9TMOXVTu75Ff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(lm2.intercept_, lm2.coef_)"
      ],
      "metadata": {
        "id": "t0TITDbZ80X9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred1 = lm2.predict(x)"
      ],
      "metadata": {
        "id": "yTVcRz889D7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score\n",
        "r2_score(y, y_pred1)"
      ],
      "metadata": {
        "id": "tKcvmOjj9UGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "FMr6CavW9ztF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multiple Linear Regression with Python"
      ],
      "metadata": {
        "id": "3wphQHwiB_MO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = df['price']\n",
        "x = df.drop(\"price\",axis=1)"
      ],
      "metadata": {
        "id": "w7uaWbt2CGH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = LinearRegression()\n"
      ],
      "metadata": {
        "id": "D8Dq-nMXCtT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = x.dropna()\n",
        "y = y[x.index]\n",
        "lr.fit(x,y) #When I put only this line of code it said there were still some missing values so the lines of code above took care of that"
      ],
      "metadata": {
        "id": "ZBbqKAMyF7B4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr.intercept_, lr.coef_"
      ],
      "metadata": {
        "id": "JsUqjYuqGC_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = lr.predict(x)"
      ],
      "metadata": {
        "id": "ZbmNrkSGIXpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score\n",
        "r2_score(y, y_pred) #Now accuracy has moved up to 72% because other facets of the data is now considered as well"
      ],
      "metadata": {
        "id": "MUOiqvKeIkby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"Cleared House Data.csv\", index = False)"
      ],
      "metadata": {
        "id": "iz-jRdJp_6cD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "from sklearn.linear_model import LinearRegression #Replace with your actual model\n",
        "\n",
        "model = lr\n",
        "\n",
        "x_train = x\n",
        "\n",
        "#Create a dictionary to store the input widgets\n",
        "input_widgets = {}\n",
        "\n",
        "#Create text input widgets for each feature in x_train\n",
        "for feature in x_train.columns:\n",
        "  input_widgets[feature] = widgets.Text(description=feature + ':')\n",
        "prediction = 0\n",
        "#Create a prediction function\n",
        "def make_prediction(b):\n",
        "  input_features = {}\n",
        "\n",
        "  #Retrieve the input values from the widgets\n",
        "  for feature, widget in input_widgets.items():\n",
        "    input_features[feature] = widget.value\n",
        "\n",
        "  #Prepare the input data as a dictionary\n",
        "  input_data = {feature: [value] for feature, value in input_features.items()}\n",
        "\n",
        "  #Create a Dataframe from the input data\n",
        "  input_df = pd.DataFrame(input_data)\n",
        "\n",
        "  #Make a prediction\n",
        "  prediction = model.predict(input_df)\n",
        "\n",
        "  #Display the prediction\n",
        "  with oputput:\n",
        "    print(f\"Predicted Price: {prediction[0]}\")\n",
        "\n",
        "#Create a button for prediction\n",
        "predict_button = widgets.Button(description='Predict', button_style= \"danger\")\n",
        "predict_button.on_click(make_prediction)\n",
        "output = widgets.Output()\n",
        "\n",
        "#Display the widgets and prediction button\n",
        "input_widgets_list = list(input_widgets.values())\n",
        "input_widgets_list.append(predict_button)\n",
        "input_widgets_list.append(output)\n",
        "display(*input_widgets_list)\n",
        "\n",
        "#Create an output widget to display the prediction\n",
        "output = widgets.Output()\n"
      ],
      "metadata": {
        "id": "C_9U63LrAIg2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}